{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
||||||| constructed merge base
   "execution_count": 52,
=======
   "execution_count": 1,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "f5737a66-1bd1-46ec-8e4d-b93b31161662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbmi.exp_modular_fine_tuning.train import MODULAR_ADDITION_113_CLOCK_CONFIG\n",
    "from gbmi.exp_modular_fine_tuning.train import ModularFineTuningTrainingWrapper\n",
    "from gbmi.model import train_or_load_model\n",
    "import torch\n",
    "import einops\n",
    "from torch import tensor\n",
    "from math import *\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
||||||| constructed merge base
   "execution_count": 53,
=======
   "execution_count": 2,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "969de5aa-d4b7-4730-ad5c-2d447fba4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
||||||| constructed merge base
   "execution_count": 54,
=======
   "execution_count": 3,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "0938cd15-21af-4181-a8ae-3729c4956dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): Identity()\n",
       "      (ln2): Identity()\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
||||||| constructed merge base
     "execution_count": 54,
=======
     "execution_count": 3,
>>>>>>> Added gradient descent for fine tuning model for mod q
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MODULAR_ADDITION_113_CLOCK_CONFIG\n",
    "runtime, model = train_or_load_model(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
||||||| constructed merge base
   "execution_count": 55,
=======
   "execution_count": 25,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "80866175-d018-4a6e-8fcb-1738e13de1fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Identity' object has no attribute 'W_E'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      2\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_E\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m      4\u001b[0m embedding_matrix\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39membed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gbmi-SjKVSTWC-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Identity' object has no attribute 'W_E'"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "embedding_matrix = model.embed.W_E.clone()\n",
    "embedding_matrix.requires_grad = False\n",
    "model.embed = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
||||||| constructed merge base
   "execution_count": 56,
=======
   "execution_count": 39,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "901690ff-2479-4206-8018-024f6c926aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits,labels):\n",
    "    \n",
    "    if len(logits.shape)==3:\n",
    "        logits = logits[:,:, -1]\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
||||||| constructed merge base
   "execution_count": 81,
=======
   "execution_count": 101,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "609c64a6-8ecd-4a29-9f68-0c9bf51112dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "tensor([[[-13.4044,  -8.4327,  -7.3788],\n",
      "         [ -7.9987,  -1.4680,  -1.8959],\n",
      "         [ -7.1152,  -0.9084,  -2.1727],\n",
||||||| constructed merge base
      "tensor([[[-6.1755, -0.9262,  0.4199],\n",
      "         [ 8.8307,  2.8965,  1.1911],\n",
      "         [ 0.6976,  0.2317,  0.9714],\n",
=======
      "True\n",
      "tensor([[[-5.6113, -1.6324, -0.7081],\n",
      "         [ 4.4598,  2.1377,  3.4671],\n",
      "         [ 4.9144,  2.7077,  2.6583],\n",
>>>>>>> Added gradient descent for fine tuning model for mod q
      "         ...,\n",
<<<<<<< HEAD
      "         [ -1.9001,  -4.1179,  -1.8394],\n",
      "         [ -2.4754,  -4.2545,  -1.9024],\n",
      "         [ -3.1520,  -0.2011,   0.8079]],\n",
||||||| constructed merge base
      "         [ 4.6736,  2.9706,  0.2907],\n",
      "         [-9.5380, -1.7593, -1.5765],\n",
      "         [-1.0700, -0.4691,  0.0631]],\n",
=======
      "         [12.8711,  3.9381,  4.6663],\n",
      "         [-3.7543, -1.4235, -1.9024],\n",
      "         [ 7.0144,  2.3696,  1.3972]],\n",
>>>>>>> Added gradient descent for fine tuning model for mod q
      "\n",
<<<<<<< HEAD
      "        [[-14.1161, -10.7503,  -9.6303],\n",
      "         [ -5.1289,  -2.6639,  -3.0494],\n",
      "         [ -3.4011,  -2.2645,  -2.4352],\n",
||||||| constructed merge base
      "        [[-0.3219, -1.7656, -1.2217],\n",
      "         [ 4.9145,  2.5245,  0.6075],\n",
      "         [ 0.1035,  0.4361,  0.5748],\n",
=======
      "        [[-4.6051, -1.8866, -2.0971],\n",
      "         [ 5.3803,  2.4402,  2.8043],\n",
      "         [ 7.8653,  4.5636,  3.7210],\n",
>>>>>>> Added gradient descent for fine tuning model for mod q
      "         ...,\n",
<<<<<<< HEAD
      "         [ -0.4208,  -1.4352,  -1.4084],\n",
      "         [  1.0368,  -3.0338,  -2.8916],\n",
      "         [  0.8199,   0.5058,   0.9510]]], device='cuda:0',\n",
||||||| constructed merge base
      "         [ 4.8511,  3.5585,  0.9854],\n",
      "         [-7.0302, -2.4302, -0.7050],\n",
      "         [-2.3406,  0.6992,  2.8796]]], device='cuda:0',\n",
=======
      "         [ 8.0515,  3.1170,  3.3400],\n",
      "         [-3.5878, -1.3706, -1.1906],\n",
      "         [ 8.8866,  3.4218,  1.5590]]], device='cuda:0',\n",
>>>>>>> Added gradient descent for fine tuning model for mod q
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p = 113\n",
    "q = 2*p\n",
    "class DifferentModClock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DifferentModClock,self).__init__()\n",
    "        self.W_e = torch.nn.Parameter(-0.5/sqrt(p+1) + torch.rand((q+1,p+1))/sqrt(p+1))\n",
    "        self.W_u = torch.nn.Parameter(-0.5/sqrt(p) + torch.rand((q,p))/sqrt(p))\n",
    "        print(self.W_e.is_leaf)\n",
    "    def forward(self,x):\n",
    "        z = torch.nn.functional.one_hot(x.to(device)).to(torch.float)\n",
    "        \n",
    "        y = z @ self.W_e @ embedding_matrix\n",
    "        if(len(y.shape)!=3):\n",
    "            y = y.unsqueeze(0)\n",
    "        return self.W_u @ (einops.rearrange(model(y),\"b d p -> b p d\"))\n",
    "        \n",
    "clock = DifferentModClock()\n",
    "clock.to('cuda')\n",
    "print(clock(torch.tensor([[1,2,226],[2,5,226]])))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
||||||| constructed merge base
   "execution_count": 82,
=======
   "execution_count": 107,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "23111000-d93f-44af-a8c1-30a2ead4187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vector = einops.repeat(torch.arange(q), \"i -> (i j)\", j=q)\n",
    "b_vector = einops.repeat(torch.arange(q), \"j -> (i j)\", i=q)\n",
    "equals_vector = einops.repeat(torch.tensor(q), \" -> (i j)\", i=q, j=q)\n",
    "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)\n",
    "test_dataset = dataset[int(0.8*len(dataset)):]\n",
    "train_dataset = dataset[:int(0.8*len(dataset))]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
||||||| constructed merge base
   "execution_count": 83,
=======
   "execution_count": 104,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "3cfcdbf7-49f4-431f-9781-9f187aaf4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim =torch.optim.AdamW(clock.parameters())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
||||||| constructed merge base
   "execution_count": 84,
=======
   "execution_count": 111,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "fa0c0f06-6b35-4084-9e03-9171522d4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_batch():\n",
    "        test_labels = (test_dataset[:,0] + test_dataset[:,1]) %q\n",
    "        train_labels = (train_dataset[:,0] + train_dataset[:,1]) %q\n",
    "        train_y_preds = clock(train_dataset)\n",
    "        with torch.no_grad():\n",
    "            test_loss = loss_fn(clock(test_dataset),test_labels)\n",
    "        train_loss = loss_fn(train_y_preds, train_labels)\n",
    "        \n",
    "        train_loss.backward()\n",
    "  \n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        return f\"train loss:{train_loss}, test loss:{test_loss}\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
||||||| constructed merge base
   "execution_count": 90,
=======
   "execution_count": 113,
>>>>>>> Added gradient descent for fine tuning model for mod q
   "id": "fae6e101-b1be-4987-9c8a-09109fc2479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "torch.Size([51076, 226, 3])\n",
      "tensor(11.6124, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n"
||||||| constructed merge base
      "torch.Size([51076, 226, 3])\n",
      "tensor(11.5251, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n"
=======
      "train loss:0.38275332211732327, test loss:1.131400664316199\n",
      "train loss:0.37567360490690976, test loss:1.1234278484061757\n",
      "train loss:0.37359858688325265, test loss:1.1223254766770736\n",
      "train loss:0.3735103868924992, test loss:1.1231133163372031\n",
      "train loss:0.36992586529833443, test loss:1.1159870870319277\n",
      "train loss:0.3658854433221356, test loss:1.106093854327626\n",
      "train loss:0.3629027336599664, test loss:1.0994947441590666\n",
      "train loss:0.3608286176893845, test loss:1.0966790963688817\n",
      "train loss:0.3581995175258961, test loss:1.0942592657812642\n",
      "train loss:0.3544751659987958, test loss:1.090886884339052\n",
      "train loss:0.3519352715245581, test loss:1.0881531542498593\n",
      "train loss:0.3492882557227733, test loss:1.0856193586764944\n",
      "train loss:0.3474645741561127, test loss:1.0830415049967006\n",
      "train loss:0.3456884807460226, test loss:1.077867231937656\n",
      "train loss:0.3440912078369486, test loss:1.0728175344229933\n",
      "train loss:0.3422112354397092, test loss:1.06808979559681\n",
      "train loss:0.3400434065880942, test loss:1.0636374795796437\n",
      "train loss:0.3382923866764709, test loss:1.0614148837763893\n",
      "train loss:0.33516242444020367, test loss:1.0608303403327881\n",
      "train loss:0.3335521662240666, test loss:1.0615792841817533\n",
      "train loss:0.3319801980397624, test loss:1.0611485718180147\n",
      "train loss:0.3305171709682234, test loss:1.058405361683573\n",
      "train loss:0.3289110929208793, test loss:1.0537053967328687\n",
      "train loss:0.3275114678401272, test loss:1.0490403301449456\n",
      "train loss:0.3261125409369076, test loss:1.0450043103680537\n",
      "train loss:0.32471243879177564, test loss:1.0417731349289823\n",
      "train loss:0.3233766085501086, test loss:1.0398225416640907\n",
      "train loss:0.32065620361875036, test loss:1.0378813654668766\n",
      "train loss:0.31938191152751877, test loss:1.0370398668566294\n",
      "train loss:0.3181276049180955, test loss:1.0356913166825283\n",
      "train loss:0.31686406024290953, test loss:1.033718864924411\n",
      "train loss:0.3156055401378959, test loss:1.031259127053198\n",
      "train loss:0.3143942123042799, test loss:1.028575401973413\n",
      "train loss:0.3132344159601653, test loss:1.0260595893428848\n",
      "train loss:0.3120699306279723, test loss:1.024142167379001\n",
      "train loss:0.31092628260729516, test loss:1.0229639757548317\n",
      "train loss:0.30865273179532976, test loss:1.0211663235715838\n",
      "train loss:0.3075643225352523, test loss:1.0199317213597483\n",
      "train loss:0.3064675466277874, test loss:1.0183018209556918\n",
      "train loss:0.3053996729669757, test loss:1.0162537923601394\n",
      "train loss:0.30433333684367353, test loss:1.013964397226547\n",
      "train loss:0.3032750017473456, test loss:1.011819088174277\n",
      "train loss:0.30223340240732105, test loss:1.0100851449439263\n",
      "train loss:0.3012205709739725, test loss:1.0086712731389338\n",
      "train loss:0.30021543849250903, test loss:1.0072728869305998\n",
      "train loss:0.29821167611286603, test loss:1.003868597128466\n",
      "train loss:0.2972274631799839, test loss:1.0023412441287645\n",
      "train loss:0.29627267147701863, test loss:1.0012344483353672\n",
      "train loss:0.295319097157239, test loss:1.0002659731431025\n",
      "train loss:0.294377160779793, test loss:0.999071893579996\n",
      "train loss:0.29343215722641436, test loss:0.9975823636869292\n",
      "train loss:0.2925014586666579, test loss:0.9960722877418338\n",
      "train loss:0.29158734851783463, test loss:0.9946807774871482\n",
      "train loss:0.29067969117281955, test loss:0.993297103193473\n",
      "train loss:0.28888685148024235, test loss:0.9901684469726104\n",
      "train loss:0.2880020667481556, test loss:0.9886403683708537\n",
      "train loss:0.28712209119760657, test loss:0.987361374052208\n",
      "train loss:0.28625809862863805, test loss:0.9863113203319315\n",
      "train loss:0.2853988908499721, test loss:0.9852884915889801\n",
      "train loss:0.2845434834699664, test loss:0.9841110706529467\n",
      "train loss:0.28369675358881924, test loss:0.9826925943054716\n",
      "train loss:0.28285864092471, test loss:0.9810918462347701\n",
      "train loss:0.28202701552871023, test loss:0.9795014448752865\n",
      "train loss:0.28038478135362666, test loss:0.9770000661187139\n",
      "train loss:0.2795730045533431, test loss:0.9760121221156216\n",
      "train loss:0.27876640512321416, test loss:0.9750564495581646\n",
      "train loss:0.2779682810543594, test loss:0.9740555149014162\n",
      "train loss:0.27717586908911673, test loss:0.9728841057631427\n",
      "train loss:0.2763890246554834, test loss:0.9714745313648058\n",
      "train loss:0.27560705554801773, test loss:0.9699063987165328\n",
      "train loss:0.2748319929569966, test loss:0.9683629621423139\n",
      "train loss:0.2740629234598874, test loss:0.9669918894613383\n",
      "train loss:0.27254095183625726, test loss:0.9648846305362995\n",
      "train loss:0.2717878287096363, test loss:0.9639637997147663\n",
      "train loss:0.27104068472329995, test loss:0.9629546303138617\n",
      "train loss:0.27029872893531043, test loss:0.9618186723196871\n",
      "train loss:0.26956218313529556, test loss:0.9606087429930461\n",
      "train loss:0.2688309609586139, test loss:0.9593866162928517\n",
      "train loss:0.2681044760983396, test loss:0.9581574860014553\n",
      "train loss:0.26738280321534147, test loss:0.9569229769830491\n",
      "train loss:0.2666662197465314, test loss:0.9557390142551003\n",
      "train loss:0.2652480569589444, test loss:0.9537037169880528\n",
      "train loss:0.2645456439321997, test loss:0.952749859097521\n",
      "train loss:0.26384814652000044, test loss:0.9517341505460162\n",
      "train loss:0.2631553326365954, test loss:0.9506468256807031\n",
      "train loss:0.2624669472548983, test loss:0.9495071621169291\n",
      "train loss:0.26178256443026426, test loss:0.9483615032166308\n",
      "train loss:0.26110261528876905, test loss:0.9472678597818599\n",
      "train loss:0.26042689026618043, test loss:0.9462562807477145\n",
      "train loss:0.25975527944204296, test loss:0.9452828376449881\n",
      "train loss:0.2584249275552274, test loss:0.9432519530787752\n",
      "train loss:0.2577658402995862, test loss:0.9421898374821684\n",
      "train loss:0.25711112069148656, test loss:0.941121024738095\n",
      "train loss:0.25646066590369304, test loss:0.940068988733342\n",
      "train loss:0.2558141191434102, test loss:0.9390499691401386\n",
      "train loss:0.25517170898892044, test loss:0.9380542089307903\n",
      "train loss:0.25453323862619326, test loss:0.9370515914555222\n",
      "train loss:0.25389854372550613, test loss:0.9360177493675744\n",
      "train loss:0.2532676009439907, test loss:0.9349518050550164\n",
      "train loss:0.2520164923829851, test loss:0.9328489913148508\n",
      "train loss:0.25139606612737936, test loss:0.93184409574143\n",
      "train loss:0.2507790527605265, test loss:0.9308462557615551\n",
      "train loss:0.250165641992021, test loss:0.929845178677707\n",
      "train loss:0.24955561512128643, test loss:0.9288359074068571\n",
      "train loss:0.24894901777539163, test loss:0.9278166054929542\n",
      "train loss:0.24834567324101375, test loss:0.9267924206243581\n",
      "train loss:0.24774560923823835, test loss:0.9257755726054278\n",
      "train loss:0.24714854296620267, test loss:0.9247861375963973\n",
      "train loss:0.2459634669830741, test loss:0.9228670765056991\n",
      "train loss:0.24537562411398514, test loss:0.9219143579560769\n",
      "train loss:0.24479083564561818, test loss:0.9209707326073103\n",
      "train loss:0.24420919710712333, test loss:0.9200386947901965\n",
      "train loss:0.24363060835895364, test loss:0.919111947531548\n",
      "train loss:0.24305500754736317, test loss:0.9181789914115625\n",
      "train loss:0.24248249442476258, test loss:0.9172374004179381\n",
      "train loss:0.2419126641267898, test loss:0.9162888789732305\n",
      "train loss:0.24134575774983372, test loss:0.9153438553632406\n",
      "train loss:0.24022004186809875, test loss:0.9134734770263281\n",
      "train loss:0.23966132216755706, test loss:0.9125365418255325\n",
      "train loss:0.2391053813604863, test loss:0.9115857388725754\n",
      "train loss:0.23855204023105792, test loss:0.9106233507035795\n",
      "train loss:0.23800122925756537, test loss:0.9096557041722273\n",
      "train loss:0.23745303553218464, test loss:0.9086872124560389\n",
      "train loss:0.2369074115431827, test loss:0.9077248627708195\n",
      "train loss:0.2363645539896213, test loss:0.906775391878778\n",
      "train loss:0.2358241068371461, test loss:0.9058362699385694\n",
      "train loss:0.23475063508655908, test loss:0.9039658588723646\n",
      "train loss:0.2342174369108995, test loss:0.903034936396872\n",
      "train loss:0.2336865975412611, test loss:0.9021137184130058\n",
      "train loss:0.23315819937422322, test loss:0.9012057090836608\n",
      "train loss:0.2326322936049405, test loss:0.9003076896745867\n",
      "train loss:0.23210871296846186, test loss:0.8994202786696964\n",
      "train loss:0.23158743456029066, test loss:0.8985448577109463\n",
      "train loss:0.2310684949966308, test loss:0.8976808493539271\n",
      "train loss:0.23055188800249993, test loss:0.896827474785561\n",
      "train loss:0.2295254115526617, test loss:0.8951525629681396\n",
      "train loss:0.22901540623782016, test loss:0.8943217420613002\n",
      "train loss:0.22850765962421024, test loss:0.8934935161634037\n",
      "train loss:0.2280020122446391, test loss:0.8926692927086809\n",
      "train loss:0.2274985323581064, test loss:0.8918521534468822\n",
      "train loss:0.2269970011386965, test loss:0.8910397299267943\n",
      "train loss:0.2264976150070974, test loss:0.8902303319228223\n",
      "train loss:0.22600038630596941, test loss:0.8894144879356095\n",
      "train loss:0.22550520285098907, test loss:0.8885905491975925\n",
      "train loss:0.22452091477006367, test loss:0.8869275869528391\n",
      "train loss:0.22403165224730315, test loss:0.8860937699806537\n",
      "train loss:0.2235444250344937, test loss:0.8852612165893738\n",
      "train loss:0.22305918601861655, test loss:0.8844292170707314\n",
      "train loss:0.22257596279035397, test loss:0.8835941469954439\n",
      "train loss:0.22209470253459962, test loss:0.8827530172613756\n",
      "train loss:0.22161531173589463, test loss:0.8819088387622626\n",
      "train loss:0.22113786311781633, test loss:0.8810681239453699\n",
      "train loss:0.22066242969048586, test loss:0.8802402598246161\n",
      "train loss:0.21971670476238606, test loss:0.8786131343717559\n",
      "train loss:0.21924677923972444, test loss:0.8778086303791365\n",
      "train loss:0.21877866179352062, test loss:0.8770083874760501\n",
      "train loss:0.2183124109189984, test loss:0.8762205449067502\n",
      "train loss:0.2178479672056515, test loss:0.875448388292425\n",
      "train loss:0.2173852661189409, test loss:0.8746911953058215\n",
      "train loss:0.2169245091848626, test loss:0.8739368681226886\n",
      "train loss:0.216465668043314, test loss:0.8731817086165629\n",
      "train loss:0.21600879907773463, test loss:0.8724267880961701\n",
      "train loss:0.21510037953453515, test loss:0.8709203126087565\n",
      "train loss:0.21464896290479968, test loss:0.8701654525048519\n",
      "train loss:0.2141992233457593, test loss:0.8694047546549041\n",
      "train loss:0.21375134338289248, test loss:0.868635717822725\n",
      "train loss:0.21330521153111118, test loss:0.8678645489307514\n",
      "train loss:0.2128607352261231, test loss:0.867090191658244\n",
      "train loss:0.2124179486076051, test loss:0.8663168957472913\n",
      "train loss:0.21197678959499663, test loss:0.8655455183492401\n",
      "train loss:0.21153726936003098, test loss:0.8647735397871877\n",
      "train loss:0.21066316700038684, test loss:0.8632286157768301\n",
      "train loss:0.2102284746983066, test loss:0.8624587033461564\n",
      "train loss:0.20979533772225864, test loss:0.8616977532683952\n",
      "train loss:0.20936384414047457, test loss:0.8609452692941867\n",
      "train loss:0.20893394887695207, test loss:0.8602012082716566\n",
      "train loss:0.20850554033320998, test loss:0.8594660301945484\n",
      "train loss:0.20807866321680435, test loss:0.8587371321501965\n",
      "train loss:0.2076532752350542, test loss:0.8580159536013626\n",
      "train loss:0.2072292370949957, test loss:0.8573035594248466\n",
      "train loss:0.20638578165972274, test loss:0.8559003715416014\n",
      "train loss:0.205966218598236, test loss:0.8552006083479847\n",
      "train loss:0.20554827232815934, test loss:0.8544995073068195\n",
      "train loss:0.20513171479060452, test loss:0.8537987662945051\n",
      "train loss:0.20471660726672378, test loss:0.8530952437592427\n",
      "train loss:0.2043030576469087, test loss:0.8523873993963121\n",
      "train loss:0.2038908246898485, test loss:0.8516715004769387\n",
      "train loss:0.2034800251072595, test loss:0.850948822380588\n",
      "train loss:0.20307066777900024, test loss:0.8502244052260383\n",
      "train loss:0.20225621007426736, test loss:0.8487893632311003\n",
      "train loss:0.20185118743268268, test loss:0.8480762704282938\n",
      "train loss:0.20144746056792226, test loss:0.847367284456749\n",
      "train loss:0.20104521025316052, test loss:0.8466583860284854\n",
      "train loss:0.2006441691290733, test loss:0.8459522561673362\n",
      "train loss:0.20024448051547927, test loss:0.8452494027529732\n",
      "train loss:0.19984612014051187, test loss:0.8445468710474588\n",
      "train loss:0.1994490085267579, test loss:0.8438405356959889\n",
      "train loss:0.1990531391572879, test loss:0.843134681821988\n",
      "train loss:0.1982655555032317, test loss:0.8417277336333022\n",
      "train loss:0.19787365523891712, test loss:0.8410296655300613\n",
      "train loss:0.19748291759362033, test loss:0.8403297179470784\n",
      "train loss:0.19709334817013915, test loss:0.8396223269782804\n",
      "train loss:0.19670488586978874, test loss:0.8389081667906025\n",
      "train loss:0.19631762581436032, test loss:0.8381940487239963\n",
      "train loss:0.19593157535595884, test loss:0.837485237698659\n",
      "train loss:0.19554675208756464, test loss:0.8367828346802849\n",
      "train loss:0.19516325039689877, test loss:0.8360814405142487\n",
      "train loss:0.1943999419872014, test loss:0.8346784726944866\n",
      "train loss:0.19402021468327593, test loss:0.8339779133049895\n",
      "train loss:0.19364160583370907, test loss:0.8332781431789745\n",
      "train loss:0.19326422615868857, test loss:0.8325799010247807\n",
      "train loss:0.1928880223868047, test loss:0.831881479904786\n",
      "train loss:0.19251305084022746, test loss:0.8311755683849357\n",
      "train loss:0.192139222546934, test loss:0.8304671577951162\n",
      "train loss:0.191766556870453, test loss:0.8297627248292395\n",
      "train loss:0.1913950151606141, test loss:0.829069399568229\n",
      "train loss:0.1906556209432466, test loss:0.8277121611869578\n",
      "train loss:0.190287584607513, test loss:0.8270443662915457\n",
      "train loss:0.18992077814973882, test loss:0.8263781999008841\n",
      "train loss:0.18955517993159274, test loss:0.8257182957257828\n",
      "train loss:0.18919062262540523, test loss:0.8250635412258027\n",
      "train loss:0.18882725610646836, test loss:0.8244128058163505\n",
      "train loss:0.18846503919461635, test loss:0.8237615713585669\n",
      "train loss:0.18810403915702173, test loss:0.8231048071005042\n",
      "train loss:0.18774409873946613, test loss:0.8224398099787505\n",
      "train loss:0.18702775580297704, test loss:0.8210943585401648\n",
      "train loss:0.18667126958655103, test loss:0.8204222857791872\n",
      "train loss:0.18631595604497198, test loss:0.8197529225656434\n",
      "train loss:0.18596164946183394, test loss:0.8190903297865996\n",
      "train loss:0.18560839637967488, test loss:0.8184270811835919\n",
      "train loss:0.18525620149709332, test loss:0.8177606644114275\n",
      "train loss:0.18490499275047106, test loss:0.8170925967297408\n",
      "train loss:0.1845548413701533, test loss:0.8164238743583833\n",
      "train loss:0.1842056525003935, test loss:0.8157609997824371\n",
      "train loss:0.18351023804879543, test loss:0.8144563975813272\n",
      "train loss:0.18316407821593098, test loss:0.8138098644996441\n",
      "train loss:0.1828189269029144, test loss:0.8131679607507675\n",
      "train loss:0.18247475055316276, test loss:0.8125336246610069\n",
      "train loss:0.18213162359512622, test loss:0.81190561356962\n",
      "train loss:0.18178951543405394, test loss:0.8112826468065621\n",
      "train loss:0.18144849606631674, test loss:0.8106592357035285\n",
      "train loss:0.18110835980901377, test loss:0.8100308790381779\n",
      "train loss:0.1807692667570811, test loss:0.8094067968456885\n",
      "train loss:0.18009393694960552, test loss:0.8081763304900873\n",
      "train loss:0.17975772329751347, test loss:0.8075666402434026\n",
      "train loss:0.17942256905464218, test loss:0.806955450469978\n",
      "train loss:0.17908835186237276, test loss:0.8063399347982744\n",
      "train loss:0.17875514221314798, test loss:0.8057233787802178\n",
      "train loss:0.17842293756349623, test loss:0.8051133311940375\n",
      "train loss:0.17809173956422822, test loss:0.8045077231405412\n",
      "train loss:0.1777615767122263, test loss:0.803906524930213\n",
      "train loss:0.17743246668666415, test loss:0.8033063487952838\n",
      "train loss:0.17677712992067562, test loss:0.8021118424090664\n",
      "train loss:0.1764508838098128, test loss:0.8015164357421724\n",
      "train loss:0.17612564366158448, test loss:0.8009228007784526\n",
      "train loss:0.1758013525767782, test loss:0.800330218682543\n",
      "train loss:0.1754780107953893, test loss:0.7997386341892186\n",
      "train loss:0.17515559720871468, test loss:0.7991511061974002\n",
      "train loss:0.1748340531888232, test loss:0.7985676131186252\n",
      "train loss:0.17451339606787702, test loss:0.7979805866337015\n",
      "train loss:0.17419361793224247, test loss:0.7973909042709902\n",
      "train loss:0.17355708110217705, test loss:0.7962128721436171\n",
      "train loss:0.17324028662999652, test loss:0.7956264137449875\n",
      "train loss:0.1729243673452829, test loss:0.7950388725503126\n",
      "train loss:0.17260941333814708, test loss:0.794448565874435\n",
      "train loss:0.17229539506213548, test loss:0.7938534115737756\n",
      "train loss:0.17198224520748198, test loss:0.7932601242111171\n",
      "train loss:0.17166989241498792, test loss:0.7926739170072862\n",
      "train loss:0.171358539706958, test loss:0.7920948041399922\n",
      "train loss:0.1710480395398027, test loss:0.7915164719796355\n",
      "train loss:0.1704297241878745, test loss:0.7903551258609901\n",
      "train loss:0.17012193176211235, test loss:0.7897797503666965\n",
      "train loss:0.1698150375544295, test loss:0.7892123157842302\n",
      "train loss:0.1695089403994525, test loss:0.7886542908285188\n",
      "train loss:0.1692038217582554, test loss:0.788097607631644\n",
      "train loss:0.16889961145650995, test loss:0.7875401244303643\n",
      "train loss:0.16859618779844243, test loss:0.7869851882823505\n",
      "train loss:0.16829366091715806, test loss:0.7864337586580684\n",
      "train loss:0.16799202906421787, test loss:0.7858788745310109\n",
      "train loss:0.16739126378437663, test loss:0.7847474855765212\n",
      "train loss:0.16709222109529118, test loss:0.7841724703717796\n",
      "train loss:0.1667939364325845, test loss:0.7835978711713318\n",
      "train loss:0.16649645109870678, test loss:0.783024866470897\n",
      "train loss:0.1661997403365621, test loss:0.7824532546602351\n",
      "train loss:0.16590389074798767, test loss:0.7818821451020612\n",
      "train loss:0.16560881057774526, test loss:0.7813098726797867\n",
      "train loss:0.16531449686408184, test loss:0.7807410636279857\n",
      "train loss:0.1650209340156538, test loss:0.7801801870340531\n",
      "train loss:0.16443625070497922, test loss:0.7790805663213954\n",
      "train loss:0.16414514014790865, test loss:0.7785382517135995\n",
      "train loss:0.16385479753197582, test loss:0.777996524301068\n",
      "train loss:0.16356533076523871, test loss:0.7774557378681104\n",
      "train loss:0.16327648450760673, test loss:0.7769206276081352\n",
      "train loss:0.16298854700107374, test loss:0.7763895015580762\n",
      "train loss:0.1627012695300842, test loss:0.7758650668103184\n",
      "train loss:0.16241473770718207, test loss:0.775338254123669\n",
      "train loss:0.16212896287212775, test loss:0.7748084312570375\n",
      "train loss:0.16155961544840428, test loss:0.7737454154233215\n",
      "train loss:0.16127607954554263, test loss:0.7732126746592023\n",
      "train loss:0.16099326269922123, test loss:0.7726761747324918\n",
      "train loss:0.16071119334281728, test loss:0.7721360193856684\n",
      "train loss:0.16042990855664688, test loss:0.7715956897790904\n",
      "train loss:0.1601493061199479, test loss:0.7710559397137269\n",
      "train loss:0.15986949358858238, test loss:0.7705147626979755\n",
      "train loss:0.15959035826257523, test loss:0.7699755038800761\n",
      "train loss:0.1593120244908374, test loss:0.7694338861214899\n",
      "train loss:0.15875749551426055, test loss:0.768357602037201\n",
      "train loss:0.15848135298422172, test loss:0.7678242865087048\n",
      "train loss:0.1582060010794874, test loss:0.7672936381618104\n",
      "train loss:0.15793130901810382, test loss:0.7667731842151787\n",
      "train loss:0.15765731395317986, test loss:0.7662590285592639\n",
      "train loss:0.1573839582811335, test loss:0.7657440179108032\n",
      "train loss:0.15711138568224042, test loss:0.7652247153418118\n",
      "train loss:0.15683945819424017, test loss:0.7647034443718473\n",
      "train loss:0.1565682789040517, test loss:0.7641819613343668\n",
      "train loss:0.15602816307753198, test loss:0.7631437944323736\n",
      "train loss:0.15575925431588722, test loss:0.7626266441352938\n",
      "train loss:0.15549111015521017, test loss:0.7621018754538653\n",
      "train loss:0.15522357879180348, test loss:0.7615699053613307\n",
      "train loss:0.154956746497315, test loss:0.7610335403992678\n",
      "train loss:0.1546905915827274, test loss:0.7604959142704656\n",
      "train loss:0.15442514694205894, test loss:0.7599549413337139\n",
      "train loss:0.15416042231047175, test loss:0.7594095987376348\n",
      "train loss:0.15389639300070032, test loss:0.7588636890818968\n",
      "train loss:0.1533704444468855, test loss:0.7577639321050308\n",
      "train loss:0.15310851619266963, test loss:0.7572106735679686\n",
      "train loss:0.1528472521520608, test loss:0.7566574847199008\n",
      "train loss:0.15258654868656435, test loss:0.756107137200514\n",
      "train loss:0.15232660131827655, test loss:0.7555623701629636\n",
      "train loss:0.1520672155645438, test loss:0.7550175826231837\n",
      "train loss:0.15180843506918468, test loss:0.7544724972137324\n",
      "train loss:0.15155033075431346, test loss:0.7539272549474116\n",
      "train loss:0.1512928079527503, test loss:0.7533822596667692\n",
      "train loss:0.15077972422969232, test loss:0.7523081754884616\n",
      "train loss:0.1505241799298764, test loss:0.7517766762412269\n",
      "train loss:0.15026919520615253, test loss:0.7512480050913847\n",
      "train loss:0.1500147768649739, test loss:0.7507207423089676\n",
      "train loss:0.1497609309321257, test loss:0.7501953512764755\n",
      "train loss:0.14950773723275518, test loss:0.7496748964874939\n",
      "train loss:0.14925511872169128, test loss:0.7491596772685748\n",
      "train loss:0.14900305943297332, test loss:0.7486524189485755\n",
      "train loss:0.14875160244942134, test loss:0.7481496279953781\n",
      "train loss:0.14825055570991563, test loss:0.7471511037255709\n",
      "train loss:0.1480009908897715, test loss:0.7466552193860525\n",
      "train loss:0.14775202546941987, test loss:0.7461583212615064\n",
      "train loss:0.14750370082492595, test loss:0.7456604091319012\n",
      "train loss:0.1472560000022644, test loss:0.7451562517336181\n",
      "train loss:0.14700890328753696, test loss:0.7446505912934285\n",
      "train loss:0.1467624621933408, test loss:0.7441473912700084\n",
      "train loss:0.14651658599368175, test loss:0.7436464057713802\n",
      "train loss:0.14627139616051527, test loss:0.7431485124751132\n",
      "train loss:0.14578276216032995, test loss:0.7421512366014159\n",
      "train loss:0.14553938242763576, test loss:0.7416500041933827\n",
      "train loss:0.14529656696080767, test loss:0.7411530472191848\n",
      "train loss:0.1450544199082223, test loss:0.7406572517945019\n",
      "train loss:0.1448129385193097, test loss:0.7401582755015965\n",
      "train loss:0.1445719637628498, test loss:0.7396644198003299\n",
      "train loss:0.14433153620221204, test loss:0.7391752172095017\n",
      "train loss:0.14409169928443014, test loss:0.738682959168706\n",
      "train loss:0.1438524306136815, test loss:0.7381886459527285\n",
      "train loss:0.14337564198310462, test loss:0.7372085753188236\n",
      "train loss:0.1431381777460011, test loss:0.7367234655919379\n",
      "train loss:0.1429012576767586, test loss:0.7362415975871865\n",
      "train loss:0.14266495642093166, test loss:0.7357628831813271\n",
      "train loss:0.14242929120686135, test loss:0.7352822920085336\n",
      "train loss:0.14219417129674738, test loss:0.734804147427332\n",
      "train loss:0.14195965148151635, test loss:0.734327946628097\n",
      "train loss:0.1417257530249652, test loss:0.7338509155367842\n",
      "train loss:0.1414924310060521, test loss:0.7333713774468789\n",
      "train loss:0.14102746684184392, test loss:0.7324098908279283\n",
      "train loss:0.14079576756646575, test loss:0.7319270620238364\n",
      "train loss:0.14056465065531284, test loss:0.7314481458734188\n",
      "train loss:0.1403340522222638, test loss:0.7309727627365069\n",
      "train loss:0.14010402530926952, test loss:0.7304948972274433\n",
      "train loss:0.13987470097798943, test loss:0.730016433334294\n",
      "train loss:0.13964582217425836, test loss:0.7295377057267337\n",
      "train loss:0.13941750462959412, test loss:0.7290589329040199\n",
      "train loss:0.1391898306930386, test loss:0.7285828207387406\n",
      "train loss:0.13873600096616218, test loss:0.7276326838104864\n",
      "train loss:0.13850990462060947, test loss:0.7271600220489449\n",
      "train loss:0.1382843134550391, test loss:0.7266850944225544\n",
      "train loss:0.13805925130455826, test loss:0.7262046296704958\n",
      "train loss:0.13783466017309018, test loss:0.7257224597631842\n",
      "train loss:0.1376106575516282, test loss:0.7252388439796844\n",
      "train loss:0.1373871797272943, test loss:0.7247539079652956\n",
      "train loss:0.13716419713070466, test loss:0.7242730128351954\n",
      "train loss:0.13694170516013848, test loss:0.7237967706264151\n",
      "train loss:0.13649827220730784, test loss:0.7228391970101774\n",
      "train loss:0.13627738003566292, test loss:0.7223619844693016\n",
      "train loss:0.13605701901286213, test loss:0.7218869755907099\n",
      "train loss:0.1358372108951018, test loss:0.7214155636940462\n",
      "train loss:0.13561789974209906, test loss:0.7209504941493154\n",
      "train loss:0.135399038265877, test loss:0.7204890894120007\n",
      "train loss:0.13518072749008278, test loss:0.720027411000731\n",
      "train loss:0.1349629320252325, test loss:0.7195709635235941\n",
      "train loss:0.13474567238136842, test loss:0.7191171971865925\n",
      "train loss:0.1343126409637203, test loss:0.718200210596876\n",
      "train loss:0.1340969213043744, test loss:0.7177418157287596\n",
      "train loss:0.13388166922185232, test loss:0.7172878929812345\n",
      "train loss:0.13366692699438226, test loss:0.7168358901219769\n",
      "train loss:0.1334526689519634, test loss:0.716382069099449\n",
      "train loss:0.1332389207569087, test loss:0.715927260013916\n",
      "train loss:0.13302577901265183, test loss:0.7154708436556426\n",
      "train loss:0.13281307207223447, test loss:0.7150115429638282\n",
      "train loss:0.13260087278065152, test loss:0.7145501325391744\n",
      "train loss:0.13217799397660818, test loss:0.7136315427216595\n",
      "train loss:0.1319673168268487, test loss:0.7131730749568757\n",
      "train loss:0.13175710835050822, test loss:0.7127179511840274\n",
      "train loss:0.13154743634794788, test loss:0.7122653595048228\n",
      "train loss:0.1313383346847251, test loss:0.7118110177797182\n",
      "train loss:0.13112968687247906, test loss:0.7113529136678967\n",
      "train loss:0.13092152612765912, test loss:0.7108962218745518\n",
      "train loss:0.13071390959446463, test loss:0.7104452554673529\n",
      "train loss:0.13050675071097387, test loss:0.7099974202680179\n",
      "train loss:0.13009390743907104, test loss:0.7091141397949317\n",
      "train loss:0.129888280084475, test loss:0.708671619749298\n",
      "train loss:0.12968302047119099, test loss:0.7082300182923231\n",
      "train loss:0.12947838661130148, test loss:0.7077869692021992\n",
      "train loss:0.12927413821742828, test loss:0.7073460565747245\n",
      "train loss:0.12907038261759335, test loss:0.7069063663397546\n",
      "train loss:0.12886710967490053, test loss:0.7064678465948812\n",
      "train loss:0.12866427809970496, test loss:0.706024328960126\n",
      "train loss:0.12846189396133184, test loss:0.7055776568456713\n",
      "train loss:0.12805846988408834, test loss:0.7046902267586351\n",
      "train loss:0.12785743392807963, test loss:0.7042451648517879\n",
      "train loss:0.12765679593601517, test loss:0.7038015701277677\n",
      "train loss:0.12745666008226236, test loss:0.7033622042595319\n",
      "train loss:0.1272569546868465, test loss:0.7029267304945523\n",
      "train loss:0.12705772431719944, test loss:0.7024926014419133\n",
      "train loss:0.12685896810498123, test loss:0.7020593850061486\n",
      "train loss:0.1266606627688627, test loss:0.7016256624984626\n",
      "train loss:0.126462768384062, test loss:0.7011941613834142\n",
      "train loss:0.12606829277815226, test loss:0.7003350342767427\n",
      "train loss:0.12587169004239165, test loss:0.6999074527041395\n",
      "train loss:0.12567555383070303, test loss:0.6994764410826764\n",
      "train loss:0.12547981880956052, test loss:0.6990423494390293\n",
      "train loss:0.12528449648151369, test loss:0.6986066742685555\n",
      "train loss:0.1250896830635175, test loss:0.6981719826603117\n",
      "train loss:0.12489527689899435, test loss:0.6977409681488909\n",
      "train loss:0.12470134962999377, test loss:0.6973154507546704\n",
      "train loss:0.12450783922133066, test loss:0.6968948569775925\n",
      "train loss:0.12412212959360204, test loss:0.6960540145513755\n",
      "train loss:0.12392986485847252, test loss:0.6956318149905294\n",
      "train loss:0.12373806945580575, test loss:0.6952032305646644\n",
      "train loss:0.12354671080288039, test loss:0.6947763643492327\n",
      "train loss:0.12335575792803685, test loss:0.69434997292674\n",
      "train loss:0.12316520239294568, test loss:0.6939214916680225\n",
      "train loss:0.12297516211474128, test loss:0.6934940270178075\n",
      "train loss:0.12278557655841016, test loss:0.6930627303165046\n",
      "train loss:0.12259631665398737, test loss:0.69262996892659\n",
      "train loss:0.12221920602182222, test loss:0.6917573027808765\n",
      "train loss:0.12203124702511475, test loss:0.6913225537727616\n",
      "train loss:0.12184375611011568, test loss:0.6908914510766963\n",
      "train loss:0.12165662460179821, test loss:0.690463336906164\n",
      "train loss:0.12146996545233038, test loss:0.690033962959536\n",
      "train loss:0.12128371240647878, test loss:0.6896107300798993\n",
      "train loss:0.12109780422536175, test loss:0.6891917020020425\n",
      "train loss:0.12091232155546489, test loss:0.6887788206726373\n",
      "train loss:0.12072727224544984, test loss:0.6883609857998848\n",
      "train loss:0.12035838158588294, test loss:0.6875303699848535\n",
      "train loss:0.12017454742994728, test loss:0.6871138506241316\n",
      "train loss:0.11999105952882032, test loss:0.6866930883891035\n",
      "train loss:0.11980799545831518, test loss:0.6862709881784776\n",
      "train loss:0.11962535722447219, test loss:0.6858480806290166\n",
      "train loss:0.11944311544426767, test loss:0.6854248447498396\n",
      "train loss:0.11926131119138293, test loss:0.6850013292813678\n",
      "train loss:0.1190798459321405, test loss:0.6845740565841768\n",
      "train loss:0.11889879284075987, test loss:0.6841487080108296\n",
      "train loss:0.11853784572191, test loss:0.6833078999735491\n",
      "train loss:0.11835791424300704, test loss:0.6828928004129132\n",
      "train loss:0.11817843428161376, test loss:0.6824769167303244\n",
      "train loss:0.11799932583657262, test loss:0.6820600862054007\n",
      "train loss:0.11782068855641817, test loss:0.681646026397743\n",
      "train loss:0.11764242555205752, test loss:0.6812373085785609\n",
      "train loss:0.11746455705970094, test loss:0.6808351462750873\n",
      "train loss:0.11728707048236062, test loss:0.6804382462233105\n",
      "train loss:0.11710996949645111, test loss:0.6800449620504496\n",
      "train loss:0.11675693386409058, test loss:0.679257060023921\n",
      "train loss:0.11658105846662409, test loss:0.6788633458010462\n",
      "train loss:0.11640552446962897, test loss:0.6784669267427402\n",
      "train loss:0.11623033330301655, test loss:0.6780723881286277\n",
      "train loss:0.11605554262812498, test loss:0.6776787369967823\n",
      "train loss:0.11588109354386568, test loss:0.6772822159927513\n",
      "train loss:0.11570699774414686, test loss:0.6768854002045408\n",
      "train loss:0.11553328866353736, test loss:0.6764884518808449\n",
      "train loss:0.11535990283247166, test loss:0.6760966217451304\n",
      "train loss:0.11501413881638513, test loss:0.6753002930189648\n",
      "train loss:0.11484176618790094, test loss:0.6748978367689342\n",
      "train loss:0.11466981575315238, test loss:0.6744972766280238\n",
      "train loss:0.11449811963687936, test loss:0.6740944404045154\n",
      "train loss:0.11432685020694755, test loss:0.6736916289582464\n",
      "train loss:0.11415589748011773, test loss:0.6732838699490296\n",
      "train loss:0.11398532031390386, test loss:0.6728764735010947\n",
      "train loss:0.11381507028955833, test loss:0.6724739325933821\n",
      "train loss:0.11364517127502961, test loss:0.6720733088916047\n",
      "train loss:0.11330640148936218, test loss:0.6712728855313235\n",
      "train loss:0.11313759838080341, test loss:0.6708811240056018\n",
      "train loss:0.11296914471712315, test loss:0.6704887673036966\n",
      "train loss:0.1128010359606447, test loss:0.670090347903056\n",
      "train loss:0.11263327189266292, test loss:0.6696925296603815\n",
      "train loss:0.1124658405786238, test loss:0.6692978556976038\n",
      "train loss:0.1122987970581626, test loss:0.6689047819160514\n",
      "train loss:0.11213204113962481, test loss:0.6685139917325068\n",
      "train loss:0.11196564447045489, test loss:0.6681248998944878\n",
      "train loss:0.1116338472823549, test loss:0.6673484559016992\n",
      "train loss:0.11146846075779239, test loss:0.6669596757645165\n",
      "train loss:0.11130341124302118, test loss:0.6665717273457659\n",
      "train loss:0.11113873954774897, test loss:0.66618346071166\n",
      "train loss:0.1109743202729728, test loss:0.6657939831564581\n",
      "train loss:0.11081021485745178, test loss:0.6654040700705061\n",
      "train loss:0.1106464311434855, test loss:0.6650146765310335\n",
      "train loss:0.11048298322895078, test loss:0.6646206539942202\n",
      "train loss:0.11031985734570068, test loss:0.6642230608339668\n",
      "train loss:0.10999462532520245, test loss:0.663431788587432\n",
      "train loss:0.10983247804895203, test loss:0.6630394736843295\n",
      "train loss:0.10967066739872144, test loss:0.6626514837199523\n",
      "train loss:0.10950918478819939, test loss:0.6622674363050066\n",
      "train loss:0.10934803503781294, test loss:0.6618840711986321\n",
      "train loss:0.10918717786702663, test loss:0.6615076152604685\n",
      "train loss:0.10902664406885697, test loss:0.6611358112519222\n",
      "train loss:0.10886641961992999, test loss:0.6607631780954927\n",
      "train loss:0.10870653740084098, test loss:0.6603928939497432\n",
      "train loss:0.1083877802245806, test loss:0.659661632520907\n",
      "train loss:0.10822890146317063, test loss:0.6592970068109354\n",
      "train loss:0.10807036424401918, test loss:0.6589324762092068\n",
      "train loss:0.10791212125934144, test loss:0.6585712715473305\n",
      "train loss:0.10775421745062848, test loss:0.6582104453505222\n",
      "train loss:0.10759660649499662, test loss:0.6578450064514594\n",
      "train loss:0.10743929928621337, test loss:0.6574780551530908\n",
      "train loss:0.10728232969014988, test loss:0.6571082237807161\n",
      "train loss:0.10712570214823047, test loss:0.6567453527596047\n",
      "train loss:0.10681343048189297, test loss:0.6560225728234717\n",
      "train loss:0.1066577186841608, test loss:0.6556635312786296\n",
      "train loss:0.10650238222558331, test loss:0.6552981730867716\n",
      "train loss:0.10634736649169807, test loss:0.6549314708635263\n",
      "train loss:0.10619264451468177, test loss:0.6545664584584074\n",
      "train loss:0.1060382582521357, test loss:0.6541980368002474\n",
      "train loss:0.10588416375727899, test loss:0.6538294065436301\n",
      "train loss:0.1057304536119322, test loss:0.6534622890084253\n",
      "train loss:0.10557701349518642, test loss:0.6530940096344792\n",
      "train loss:0.10527116757927225, test loss:0.6523548553930409\n",
      "train loss:0.10511868758365789, test loss:0.6519968844107878\n",
      "train loss:0.10496652069904486, test loss:0.6516434381862066\n",
      "train loss:0.10481468461713658, test loss:0.6512889174954818\n",
      "train loss:0.1046631761013523, test loss:0.6509366927327246\n",
      "train loss:0.10451195513659613, test loss:0.650592020629823\n",
      "train loss:0.10436102177916734, test loss:0.6502509438642073\n",
      "train loss:0.10421039612896484, test loss:0.6499138560217839\n",
      "train loss:0.104059982949509, test loss:0.6495767403464463\n",
      "train loss:0.10376009664833195, test loss:0.6489074402162637\n",
      "train loss:0.10361059415113986, test loss:0.6485804608732337\n",
      "train loss:0.10346136198854683, test loss:0.6482541863899736\n",
      "train loss:0.1033124122819951, test loss:0.6479305469963679\n",
      "train loss:0.1031637470125474, test loss:0.6476113815998181\n",
      "train loss:0.10301533182761102, test loss:0.647290127746391\n",
      "train loss:0.10286729735111576, test loss:0.6469690766407136\n",
      "train loss:0.10271955739313139, test loss:0.6466457622452935\n",
      "train loss:0.10257207881233128, test loss:0.6463191447246585\n",
      "train loss:0.10227802469165342, test loss:0.6456652202547675\n",
      "train loss:0.10213140148015208, test loss:0.645336309713128\n",
      "train loss:0.10198505270146552, test loss:0.6450037646591342\n",
      "train loss:0.10183896000470159, test loss:0.6446651803309733\n",
      "train loss:0.10169320403947016, test loss:0.6443316535524828\n",
      "train loss:0.1015477111964524, test loss:0.6440006043248939\n",
      "train loss:0.10140248619463066, test loss:0.6436711557086796\n",
      "train loss:0.10125753845540268, test loss:0.6433381346338071\n",
      "train loss:0.10111292373834835, test loss:0.6430090638300622\n",
      "train loss:0.10082443044776258, test loss:0.6423725861357155\n",
      "train loss:0.10068060214351253, test loss:0.6420519590663188\n",
      "train loss:0.10053707497441158, test loss:0.6417207326395435\n",
      "train loss:0.10039385034074587, test loss:0.6413951991709768\n",
      "train loss:0.10025092861662951, test loss:0.6410805758718409\n",
      "train loss:0.10010825455933599, test loss:0.640758103552616\n",
      "train loss:0.09996591532715818, test loss:0.6404293921486763\n",
      "train loss:0.09982381937291213, test loss:0.6401025148416983\n",
      "train loss:0.09968203381252143, test loss:0.639779571635022\n",
      "train loss:0.09939936471377153, test loss:0.6391184505685295\n",
      "train loss:0.09925847211619271, test loss:0.6387841972099657\n",
      "train loss:0.09911781840954609, test loss:0.6384530826205249\n",
      "train loss:0.09897742128609252, test loss:0.6381221337496855\n",
      "train loss:0.09883735012025961, test loss:0.6377935545606683\n",
      "train loss:0.0986974738716907, test loss:0.6374647880676808\n",
      "train loss:0.09855797154784017, test loss:0.637139142494549\n",
      "train loss:0.098418684653299, test loss:0.6368167916469002\n",
      "train loss:0.09827966122101955, test loss:0.6364929396968222\n",
      "train loss:0.09800252002312336, test loss:0.6358453155428742\n",
      "train loss:0.09786434242104582, test loss:0.635528078098691\n",
      "train loss:0.09772643660216945, test loss:0.6352138551716247\n",
      "train loss:0.09758878142582489, test loss:0.63489174533572\n",
      "train loss:0.09745138311673317, test loss:0.634567131735386\n",
      "train loss:0.09731422318331738, test loss:0.6342477934515445\n",
      "train loss:0.09717739463903334, test loss:0.6339291111116163\n",
      "train loss:0.09704081865692125, test loss:0.6336047049673422\n",
      "train loss:0.09690451522885371, test loss:0.6332830628415203\n",
      "train loss:0.09663269348411434, test loss:0.6326379374283726\n",
      "train loss:0.0964971786262986, test loss:0.6323144157914184\n",
      "train loss:0.09636196310736524, test loss:0.6319967051899773\n",
      "train loss:0.09622693034569102, test loss:0.6316797907448223\n",
      "train loss:0.0960922474306799, test loss:0.6313642426948209\n",
      "train loss:0.09595780873651749, test loss:0.6310481141211091\n",
      "train loss:0.09582363841160828, test loss:0.6307316936385193\n",
      "train loss:0.09568976261435776, test loss:0.6304122489704708\n",
      "train loss:0.09555609443102198, test loss:0.630095648130083\n",
      "train loss:0.09528960644505975, test loss:0.6294559849304683\n",
      "train loss:0.09515677122713383, test loss:0.6291399951057602\n",
      "train loss:0.09502417699709638, test loss:0.6288208629531695\n",
      "train loss:0.09489182418911067, test loss:0.6284979876819042\n",
      "train loss:0.09475975485295764, test loss:0.6281780045464355\n",
      "train loss:0.09462795652176541, test loss:0.6278646051296745\n",
      "train loss:0.09449637851546208, test loss:0.6275524270985079\n",
      "train loss:0.0943650716151349, test loss:0.6272396621024581\n",
      "train loss:0.09423398629584857, test loss:0.6269272839104878\n",
      "train loss:0.09397264018673014, test loss:0.6263123041811426\n",
      "train loss:0.09384234813714987, test loss:0.6260069790176427\n",
      "train loss:0.09371231137431632, test loss:0.6257031017577123\n",
      "train loss:0.09358250501181642, test loss:0.6253942028468785\n",
      "train loss:0.09345297001083497, test loss:0.6250883223504096\n",
      "train loss:0.0933237146748906, test loss:0.6247849463849582\n",
      "train loss:0.09319470890338367, test loss:0.6244764062374037\n",
      "train loss:0.09306592441356293, test loss:0.6241736959521323\n",
      "train loss:0.09293736837835515, test loss:0.6238763215633893\n",
      "train loss:0.09268104296792282, test loss:0.6232792322179987\n",
      "train loss:0.09255326217780276, test loss:0.6229816696332128\n",
      "train loss:0.09242572448280716, test loss:0.6226903635036174\n",
      "train loss:0.09229842048430784, test loss:0.6223965212845645\n",
      "train loss:0.09217137611329967, test loss:0.6220969069749288\n",
      "train loss:0.0920445587702206, test loss:0.6218037643723014\n",
      "train loss:0.09191800137306243, test loss:0.6215176613976605\n",
      "train loss:0.09179168589286978, test loss:0.6212292069255841\n",
      "train loss:0.09166558805228187, test loss:0.6209272684553503\n",
      "train loss:0.09141415639008475, test loss:0.6203291015552617\n",
      "train loss:0.0912888197310437, test loss:0.620031901722363\n",
      "train loss:0.09116368511972842, test loss:0.6197271204949351\n",
      "train loss:0.0910387819041369, test loss:0.619417817095274\n",
      "train loss:0.0909141433682129, test loss:0.619112663362761\n",
      "train loss:0.09078971728807725, test loss:0.6188135038767318\n",
      "train loss:0.09066556540010991, test loss:0.6185146795453579\n",
      "train loss:0.09054162734559301, test loss:0.6182114022903534\n",
      "train loss:0.09041803577080999, test loss:0.6179037139601211\n",
      "train loss:0.09017143245909214, test loss:0.6172968229430535\n",
      "train loss:0.09004851579685415, test loss:0.6169910624763842\n",
      "train loss:0.08992581192535404, test loss:0.6166820674334155\n",
      "train loss:0.08980335001405707, test loss:0.6163753492591552\n",
      "train loss:0.08968111701231189, test loss:0.6160667334119785\n",
      "train loss:0.0895591243800338, test loss:0.6157562286158258\n",
      "train loss:0.08943735406524436, test loss:0.6154492705939879\n",
      "train loss:0.08931578460687202, test loss:0.6151452048896435\n",
      "train loss:0.0891943743736829, test loss:0.6148382272375734\n",
      "train loss:0.0889523923282249, test loss:0.6142132959883669\n",
      "train loss:0.08883174235586849, test loss:0.6138998535814922\n",
      "train loss:0.08871129055566868, test loss:0.6135935601611093\n",
      "train loss:0.08859107099822155, test loss:0.6132880761272986\n",
      "train loss:0.08847106022853662, test loss:0.6129797781805233\n",
      "train loss:0.0883512765286577, test loss:0.6126732140459554\n",
      "train loss:0.0882317627576816, test loss:0.6123717197653995\n",
      "train loss:0.08811245522927673, test loss:0.6120707496460723\n",
      "train loss:0.08799337837553574, test loss:0.6117734533601955\n",
      "train loss:0.08775590018068313, test loss:0.6111786349137861\n",
      "train loss:0.08763748776604893, test loss:0.6108774817811525\n",
      "train loss:0.08751921199325878, test loss:0.6105789435017392\n",
      "train loss:0.08740122570085186, test loss:0.6102778868163351\n",
      "train loss:0.08728339099439572, test loss:0.6099759421478407\n",
      "train loss:0.08716583138169, test loss:0.6096818625850888\n",
      "train loss:0.0870484606193016, test loss:0.6093854731155414\n",
      "train loss:0.08693128224759228, test loss:0.6090898996591283\n",
      "train loss:0.08681431655827859, test loss:0.6087983580175105\n",
      "train loss:0.08658103512553626, test loss:0.6082142837108813\n",
      "train loss:0.08646470677170422, test loss:0.6079292863432446\n",
      "train loss:0.08634855752519394, test loss:0.6076407847391646\n",
      "train loss:0.08623266717453307, test loss:0.6073527641752047\n",
      "train loss:0.08611696334925256, test loss:0.6070725791515028\n",
      "train loss:0.08600145918407996, test loss:0.6067904941618993\n",
      "train loss:0.08588618399105431, test loss:0.6065056337350305\n",
      "train loss:0.08577105273149593, test loss:0.6062245362884582\n",
      "train loss:0.08565618397874612, test loss:0.6059402501055444\n",
      "train loss:0.08542697095551618, test loss:0.6053651935884706\n",
      "train loss:0.08531269276918667, test loss:0.6050814696941496\n",
      "train loss:0.08519862706184024, test loss:0.6048051924407712\n",
      "train loss:0.08508475101302579, test loss:0.6045290417020793\n",
      "train loss:0.08497110400340033, test loss:0.6042491900277185\n",
      "train loss:0.08485770161327617, test loss:0.6039720655724629\n",
      "train loss:0.08474445212738202, test loss:0.6036957039707056\n",
      "train loss:0.08463143925819851, test loss:0.6034150261210134\n",
      "train loss:0.08451865395076316, test loss:0.603133430190642\n",
      "train loss:0.08429371503632527, test loss:0.602580035932222\n",
      "train loss:0.08418153116914133, test loss:0.6023058986978082\n",
      "train loss:0.08406959091118986, test loss:0.6020352047553496\n",
      "train loss:0.08395785472022183, test loss:0.601761617754882\n",
      "train loss:0.08384633290513915, test loss:0.6014895876090689\n",
      "train loss:0.08373504653361799, test loss:0.6012206138992515\n",
      "train loss:0.08362394596912927, test loss:0.6009473681889416\n",
      "train loss:0.08351303317558491, test loss:0.6006675196280673\n",
      "train loss:0.0834023345462923, test loss:0.6003909511266037\n",
      "train loss:0.0831815614463055, test loss:0.5998402773191571\n",
      "train loss:0.0830714908020234, test loss:0.5995583886677948\n",
      "train loss:0.08296163180956635, test loss:0.5992788464406446\n",
      "train loss:0.08285191936455263, test loss:0.598993098246915\n",
      "train loss:0.08274245307758411, test loss:0.5987091130924233\n",
      "train loss:0.08263316977312736, test loss:0.5984361230968254\n",
      "train loss:0.08252409998906374, test loss:0.598158545391483\n",
      "train loss:0.08241526954498497, test loss:0.59786957927198\n",
      "train loss:0.08230663103725967, test loss:0.5975844459293256\n",
      "train loss:0.0820899349520631, test loss:0.597026181447555\n",
      "train loss:0.08198188596017451, test loss:0.5967409998216772\n",
      "train loss:0.08187403232068804, test loss:0.596459278553113\n",
      "train loss:0.08176640715488048, test loss:0.5961769598828532\n",
      "train loss:0.08165893066539116, test loss:0.59589653841742\n",
      "train loss:0.0815516556416127, test loss:0.5956166922113134\n",
      "train loss:0.08144459819573732, test loss:0.5953401879430057\n",
      "train loss:0.08133776041967872, test loss:0.5950692420160972\n",
      "train loss:0.08123112536824723, test loss:0.594801745903176\n",
      "train loss:0.08101841376336172, test loss:0.5942602208485595\n",
      "train loss:0.08091233519007036, test loss:0.5939899003750417\n",
      "train loss:0.0808064694152277, test loss:0.5937280450471183\n",
      "train loss:0.08070083215776831, test loss:0.5934736274064406\n",
      "train loss:0.08059539014865931, test loss:0.5932116812047113\n",
      "train loss:0.08049012488641305, test loss:0.592946630981454\n",
      "train loss:0.08038502001964191, test loss:0.592693696872188\n",
      "train loss:0.08028013371798276, test loss:0.5924451744724285\n",
      "train loss:0.08017545151044973, test loss:0.5921977761290019\n",
      "train loss:0.07996655559828407, test loss:0.5917021164174441\n",
      "train loss:0.07986239838194037, test loss:0.5914582768391745\n",
      "train loss:0.0797583483780881, test loss:0.5912131087996171\n",
      "train loss:0.07965454612457458, test loss:0.5909670199641307\n",
      "train loss:0.07955089603197628, test loss:0.5907208918339705\n",
      "train loss:0.07944746693242981, test loss:0.5904754989322619\n",
      "train loss:0.07934416066033739, test loss:0.5902305786955053\n",
      "train loss:0.07924104967570972, test loss:0.5899857005280484\n",
      "train loss:0.07913805449534264, test loss:0.5897435182436136\n",
      "train loss:0.07893269041615504, test loss:0.5892468456917862\n",
      "train loss:0.07883022864538052, test loss:0.5889950903606895\n",
      "train loss:0.07872799114506313, test loss:0.5887441532877145\n",
      "train loss:0.07862590874266233, test loss:0.5884902061794786\n",
      "train loss:0.07852399984882064, test loss:0.5882318027326611\n",
      "train loss:0.07842224725173114, test loss:0.5879773319812328\n",
      "train loss:0.07832068682525939, test loss:0.5877224454808996\n",
      "train loss:0.07821932684688117, test loss:0.5874647149288986\n",
      "train loss:0.07811813238488087, test loss:0.5872063122557976\n",
      "train loss:0.0779162303959831, test loss:0.5866880694338387\n",
      "train loss:0.07781555554469377, test loss:0.5864314252299938\n",
      "train loss:0.07771501763386351, test loss:0.5861759203440836\n",
      "train loss:0.07761466609431059, test loss:0.5859210031912624\n",
      "train loss:0.07751450336332137, test loss:0.5856703352248831\n",
      "train loss:0.07741448082296717, test loss:0.5854238537999769\n",
      "train loss:0.07731465997874537, test loss:0.5851775892137893\n",
      "train loss:0.07721498672762472, test loss:0.5849360123250053\n",
      "train loss:0.07711551210935082, test loss:0.5846927282440536\n",
      "train loss:0.0769170255223173, test loss:0.5841978038045615\n",
      "train loss:0.0768180245406847, test loss:0.5839587670491662\n",
      "train loss:0.0767191849980222, test loss:0.5837219554637465\n",
      "train loss:0.07662051046950596, test loss:0.5834783255772529\n",
      "train loss:0.07652206124284058, test loss:0.5832367294336187\n",
      "train loss:0.07642373369442218, test loss:0.5830007859839814\n",
      "train loss:0.0763256009405837, test loss:0.5827620788090405\n",
      "train loss:0.07622762956324543, test loss:0.5825189070139335\n",
      "train loss:0.07612985399362174, test loss:0.5822714256304113\n",
      "train loss:0.07593478832178174, test loss:0.5817869608006436\n",
      "train loss:0.0758375057081908, test loss:0.5815430803395981\n",
      "train loss:0.0757404066684814, test loss:0.5812922943430117\n",
      "train loss:0.0756434990278966, test loss:0.581043101300634\n",
      "train loss:0.07554667557606116, test loss:0.5807962395632603\n",
      "train loss:0.07545006985450291, test loss:0.5805508097516912\n",
      "train loss:0.07535361380027765, test loss:0.5802974951454934\n",
      "train loss:0.07525732686428337, test loss:0.580040252569234\n",
      "train loss:0.07516122524044157, test loss:0.5797882725267135\n",
      "train loss:0.07496949167834978, test loss:0.5792831332196046\n",
      "train loss:0.07487387748894588, test loss:0.5790273806377357\n",
      "train loss:0.07477839517640758, test loss:0.5787740933221269\n",
      "train loss:0.0746831222422707, test loss:0.578524179722326\n",
      "train loss:0.07458798168364741, test loss:0.5782762225182743\n",
      "train loss:0.07449301272635331, test loss:0.578026884283256\n",
      "train loss:0.07439818293469365, test loss:0.5777767232819192\n",
      "train loss:0.07430353204548058, test loss:0.5775291566733701\n",
      "train loss:0.07420904806878287, test loss:0.5772817457681674\n",
      "train loss:0.07402056340453707, test loss:0.5767871714173016\n",
      "train loss:0.07392654186810121, test loss:0.5765361401631101\n",
      "train loss:0.07383261475889291, test loss:0.5762905056042383\n",
      "train loss:0.07373892476524078, test loss:0.5760539091629076\n",
      "train loss:0.07364534017006796, test loss:0.5758207077281908\n",
      "train loss:0.07355196377086026, test loss:0.5755831088934275\n",
      "train loss:0.0734587049278394, test loss:0.5753474479372696\n",
      "train loss:0.07336559277826323, test loss:0.575114239465292\n",
      "train loss:0.07327267535952998, test loss:0.5748883242999387\n",
      "train loss:0.07308728581614923, test loss:0.5744311633380795\n",
      "train loss:0.07299484337185863, test loss:0.5742040152656248\n",
      "train loss:0.07290251731219582, test loss:0.5739834860114676\n",
      "train loss:0.07281036488121068, test loss:0.5737600061901049\n",
      "train loss:0.07271840416745237, test loss:0.5735285158733469\n",
      "train loss:0.07262657704905943, test loss:0.5732969799800397\n",
      "train loss:0.07253489612054251, test loss:0.5730720803446246\n",
      "train loss:0.07244340991487097, test loss:0.5728436622499893\n",
      "train loss:0.07235206974586406, test loss:0.5726100735334773\n",
      "train loss:0.07216984673675762, test loss:0.5721360913802018\n",
      "train loss:0.07207895938240316, test loss:0.5718975389096262\n",
      "train loss:0.0719882481118622, test loss:0.5716572433987919\n",
      "train loss:0.07189766798276953, test loss:0.5714153588085867\n",
      "train loss:0.07180725322238649, test loss:0.5711761227626011\n",
      "train loss:0.07171698276685277, test loss:0.5709341627764426\n",
      "train loss:0.0716268497301346, test loss:0.5706919236027567\n",
      "train loss:0.0715369012352567, test loss:0.5704508605363231\n",
      "train loss:0.07144708397459983, test loss:0.5702134581305008\n",
      "train loss:0.07126787596827817, test loss:0.5697409865539891\n",
      "train loss:0.07117849264476883, test loss:0.5695022927990909\n",
      "train loss:0.07108926933072061, test loss:0.5692662236983107\n",
      "train loss:0.07100018279415722, test loss:0.5690329985835044\n",
      "train loss:0.07091122343811686, test loss:0.5688005624125375\n",
      "train loss:0.0708224192938339, test loss:0.5685680007806672\n",
      "train loss:0.07073374374693012, test loss:0.568333052108797\n",
      "train loss:0.0706452344219851, test loss:0.5681001615122655\n",
      "train loss:0.0705568255196046, test loss:0.5678685440787502\n",
      "train loss:0.07038052586964276, test loss:0.5673953537407774\n",
      "train loss:0.07029259448897388, test loss:0.5671582728053217\n",
      "train loss:0.07020482813153794, test loss:0.5669206077837791\n",
      "train loss:0.07011716827322653, test loss:0.566689348287942\n",
      "train loss:0.07002969759538284, test loss:0.5664541323590604\n",
      "train loss:0.06994235902724931, test loss:0.566220164979375\n",
      "train loss:0.0698551616523038, test loss:0.5659888045900452\n",
      "train loss:0.06976812150465417, test loss:0.5657647613325462\n",
      "train loss:0.0696812279074968, test loss:0.5655390926715429\n",
      "train loss:0.06950789014228083, test loss:0.5650787399513674\n",
      "train loss:0.0694214416504175, test loss:0.5648475293326647\n",
      "train loss:0.06933511558218305, test loss:0.5646150405893446\n",
      "train loss:0.06924896259758134, test loss:0.5643800039598379\n",
      "train loss:0.0691629195003121, test loss:0.5641463648540931\n",
      "train loss:0.06907703628289041, test loss:0.5639129930114865\n",
      "train loss:0.06899130618614818, test loss:0.5636788561856466\n",
      "train loss:0.06890572290627599, test loss:0.563439421430726\n",
      "train loss:0.0688203029848631, test loss:0.5632005355231694\n",
      "train loss:0.06864977016199678, test loss:0.5627294733950325\n",
      "train loss:0.06856472574906727, test loss:0.5624880161049532\n",
      "train loss:0.06847982320703248, test loss:0.5622473913317816\n",
      "train loss:0.06839507542275604, test loss:0.5620102502484157\n",
      "train loss:0.06831041995928097, test loss:0.5617751655200907\n",
      "train loss:0.06822593063183446, test loss:0.5615424669910019\n",
      "train loss:0.06814156672399115, test loss:0.5613155452033941\n",
      "train loss:0.06805732829399354, test loss:0.5610931622002177\n",
      "train loss:0.0679732458752413, test loss:0.5608699592873351\n",
      "train loss:0.06780547945662727, test loss:0.5604189042682413\n",
      "train loss:0.06772176210965142, test loss:0.5602005142085013\n",
      "train loss:0.06763821752161665, test loss:0.5599786356502195\n",
      "train loss:0.06755480662278494, test loss:0.5597538575464858\n",
      "train loss:0.06747150187696323, test loss:0.5595319676172\n",
      "train loss:0.0673883859080944, test loss:0.5593130907462097\n",
      "train loss:0.06730538541917423, test loss:0.5590967900306623\n",
      "train loss:0.06722253215487406, test loss:0.5588804049440006\n",
      "train loss:0.06713979353774152, test loss:0.5586619382181538\n",
      "train loss:0.0669747952258128, test loss:0.5582202862612181\n",
      "train loss:0.06689250671233413, test loss:0.5580018464649046\n",
      "train loss:0.06681033087366572, test loss:0.5577844737280292\n",
      "train loss:0.06672829558035899, test loss:0.5575625789379358\n",
      "train loss:0.06664640992818308, test loss:0.5573375811540192\n",
      "train loss:0.06656461895886191, test loss:0.5571183511283087\n",
      "train loss:0.0664829417685159, test loss:0.5569021618961384\n",
      "train loss:0.06640141209821002, test loss:0.5566822616147741\n",
      "train loss:0.06632001096843851, test loss:0.5564594325625195\n",
      "train loss:0.0661576096677544, test loss:0.5560269162733956\n",
      "train loss:0.06607658339421918, test loss:0.5558110445309665\n",
      "train loss:0.06599571586409318, test loss:0.5555910497220089\n",
      "train loss:0.06591496224809511, test loss:0.5553735007606814\n",
      "train loss:0.06583433061657479, test loss:0.5551620134063059\n",
      "train loss:0.06575387139998119, test loss:0.5549447488298426\n",
      "train loss:0.0656734958082232, test loss:0.5547221722556279\n",
      "train loss:0.06559325109179232, test loss:0.5545036289046992\n",
      "train loss:0.06551316501836568, test loss:0.5542823784834459\n",
      "train loss:0.0653533432737707, test loss:0.5538405287238197\n",
      "train loss:0.06527364612450813, test loss:0.5536211677835676\n",
      "train loss:0.06519406887095319, test loss:0.5534013439900977\n",
      "train loss:0.06511460483020458, test loss:0.5531708427809691\n",
      "train loss:0.06503531609060698, test loss:0.5529447347100932\n",
      "train loss:0.06495611548084161, test loss:0.5527224403885983\n",
      "train loss:0.06487705328501804, test loss:0.552494455250821\n",
      "train loss:0.06479813129446485, test loss:0.5522613546011176\n",
      "train loss:0.06471930865899439, test loss:0.5520343141498987\n",
      "train loss:0.06456207039553823, test loss:0.5515835059973799\n",
      "train loss:0.06448366733364111, test loss:0.5513575307230963\n",
      "train loss:0.064405379374445, test loss:0.5511352273527081\n",
      "train loss:0.06432720832027457, test loss:0.5509126145663179\n",
      "train loss:0.06424916576100048, test loss:0.5506886126302168\n",
      "train loss:0.06417129339890423, test loss:0.5504692249780753\n",
      "train loss:0.06409352707503743, test loss:0.5502508372283472\n",
      "train loss:0.06401591127530355, test loss:0.550029420319631\n",
      "train loss:0.0639383673898011, test loss:0.549809306763374\n",
      "train loss:0.06378373638649518, test loss:0.5493729404797377\n",
      "train loss:0.06370658284130988, test loss:0.5491452255300491\n",
      "train loss:0.06362957054470134, test loss:0.5489209930418152\n",
      "train loss:0.06355272434713734, test loss:0.5487011619133865\n",
      "train loss:0.06347596536634373, test loss:0.5484823594062784\n",
      "train loss:0.06339937476998453, test loss:0.5482581348719944\n",
      "train loss:0.06332286689328154, test loss:0.548035569164157\n",
      "train loss:0.06324651632097808, test loss:0.5478145689936409\n",
      "train loss:0.06317027182446115, test loss:0.5475952946093048\n",
      "train loss:0.06301817070298911, test loss:0.5471600880850178\n",
      "train loss:0.06294229089883649, test loss:0.5469499810267483\n",
      "train loss:0.06286655165344243, test loss:0.546745438565352\n",
      "train loss:0.06279091536061582, test loss:0.546538520198171\n",
      "train loss:0.06271542640645107, test loss:0.546331134581812\n",
      "train loss:0.06264004384348512, test loss:0.5461231688150221\n",
      "train loss:0.06256479305461751, test loss:0.5459198902020023\n",
      "train loss:0.06248967570435058, test loss:0.5457154153805004\n",
      "train loss:0.062414675723949875, test loss:0.5455073188262556\n",
      "train loss:0.06226504302233202, test loss:0.5451022685163915\n",
      "train loss:0.06219039849621029, test loss:0.5449023479167846\n",
      "train loss:0.06211590826154425, test loss:0.5447074640753113\n",
      "train loss:0.06204151027347136, test loss:0.5445157917836569\n",
      "train loss:0.061967233005703885, test loss:0.5443227890551172\n",
      "train loss:0.06189308656007481, test loss:0.544130765123706\n",
      "train loss:0.06181904934371138, test loss:0.5439384191603744\n",
      "train loss:0.061745136003011236, test loss:0.5437465369812546\n",
      "train loss:0.061671320765601584, test loss:0.5435556623214196\n",
      "train loss:0.06152402069220796, test loss:0.5431632756817311\n",
      "train loss:0.0614505448050652, test loss:0.5429728538194637\n",
      "train loss:0.06137719278926696, test loss:0.5427772261864866\n",
      "train loss:0.06130393115957549, test loss:0.542576883296168\n",
      "train loss:0.06123081915768119, test loss:0.5423781165509544\n",
      "train loss:0.06115783259779705, test loss:0.5421776947178368\n",
      "train loss:0.061084914673444374, test loss:0.541966679762669\n",
      "train loss:0.06101216893948513, test loss:0.5417607688903398\n",
      "train loss:0.060939455289477605, test loss:0.5415576901488042\n",
      "train loss:0.06079448249365284, test loss:0.5411419536369262\n",
      "train loss:0.0607221952285938, test loss:0.5409323416446443\n",
      "train loss:0.06064999193716352, test loss:0.5407338703060602\n",
      "train loss:0.06057788067584358, test loss:0.5405335261240612\n",
      "train loss:0.06050593605590842, test loss:0.5403299447944387\n",
      "train loss:0.060434056672210544, test loss:0.5401252886283043\n",
      "train loss:0.06036231453302628, test loss:0.5399212604600477\n",
      "train loss:0.06029067039246608, test loss:0.5397210047758481\n",
      "train loss:0.06021916525890121, test loss:0.5395175237904988\n",
      "train loss:0.060076474274382036, test loss:0.5391058120356288\n",
      "train loss:0.06000526219176018, test loss:0.5389039339578737\n",
      "train loss:0.059934199039735514, test loss:0.5387062354191747\n",
      "train loss:0.05986321662815287, test loss:0.5384994555892303\n",
      "train loss:0.05979234465124762, test loss:0.5382961955281002\n",
      "train loss:0.05972158869071383, test loss:0.5380966874636277\n",
      "train loss:0.05965095494601301, test loss:0.5378966750183297\n",
      "train loss:0.05958039221703331, test loss:0.5376910550704929\n",
      "train loss:0.05951001393837642, test loss:0.5374875535969961\n",
      "train loss:0.059369502221800256, test loss:0.5370885812534357\n",
      "train loss:0.05929939426804368, test loss:0.5368845128075703\n",
      "train loss:0.05922940125615288, test loss:0.5366864628835298\n",
      "train loss:0.05915954040495916, test loss:0.5364846003500575\n",
      "train loss:0.0590897734728658, test loss:0.5362832029276434\n",
      "train loss:0.05902007420584529, test loss:0.5360814043483139\n",
      "train loss:0.0589505206090768, test loss:0.535875278903946\n",
      "train loss:0.05888108286550361, test loss:0.5356753583027828\n",
      "train loss:0.05881175424374249, test loss:0.5354730084595413\n",
      "train loss:0.05867338328773553, test loss:0.5350686444805983\n",
      "train loss:0.058604351140491714, test loss:0.5348686069844188\n",
      "train loss:0.05853542880742753, test loss:0.5346634102108228\n",
      "train loss:0.058466626122934136, test loss:0.5344631168921272\n",
      "train loss:0.05839791982289085, test loss:0.5342585298042168\n",
      "train loss:0.05832931853361781, test loss:0.5340520638647733\n",
      "train loss:0.05826083758871917, test loss:0.5338479422819605\n",
      "train loss:0.05819243979259052, test loss:0.5336478476114707\n",
      "train loss:0.05812418142555503, test loss:0.5334495519420107\n",
      "train loss:0.0579879642769988, test loss:0.5330422809666446\n",
      "train loss:0.05792004077080314, test loss:0.5328466304481616\n",
      "train loss:0.05785219735298617, test loss:0.5326486433350305\n",
      "train loss:0.057784455697401045, test loss:0.5324490536323019\n",
      "train loss:0.05771683665942643, test loss:0.5322443510399899\n",
      "train loss:0.05764931490838176, test loss:0.5320367463318066\n",
      "train loss:0.05758187050121777, test loss:0.5318321519338628\n",
      "train loss:0.05751457224444886, test loss:0.5316208923491275\n",
      "train loss:0.05744732237315591, test loss:0.5314067490405004\n",
      "train loss:0.05731316801302406, test loss:0.530999007792063\n",
      "train loss:0.05724624485317586, test loss:0.5307960733323847\n",
      "train loss:0.05717943750813596, test loss:0.5305931925068567\n",
      "train loss:0.05711270129020482, test loss:0.5303998334947744\n",
      "train loss:0.057046091141410314, test loss:0.5302082633572345\n",
      "train loss:0.0569795723760348, test loss:0.5300095478668227\n",
      "train loss:0.056913151843770285, test loss:0.5298109466171533\n",
      "train loss:0.056846851562528125, test loss:0.5296152216189595\n",
      "train loss:0.05678065689883186, test loss:0.5294187502845557\n",
      "train loss:0.05664853460267515, test loss:0.5290274831632114\n",
      "train loss:0.0565826253009844, test loss:0.5288341524160097\n",
      "train loss:0.05651683763842102, test loss:0.5286436162403839\n",
      "train loss:0.056451148517901124, test loss:0.5284437956085428\n",
      "train loss:0.05638556574383336, test loss:0.5282449998607365\n",
      "train loss:0.05632009036066994, test loss:0.5280577940710366\n",
      "train loss:0.05625472655254531, test loss:0.5278722986210466\n",
      "train loss:0.05618944056729724, test loss:0.5276773522700173\n",
      "train loss:0.05612423012597111, test loss:0.5274782104898798\n",
      "train loss:0.0559941886942259, test loss:0.5270923774808933\n",
      "train loss:0.0559293301135721, test loss:0.5268954625452238\n",
      "train loss:0.055864546788379384, test loss:0.5266949423144414\n",
      "train loss:0.055799871466658726, test loss:0.5264999033295079\n",
      "train loss:0.05573529220465762, test loss:0.5263082920484837\n",
      "train loss:0.055670795585963724, test loss:0.5261187921269722\n",
      "train loss:0.05560642104193626, test loss:0.5259237780238403\n",
      "train loss:0.055542123564837655, test loss:0.5257325440053964\n",
      "train loss:0.05547793230454143, test loss:0.5255480945404893\n",
      "train loss:0.05534982852541862, test loss:0.5251660817210608\n",
      "train loss:0.055285934257221156, test loss:0.5249739412858737\n",
      "train loss:0.05522212174006995, test loss:0.5247885199415003\n",
      "train loss:0.05515842072732592, test loss:0.5246075007296643\n",
      "train loss:0.0550947855029167, test loss:0.5244225129467283\n",
      "train loss:0.055031225500115834, test loss:0.5242355215449713\n",
      "train loss:0.05496779261365902, test loss:0.5240522388214184\n",
      "train loss:0.05490443937247898, test loss:0.5238710217160629\n",
      "train loss:0.05484118291416207, test loss:0.5236878575588003\n",
      "train loss:0.054714974395625414, test loss:0.5233199311957736\n",
      "train loss:0.0546520075779338, test loss:0.5231395535133853\n",
      "train loss:0.054589151710394725, test loss:0.5229585649316887\n",
      "train loss:0.05452639760363764, test loss:0.5227774804067098\n",
      "train loss:0.054463755594055106, test loss:0.5225937584817298\n",
      "train loss:0.054401186971290684, test loss:0.5224135888483831\n",
      "train loss:0.05433871470974078, test loss:0.5222311333597843\n",
      "train loss:0.05427634901365419, test loss:0.522046781814345\n",
      "train loss:0.054214080072822676, test loss:0.5218632615966288\n",
      "train loss:0.05408983079961343, test loss:0.5215009710170148\n",
      "train loss:0.05402788036331725, test loss:0.5213232606122591\n",
      "train loss:0.05396599763100895, test loss:0.521139845480313\n",
      "train loss:0.05390422577005945, test loss:0.5209592584143058\n",
      "train loss:0.05384252771306165, test loss:0.5207834341987859\n",
      "train loss:0.053780932619793644, test loss:0.5206030454306727\n",
      "train loss:0.053719481554887274, test loss:0.5204194403009207\n",
      "train loss:0.05365807009897203, test loss:0.520244649938462\n",
      "train loss:0.05359679433209244, test loss:0.5200714530480907\n",
      "train loss:0.05347450915039477, test loss:0.5197082401140316\n",
      "train loss:0.05341351553347549, test loss:0.5195239778479819\n",
      "train loss:0.05335260130733322, test loss:0.5193400160793924\n",
      "train loss:0.05329176525424113, test loss:0.5191516025451031\n",
      "train loss:0.05323105337933147, test loss:0.5189624509651576\n",
      "train loss:0.053170456861117724, test loss:0.5187708916835869\n",
      "train loss:0.05310990659392511, test loss:0.5185755538093055\n",
      "train loss:0.05304947521685358, test loss:0.5183834527850056\n",
      "train loss:0.052989127934257065, test loss:0.5181917387115225\n",
      "train loss:0.052868737764626965, test loss:0.5178027584966396\n",
      "train loss:0.052808686548172154, test loss:0.5176116530938124\n",
      "train loss:0.052748716945940834, test loss:0.5174256153474978\n",
      "train loss:0.05268885379634377, test loss:0.5172367302102456\n",
      "train loss:0.05262905693987676, test loss:0.5170441447383172\n",
      "train loss:0.052569370838356125, test loss:0.5168603739463702\n",
      "train loss:0.05250977568436505, test loss:0.5166782981754586\n",
      "train loss:0.052450263268370705, test loss:0.5164967699600891\n",
      "train loss:0.05239083310484816, test loss:0.5163131006960603\n",
      "train loss:0.05227228154601819, test loss:0.5159582163662036\n",
      "train loss:0.05221310987378135, test loss:0.5157802408074263\n",
      "train loss:0.0521540433700158, test loss:0.5156060932337725\n",
      "train loss:0.05209508918644942, test loss:0.5154291304538411\n",
      "train loss:0.05203620167412766, test loss:0.5152526196979674\n",
      "train loss:0.051977389793575024, test loss:0.5150808907687644\n",
      "train loss:0.05191867223537433, test loss:0.5149102143096067\n",
      "train loss:0.05186004911998188, test loss:0.5147399778071682\n",
      "train loss:0.05180151515388391, test loss:0.5145761517462856\n",
      "train loss:0.05168470074278918, test loss:0.5142394281241671\n",
      "train loss:0.05162643376487216, test loss:0.5140643681545136\n",
      "train loss:0.05156822312791489, test loss:0.5138925506269559\n",
      "train loss:0.051510142336811615, test loss:0.513725106660095\n",
      "train loss:0.051452118726085565, test loss:0.5135555604325392\n",
      "train loss:0.05139416767255344, test loss:0.5133915733400953\n",
      "train loss:0.05133637407510796, test loss:0.5132261828719193\n",
      "train loss:0.05127863529042988, test loss:0.5130594183451386\n",
      "train loss:0.05122097558344985, test loss:0.5129000817005153\n",
      "train loss:0.05110591117525292, test loss:0.5125640484552899\n",
      "train loss:0.05104850658176503, test loss:0.5124021417021545\n",
      "train loss:0.05099117532578055, test loss:0.5122412928093971\n",
      "train loss:0.05093391159424358, test loss:0.5120801711122028\n",
      "train loss:0.05087673870437831, test loss:0.5119139334214869\n",
      "train loss:0.05081964067293707, test loss:0.5117509267640717\n",
      "train loss:0.05076263584213984, test loss:0.5115913396488224\n",
      "train loss:0.050705727113738344, test loss:0.5114242566572204\n",
      "train loss:0.05064887555965917, test loss:0.5112573944114774\n",
      "train loss:0.05053544593470839, test loss:0.5109301297050322\n",
      "train loss:0.05047883786612772, test loss:0.510769101513611\n",
      "train loss:0.05042234094027041, test loss:0.5106045733477281\n",
      "train loss:0.050365911549380686, test loss:0.5104414568806626\n",
      "train loss:0.05030957725416498, test loss:0.510281570951636\n",
      "train loss:0.05025333632290682, test loss:0.5101138139675404\n",
      "train loss:0.05019716865461797, test loss:0.5099505411709301\n",
      "train loss:0.05014109616025678, test loss:0.5097941221433568\n",
      "train loss:0.0500851120835345, test loss:0.5096362807447022\n",
      "train loss:0.049973363299416304, test loss:0.5093198963345876\n",
      "train loss:0.04991761507466357, test loss:0.5091629122611736\n",
      "train loss:0.04986195175552355, test loss:0.5090019258192521\n",
      "train loss:0.04980636765039574, test loss:0.508839864722143\n",
      "train loss:0.049750873072474514, test loss:0.5086802518836562\n",
      "train loss:0.04969545627651502, test loss:0.5085222713199529\n",
      "train loss:0.04964011712794701, test loss:0.5083611689934865\n",
      "train loss:0.049584846141771256, test loss:0.5081962753477414\n",
      "train loss:0.049529681819059666, test loss:0.508035977040548\n",
      "train loss:0.04941958548982543, test loss:0.5077178730062604\n",
      "train loss:0.04936466863755662, test loss:0.5075600911980398\n",
      "train loss:0.049309826435089875, test loss:0.5073982539918492\n",
      "train loss:0.0492550751096692, test loss:0.5072422833397451\n",
      "train loss:0.04920040937264544, test loss:0.5070897805734879\n",
      "train loss:0.049145814751585706, test loss:0.5069332769610405\n",
      "train loss:0.04909128488217718, test loss:0.5067779322717316\n",
      "train loss:0.049036842272880465, test loss:0.5066221955806511\n",
      "train loss:0.04898245949499978, test loss:0.5064643509285421\n",
      "train loss:0.04887398061481037, test loss:0.5061552776776278\n",
      "train loss:0.048819887607482834, test loss:0.5059968859365245\n",
      "train loss:0.0487658257955345, test loss:0.5058380759589974\n",
      "train loss:0.0487118664033999, test loss:0.5056814399919053\n",
      "train loss:0.0486579605522855, test loss:0.5055227034066888\n",
      "train loss:0.048604154241921375, test loss:0.5053607902046302\n",
      "train loss:0.04855040241569881, test loss:0.5051994547324649\n",
      "train loss:0.04849674088762003, test loss:0.5050433326523606\n",
      "train loss:0.04844315016277731, test loss:0.5048893856625178\n",
      "train loss:0.04833620933047697, test loss:0.5045864140686416\n",
      "train loss:0.04828283761972379, test loss:0.504434005910595\n",
      "train loss:0.048229571264923284, test loss:0.5042849846601823\n",
      "train loss:0.04817637938764328, test loss:0.504134983787302\n",
      "train loss:0.048123245142491035, test loss:0.5039806722024872\n",
      "train loss:0.04807019024431477, test loss:0.503829448862974\n",
      "train loss:0.04801722307482097, test loss:0.5036820398653756\n",
      "train loss:0.04796432716109564, test loss:0.5035300443216986\n",
      "train loss:0.047911521445075385, test loss:0.5033759227627139\n",
      "train loss:0.04780610961801592, test loss:0.5030821150579747\n",
      "train loss:0.04775353150729957, test loss:0.5029292338188506\n",
      "train loss:0.04770101477326118, test loss:0.5027736691411183\n",
      "train loss:0.04764857545247758, test loss:0.5026278827058058\n",
      "train loss:0.04759620709767484, test loss:0.5024783547388639\n",
      "train loss:0.04754392266641724, test loss:0.5023193826237137\n",
      "train loss:0.04749172984681226, test loss:0.502171590022647\n",
      "train loss:0.04743960877394926, test loss:0.5020324361091558\n",
      "train loss:0.04738755368113251, test loss:0.5018804882232568\n",
      "train loss:0.04728370384339647, test loss:0.5015765775124328\n",
      "train loss:0.04723189216414905, test loss:0.501433911409665\n",
      "train loss:0.047180140435488294, test loss:0.501285226452727\n",
      "train loss:0.047128464314655096, test loss:0.5011403430132192\n",
      "train loss:0.04707688401185761, test loss:0.5009926085738857\n",
      "train loss:0.04702536787766123, test loss:0.5008444908491321\n",
      "train loss:0.046973929380673636, test loss:0.5007016507329509\n",
      "train loss:0.04692255095244044, test loss:0.5005519853403646\n",
      "train loss:0.0468712520841874, test loss:0.5004050088763968\n",
      "train loss:0.04676889669873669, test loss:0.5001025609804414\n",
      "train loss:0.04671782473764618, test loss:0.4999504197545269\n",
      "train loss:0.04666681286349403, test loss:0.49979542808470523\n",
      "train loss:0.04661588840554532, test loss:0.49963732176975567\n",
      "train loss:0.04656501217078234, test loss:0.49948124949765926\n",
      "train loss:0.04651423714533175, test loss:0.49932998697943376\n",
      "train loss:0.04646350618868723, test loss:0.4991671484105396\n",
      "train loss:0.046412841363086633, test loss:0.49901022111425336\n",
      "train loss:0.046362265497815885, test loss:0.49885551664271793\n",
      "train loss:0.04626129489799107, test loss:0.49853440616493\n",
      "train loss:0.046210931094546465, test loss:0.4983819174005601\n",
      "train loss:0.04616064913790583, test loss:0.4982199826678212\n",
      "train loss:0.04611042396989369, test loss:0.4980624137020829\n",
      "train loss:0.04606025489070992, test loss:0.4979151301043632\n",
      "train loss:0.0460101760184344, test loss:0.4977682195381536\n",
      "train loss:0.04596016020791814, test loss:0.49761858429548445\n",
      "train loss:0.04591019438144637, test loss:0.49747163577512105\n",
      "train loss:0.045860319796546443, test loss:0.497329692446064\n",
      "train loss:0.04576075287720032, test loss:0.4970317691839964\n",
      "train loss:0.04571110178101677, test loss:0.49688450059428196\n",
      "train loss:0.045661491889757225, test loss:0.4967351272759809\n",
      "train loss:0.04561194920348262, test loss:0.4965753629041354\n",
      "train loss:0.04556246872287939, test loss:0.49642383496332504\n",
      "train loss:0.04551306748135008, test loss:0.4962748174928839\n",
      "train loss:0.045463738415910636, test loss:0.49612589875064567\n",
      "train loss:0.045414441429482776, test loss:0.49597812476487896\n",
      "train loss:0.045365246644813666, test loss:0.4958293566403359\n",
      "train loss:0.045267076397731934, test loss:0.4955355903864424\n",
      "train loss:0.04521808600237863, test loss:0.4953909519208549\n",
      "train loss:0.04516916088684025, test loss:0.49524410200577423\n",
      "train loss:0.04512032243803656, test loss:0.495099059029168\n",
      "train loss:0.04507154589694331, test loss:0.4949623127294977\n",
      "train loss:0.045022833508239754, test loss:0.49482419846203685\n",
      "train loss:0.04497419560592059, test loss:0.4946849055176535\n",
      "train loss:0.0449256309342201, test loss:0.4945454307320192\n",
      "train loss:0.04487714788388302, test loss:0.49440482576290673\n",
      "train loss:0.04478038897516733, test loss:0.4941304057945096\n",
      "train loss:0.04473211244843975, test loss:0.4939961046327121\n",
      "train loss:0.04468390488719733, test loss:0.4938546953377706\n",
      "train loss:0.04463576809595211, test loss:0.49372222274453365\n",
      "train loss:0.044587698669199996, test loss:0.49358690078465195\n",
      "train loss:0.04453971403446267, test loss:0.49345315718977667\n",
      "train loss:0.04449177435109584, test loss:0.4933108204710658\n",
      "train loss:0.044443918922772156, test loss:0.49317554076209746\n",
      "train loss:0.04439611266502877, test loss:0.49304536590060294\n",
      "train loss:0.04430070853943293, test loss:0.49275980296942135\n",
      "train loss:0.044253097937213125, test loss:0.4926295566675399\n",
      "train loss:0.044205569375464784, test loss:0.4924955623928346\n",
      "train loss:0.04415810547254149, test loss:0.4923539227026197\n",
      "train loss:0.04411072147273398, test loss:0.49221421285462597\n",
      "train loss:0.04406340590295762, test loss:0.4920802667238001\n",
      "train loss:0.04401616928611922, test loss:0.4919423877436969\n",
      "train loss:0.04396897178455468, test loss:0.49179768383285133\n",
      "train loss:0.043921846027079395, test loss:0.4916650387436348\n",
      "train loss:0.04382784613789061, test loss:0.49138911827870024\n",
      "train loss:0.043780938554142244, test loss:0.49124914756437615\n",
      "train loss:0.04373409750735729, test loss:0.49111639011791497\n",
      "train loss:0.043687342656247276, test loss:0.4909824628379849\n",
      "train loss:0.04364062451928307, test loss:0.49084532228603084\n",
      "train loss:0.04359396779646466, test loss:0.49071127569452844\n",
      "train loss:0.043547378008040635, test loss:0.49057810463230095\n",
      "train loss:0.043500859052400954, test loss:0.4904402530595705\n",
      "train loss:0.043454406014427564, test loss:0.49030801065745994\n",
      "train loss:0.04336168769587582, test loss:0.49003701123499943\n",
      "train loss:0.0433154071884579, test loss:0.4899014955714885\n",
      "train loss:0.0432691999429932, test loss:0.48976457280679603\n",
      "train loss:0.043223089060348545, test loss:0.4896218688919235\n",
      "train loss:0.04317701660083435, test loss:0.48948578923185576\n",
      "train loss:0.04313102372346604, test loss:0.489350294080749\n",
      "train loss:0.04308510333950453, test loss:0.48921149507525236\n",
      "train loss:0.043039229982460035, test loss:0.489075256861718\n",
      "train loss:0.04299343625208734, test loss:0.48894236027389915\n",
      "train loss:0.0429020172516794, test loss:0.48866829383613214\n",
      "train loss:0.04285641387921118, test loss:0.4885378124425545\n",
      "train loss:0.042810850135973966, test loss:0.48840283340460666\n",
      "train loss:0.042765343011268685, test loss:0.48826195680944423\n",
      "train loss:0.042719937930146265, test loss:0.48812824538227156\n",
      "train loss:0.04267457462230654, test loss:0.48800575289439324\n",
      "train loss:0.04262926827259657, test loss:0.48786814817071295\n",
      "train loss:0.042584017580743996, test loss:0.48772825110196927\n",
      "train loss:0.04253883933095546, test loss:0.4876021108202172\n",
      "train loss:0.04244865737384931, test loss:0.4873286205851133\n",
      "train loss:0.04240365955497691, test loss:0.4871930809115054\n",
      "train loss:0.042358726930561606, test loss:0.48706698355170464\n",
      "train loss:0.0423138280221884, test loss:0.48693852712738545\n",
      "train loss:0.042268991390318866, test loss:0.48680880576797947\n",
      "train loss:0.04222421820219441, test loss:0.48667185818859326\n",
      "train loss:0.042179510437503655, test loss:0.48654219182751846\n",
      "train loss:0.04213486052353604, test loss:0.4864195577557056\n",
      "train loss:0.0420902617752733, test loss:0.4862913268508892\n",
      "train loss:0.04200124338306124, test loss:0.4860301134948613\n",
      "train loss:0.041956848320804416, test loss:0.4859078638130151\n",
      "train loss:0.04191250389165187, test loss:0.48578005468488933\n",
      "train loss:0.041868226071769994, test loss:0.48565483656160674\n",
      "train loss:0.04182397830273761, test loss:0.4855387607905618\n",
      "train loss:0.0417798156905422, test loss:0.48541921548576944\n",
      "train loss:0.04173571571403814, test loss:0.48529058077189535\n",
      "train loss:0.04169168568368236, test loss:0.48516692853694193\n",
      "train loss:0.04164769397908765, test loss:0.4850483143914768\n",
      "train loss:0.04155991287587611, test loss:0.484789430828774\n",
      "train loss:0.04151612541061735, test loss:0.4846609768865052\n",
      "train loss:0.04147239711868044, test loss:0.4845319912260844\n",
      "train loss:0.041428700702075945, test loss:0.48440019668313994\n",
      "train loss:0.04138509079485842, test loss:0.48427298919283\n",
      "train loss:0.0413415352088723, test loss:0.484143002317654\n",
      "train loss:0.04129806019206936, test loss:0.48401079032852695\n",
      "train loss:0.04125462051900385, test loss:0.48388348019299193\n",
      "train loss:0.04121124684745566, test loss:0.4837507347952497\n",
      "train loss:0.041124696741581805, test loss:0.48347788043618267\n",
      "train loss:0.04108152286977697, test loss:0.48334575574953975\n",
      "train loss:0.04103839825775777, test loss:0.4832145992164688\n",
      "train loss:0.0409953153507692, test loss:0.4830821492332288\n",
      "train loss:0.040952298812325344, test loss:0.48295107395932607\n",
      "train loss:0.040909339457666574, test loss:0.4828256416561857\n",
      "train loss:0.04086642899806464, test loss:0.48269887603949085\n",
      "train loss:0.04082359309771612, test loss:0.48256817145885395\n",
      "train loss:0.04078081097991992, test loss:0.4824422084094651\n",
      "train loss:0.040695448313044426, test loss:0.48218468850679275\n",
      "train loss:0.04065284045819827, test loss:0.4820624872093875\n",
      "train loss:0.040610288922723614, test loss:0.48193919145426173\n",
      "train loss:0.04056777007134908, test loss:0.48181524486365773\n",
      "train loss:0.04052534221373751, test loss:0.4816933516116809\n",
      "train loss:0.04048296229933827, test loss:0.48156873344687473\n",
      "train loss:0.04044064434846718, test loss:0.48144702092411606\n",
      "train loss:0.04039836920514609, test loss:0.4813323177521504\n",
      "train loss:0.04035616418505054, test loss:0.4812057423413725\n",
      "train loss:0.04027190872829357, test loss:0.48095486546098387\n",
      "train loss:0.040229894894489464, test loss:0.4808348151762434\n",
      "train loss:0.040187913077722015, test loss:0.4807082354714432\n",
      "train loss:0.04014599273770042, test loss:0.48057914529699297\n",
      "train loss:0.04010412578674199, test loss:0.4804579614625452\n",
      "train loss:0.04006233805711654, test loss:0.48033114245624187\n",
      "train loss:0.0400205909016401, test loss:0.48020699686934876\n",
      "train loss:0.03997890630331282, test loss:0.48008165303430056\n",
      "train loss:0.03993727036475808, test loss:0.47994978601590316\n",
      "train loss:0.03985414418795925, test loss:0.47971073735193187\n",
      "train loss:0.03981267537109091, test loss:0.47958196398598396\n",
      "train loss:0.039771302297243336, test loss:0.4794484199242218\n",
      "train loss:0.0397299510007993, test loss:0.47933248347618557\n",
      "train loss:0.03968864912768344, test loss:0.47921294296026384\n",
      "train loss:0.039647430181962026, test loss:0.47908030106430005\n",
      "train loss:0.03960624213642941, test loss:0.47895512410212937\n",
      "train loss:0.039565127087173114, test loss:0.4788463408433954\n",
      "train loss:0.039524061638490815, test loss:0.4787238911098917\n",
      "train loss:0.0394421023201959, test loss:0.4784905100374758\n",
      "train loss:0.039401208703263274, test loss:0.4783769524173758\n",
      "train loss:0.03936035752118513, test loss:0.47824898177816777\n",
      "train loss:0.03931956442354808, test loss:0.47813337735922595\n",
      "train loss:0.03927882946857275, test loss:0.47802195013293347\n",
      "train loss:0.03923813311186158, test loss:0.47790524163126147\n",
      "train loss:0.039197513377012855, test loss:0.4777887329562381\n",
      "train loss:0.03915694398846164, test loss:0.47768188917546217\n",
      "train loss:0.03911642045881397, test loss:0.4775682672267974\n",
      "train loss:0.039035520015510895, test loss:0.4773531274483772\n",
      "train loss:0.03899515602354645, test loss:0.47724619268292306\n",
      "train loss:0.038954837309359044, test loss:0.4771388611447871\n",
      "train loss:0.038914579805343415, test loss:0.477031907280391\n",
      "train loss:0.03887436894829374, test loss:0.4769251574802092\n",
      "train loss:0.03883420139506856, test loss:0.4768105469049307\n",
      "train loss:0.03879409367398559, test loss:0.4766947769589094\n",
      "train loss:0.038754047989499586, test loss:0.4765852234989361\n",
      "train loss:0.03871403250001701, test loss:0.47647111768272055\n",
      "train loss:0.03863417387517572, test loss:0.4762324244381096\n",
      "train loss:0.03859432326364629, test loss:0.47611961329833546\n",
      "train loss:0.03855452271052114, test loss:0.4759952939170634\n",
      "train loss:0.038514778752526915, test loss:0.4758788525516118\n",
      "train loss:0.038475064416475746, test loss:0.4757710712732899\n",
      "train loss:0.03843539736629936, test loss:0.4756529709360428\n",
      "train loss:0.03839581045580563, test loss:0.4755281332067191\n",
      "train loss:0.03835627103801406, test loss:0.47541954425676536\n",
      "train loss:0.0383167677730674, test loss:0.4753009853277235\n",
      "train loss:0.03823792520839162, test loss:0.47506240370674435\n",
      "train loss:0.03819857260203336, test loss:0.4749518757693868\n",
      "train loss:0.03815926146591999, test loss:0.4748251111697826\n",
      "train loss:0.038120038340604435, test loss:0.4747044534679688\n",
      "train loss:0.038080848839833766, test loss:0.47458985651309904\n",
      "train loss:0.03804171905872768, test loss:0.47447354174151557\n",
      "train loss:0.03800262474170726, test loss:0.47435559115766984\n",
      "train loss:0.03796359999117791, test loss:0.4742453017046701\n",
      "train loss:0.03792462615987327, test loss:0.4741368193836213\n",
      "train loss:0.03784679907817608, test loss:0.47390919275122206\n",
      "train loss:0.03780798482529787, test loss:0.47379943766083715\n",
      "train loss:0.03776919799905998, test loss:0.47368614424756056\n",
      "train loss:0.03773046921118475, test loss:0.47357385019431214\n",
      "train loss:0.037691787468893755, test loss:0.47346147788914095\n",
      "train loss:0.037653165615530296, test loss:0.473345900142965\n",
      "train loss:0.03761458258008523, test loss:0.47323729601768616\n",
      "train loss:0.037576067651196095, test loss:0.47312597574318815\n",
      "train loss:0.037537590385993504, test loss:0.47301648599345125\n",
      "train loss:0.037460817201263354, test loss:0.47279258518039624\n",
      "train loss:0.0374224937350647, test loss:0.4726784295487002\n",
      "train loss:0.03738422927293359, test loss:0.472562589688718\n",
      "train loss:0.037346014004556234, test loss:0.4724464589339723\n",
      "train loss:0.03730788696088271, test loss:0.472327391644984\n",
      "train loss:0.037269774469459796, test loss:0.47220677260411253\n",
      "train loss:0.03723171301664427, test loss:0.4720908549201589\n",
      "train loss:0.037193696913167644, test loss:0.4719679484082172\n",
      "train loss:0.03715575392654166, test loss:0.4718499740347778\n",
      "train loss:0.0370799768061317, test loss:0.47160797018792977\n",
      "train loss:0.03704216977215745, test loss:0.47148280047515595\n",
      "train loss:0.037004404739906786, test loss:0.4713594237421704\n",
      "train loss:0.036966697760102314, test loss:0.471236768278849\n",
      "train loss:0.036929045055573366, test loss:0.4711120457400859\n",
      "train loss:0.036891448736333765, test loss:0.47098620521617574\n",
      "train loss:0.03685388348019224, test loss:0.47086594687296174\n",
      "train loss:0.0368163716457672, test loss:0.47074277926422403\n",
      "train loss:0.03677892752651869, test loss:0.4706229468017746\n",
      "train loss:0.03670418421769425, test loss:0.470372489029244\n",
      "train loss:0.036666873598740844, test loss:0.47025426988798114\n",
      "train loss:0.036629619825958123, test loss:0.47012992137050835\n",
      "train loss:0.03659240302395823, test loss:0.4700067518998467\n",
      "train loss:0.03655524068824868, test loss:0.4698836982264705\n",
      "train loss:0.036518120623001825, test loss:0.46976339843085607\n",
      "train loss:0.03648104469366096, test loss:0.4696399424938072\n",
      "train loss:0.036444025754580785, test loss:0.46952645373265384\n",
      "train loss:0.03640707032756818, test loss:0.46940767751281565\n",
      "train loss:0.03633326196481205, test loss:0.46917552022365466\n",
      "train loss:0.03629642932321692, test loss:0.4690663123858376\n",
      "train loss:0.03625964862891623, test loss:0.4689518503661268\n",
      "train loss:0.03622291810023923, test loss:0.46883647802809053\n",
      "train loss:0.036186236069920455, test loss:0.46872716526916564\n",
      "train loss:0.03614959839389145, test loss:0.46862157668796267\n",
      "train loss:0.036113019974018244, test loss:0.4685042309354778\n",
      "train loss:0.03607649911425332, test loss:0.46839102714442216\n",
      "train loss:0.03603999180334787, test loss:0.46827939015009534\n",
      "train loss:0.035967185167145636, test loss:0.4680518000987641\n",
      "train loss:0.035930832331042405, test loss:0.46794015510894316\n",
      "train loss:0.03589451992212204, test loss:0.4678275557497851\n",
      "train loss:0.03585828777037274, test loss:0.4677142027134584\n",
      "train loss:0.03582207283904092, test loss:0.4675977340405749\n",
      "train loss:0.035785910148290904, test loss:0.4674804814178829\n",
      "train loss:0.035749797546110795, test loss:0.4673738862032863\n",
      "train loss:0.03571373404324143, test loss:0.46726371082624757\n",
      "train loss:0.035677713965339225, test loss:0.4671533792431671\n",
      "train loss:0.035605830079937, test loss:0.46693639296939166\n",
      "train loss:0.0355699580800978, test loss:0.466824706919842\n",
      "train loss:0.035534116393798346, test loss:0.4667091717501134\n",
      "train loss:0.035498342475244415, test loss:0.4665931765720663\n",
      "train loss:0.035462599434759816, test loss:0.46647870380799294\n",
      "train loss:0.03542690419026201, test loss:0.4663629891121513\n",
      "train loss:0.03539126560607002, test loss:0.46625114087085323\n",
      "train loss:0.035355675412409936, test loss:0.4661359084982269\n",
      "train loss:0.03532012172404446, test loss:0.46602279744334285\n",
      "train loss:0.03524918057780457, test loss:0.46580085891558026\n",
      "train loss:0.03521376174296473, test loss:0.4656816853453715\n",
      "train loss:0.035178406006191806, test loss:0.4655738106375152\n",
      "train loss:0.035143100766586194, test loss:0.46545949658528923\n",
      "train loss:0.03510783850495935, test loss:0.4653351850809398\n",
      "train loss:0.035072601422941, test loss:0.46521747837605604\n",
      "train loss:0.035037443912111936, test loss:0.4651036391989679\n",
      "train loss:0.03500232301691478, test loss:0.4649900520823909\n",
      "train loss:0.03496724841314429, test loss:0.46487139361596236\n",
      "train loss:0.03489724197562955, test loss:0.46463763093554766\n",
      "train loss:0.03486228716850579, test loss:0.46452486133261883\n",
      "train loss:0.034827406313626505, test loss:0.46441332602229496\n",
      "train loss:0.03479256952357705, test loss:0.4642960604552547\n",
      "train loss:0.0347577879423782, test loss:0.46417954153192104\n",
      "train loss:0.03472303872535206, test loss:0.4640686028632558\n",
      "train loss:0.03468831902662889, test loss:0.46395882639031755\n",
      "train loss:0.034653662953509926, test loss:0.4638433824370161\n",
      "train loss:0.03461903733566824, test loss:0.4637287113387512\n",
      "train loss:0.03454995427497447, test loss:0.463513236166682\n",
      "train loss:0.03451545620431334, test loss:0.4634029462409996\n",
      "train loss:0.03448102605635076, test loss:0.46329984578897637\n",
      "train loss:0.034446637797684254, test loss:0.463194062737294\n",
      "train loss:0.03441228090098598, test loss:0.46308402569345625\n",
      "train loss:0.03437796855938466, test loss:0.46298340491972956\n",
      "train loss:0.0343437399054215, test loss:0.4628791338831209\n",
      "train loss:0.03430952747199312, test loss:0.4627682626113138\n",
      "train loss:0.034275358443077525, test loss:0.4626630921168305\n",
      "train loss:0.034207159450029556, test loss:0.46245813267498764\n",
      "train loss:0.03417312023345061, test loss:0.4623502037795587\n",
      "train loss:0.03413913503000767, test loss:0.4622382063609083\n",
      "train loss:0.03410518441524685, test loss:0.4621349627376826\n",
      "train loss:0.0340712741623713, test loss:0.4620246465902728\n",
      "train loss:0.034037409130906374, test loss:0.46190816145892255\n",
      "train loss:0.03400359921605973, test loss:0.4617970746615233\n",
      "train loss:0.03396983544837628, test loss:0.46168634070228154\n",
      "train loss:0.03393609735595422, test loss:0.4615745731401575\n",
      "train loss:0.03386876490556598, test loss:0.4613431355065421\n",
      "train loss:0.03383515020166676, test loss:0.4612301691713586\n",
      "train loss:0.03380158971242652, test loss:0.4611169223216154\n",
      "train loss:0.03376805621707703, test loss:0.46100339980707766\n",
      "train loss:0.033734572139303094, test loss:0.46088602446184035\n",
      "train loss:0.03370114447013251, test loss:0.4607724097911806\n",
      "train loss:0.03366772103309436, test loss:0.4606566137286841\n",
      "train loss:0.03363435622678442, test loss:0.4605379771784265\n",
      "train loss:0.033601046837185566, test loss:0.46042498845489294\n",
      "train loss:0.033534521880275336, test loss:0.4601962450795401\n",
      "train loss:0.03350134079204789, test loss:0.4600810530047507\n",
      "train loss:0.033468205152427835, test loss:0.45996722749232566\n",
      "train loss:0.033435100854532156, test loss:0.4598520607794148\n",
      "train loss:0.033402033805701996, test loss:0.45973152974689463\n",
      "train loss:0.033369010120293835, test loss:0.4596098595534816\n",
      "train loss:0.03333603366116981, test loss:0.4594915552409626\n",
      "train loss:0.0333031049551129, test loss:0.45937366996568424\n",
      "train loss:0.033270223658909415, test loss:0.459254829740319\n",
      "train loss:0.03320457625456918, test loss:0.45902012001083153\n",
      "train loss:0.03317180706261374, test loss:0.4588998336983751\n",
      "train loss:0.0331391013747179, test loss:0.45878712667822064\n",
      "train loss:0.033106407310425724, test loss:0.4586758173040751\n",
      "train loss:0.03307377498036917, test loss:0.458555909700695\n",
      "train loss:0.03304120118545003, test loss:0.4584378720387132\n",
      "train loss:0.033008643971204316, test loss:0.45832079131051\n",
      "train loss:0.0329761364539996, test loss:0.4582060857680726\n",
      "train loss:0.03294368425303733, test loss:0.4580906322665258\n",
      "train loss:0.03287887588767081, test loss:0.45786766607639784\n",
      "train loss:0.0328465452805237, test loss:0.4577525710362639\n",
      "train loss:0.03281424650053083, test loss:0.4576312867029507\n",
      "train loss:0.03278198635424747, test loss:0.4575188068914557\n",
      "train loss:0.032749760976265016, test loss:0.4574152803306234\n",
      "train loss:0.032717591142233556, test loss:0.457303313580571\n",
      "train loss:0.03268545713829143, test loss:0.45718650076624073\n",
      "train loss:0.03265335553282211, test loss:0.45708169862747944\n",
      "train loss:0.032621297749289645, test loss:0.45697584608309416\n",
      "train loss:0.03255729755258107, test loss:0.45675783657135016\n",
      "train loss:0.03252535626578379, test loss:0.4566562962672941\n",
      "train loss:0.03249345111814816, test loss:0.45655197072745973\n",
      "train loss:0.032461577573943204, test loss:0.4564443315157916\n",
      "train loss:0.03242977064055394, test loss:0.45633565277300425\n",
      "train loss:0.03239798102461334, test loss:0.45623159247959816\n",
      "train loss:0.032366233431886014, test loss:0.4561265570591437\n",
      "train loss:0.03233452486000167, test loss:0.4560213754248497\n",
      "train loss:0.0323028440171073, test loss:0.4559161448641836\n",
      "train loss:0.03223964162993151, test loss:0.45570426985499324\n",
      "train loss:0.03220810110522746, test loss:0.4556022526335777\n",
      "train loss:0.032176586787522214, test loss:0.45549340481609624\n",
      "train loss:0.03214513529069165, test loss:0.4553844757426201\n",
      "train loss:0.032113718903437005, test loss:0.45527925939344593\n",
      "train loss:0.03208233289466, test loss:0.4551743378157725\n",
      "train loss:0.03205098795790623, test loss:0.4550673831440366\n",
      "train loss:0.032019677756614705, test loss:0.454957715974601\n",
      "train loss:0.0319884246023547, test loss:0.4548588246335277\n",
      "train loss:0.03192598423643253, test loss:0.45464044323403113\n",
      "train loss:0.03189483624134351, test loss:0.4545354176758448\n",
      "train loss:0.031863713320364334, test loss:0.45442237156445264\n",
      "train loss:0.03183263397348957, test loss:0.4543134680329132\n",
      "train loss:0.03180158400701144, test loss:0.4542061061505194\n",
      "train loss:0.03177058251033818, test loss:0.45409421708859377\n",
      "train loss:0.03173961972753695, test loss:0.4539858322755133\n",
      "train loss:0.03170867396904071, test loss:0.4538713199057479\n",
      "train loss:0.03167777571999917, test loss:0.45375685842025226\n",
      "train loss:0.03161608784861425, test loss:0.4535419190316339\n",
      "train loss:0.03158529680877017, test loss:0.4534305899601145\n",
      "train loss:0.031554558572114394, test loss:0.4533212419260739\n",
      "train loss:0.031523838670985314, test loss:0.45321414472517096\n",
      "train loss:0.031493155758331795, test loss:0.4531061962096623\n",
      "train loss:0.03146252324335143, test loss:0.45299248383727\n",
      "train loss:0.031431934920218486, test loss:0.45287884290312974\n",
      "train loss:0.031401363231957066, test loss:0.4527698343958974\n",
      "train loss:0.031370851906703176, test loss:0.45264943670000585\n",
      "train loss:0.03130990183397088, test loss:0.45242588166428716\n",
      "train loss:0.03127950823355906, test loss:0.4523104232627959\n",
      "train loss:0.031249152632868767, test loss:0.4521935376036854\n",
      "train loss:0.031218819151102247, test loss:0.4520829400759366\n",
      "train loss:0.031188537508373613, test loss:0.4519726149355596\n",
      "train loss:0.03115828571336564, test loss:0.45185754289913327\n",
      "train loss:0.031128078752841005, test loss:0.4517454930920761\n",
      "train loss:0.031097915468886673, test loss:0.4516346298257105\n",
      "train loss:0.031067791121628777, test loss:0.45152679833270165\n",
      "train loss:0.031007656785413805, test loss:0.4513034970961284\n",
      "train loss:0.03097764757034294, test loss:0.4511945623645975\n",
      "train loss:0.030947691485224367, test loss:0.45108320789798884\n",
      "train loss:0.03091777541809692, test loss:0.4509683791216154\n",
      "train loss:0.03088788061122194, test loss:0.45086103092483026\n",
      "train loss:0.030858039666814163, test loss:0.450755642245099\n",
      "train loss:0.030828209179007613, test loss:0.45064749437018037\n",
      "train loss:0.030798451171323075, test loss:0.45054278809315207\n",
      "train loss:0.030768720217765905, test loss:0.4504400632868876\n",
      "train loss:0.030709358053330894, test loss:0.4502263063282999\n",
      "train loss:0.030679721376922684, test loss:0.45012186449010033\n",
      "train loss:0.030650130794395308, test loss:0.4500092853617085\n",
      "train loss:0.030620581997222507, test loss:0.44989734260356234\n",
      "train loss:0.030591071073520288, test loss:0.44980167719647746\n",
      "train loss:0.030561589285590236, test loss:0.44970531321500207\n",
      "train loss:0.030532162496436994, test loss:0.44959930168106327\n",
      "train loss:0.03050274387103239, test loss:0.44950240576211065\n",
      "train loss:0.030473379984834608, test loss:0.4494137032468923\n",
      "train loss:0.030414755585880693, test loss:0.4492121796088507\n",
      "train loss:0.030385493973374984, test loss:0.4491262188323194\n",
      "train loss:0.03035627719638392, test loss:0.4490334765784857\n",
      "train loss:0.03032711094264335, test loss:0.4489326530269722\n",
      "train loss:0.030297951224919524, test loss:0.44884752463964905\n",
      "train loss:0.030268836153639195, test loss:0.4487588422655995\n",
      "train loss:0.03023976354782395, test loss:0.44866527993440375\n",
      "train loss:0.03021073513829713, test loss:0.44857124458818\n",
      "train loss:0.030181735394562235, test loss:0.44847910296392324\n",
      "train loss:0.03012382575584174, test loss:0.4482858514264409\n",
      "train loss:0.030094942491518693, test loss:0.44818527006208586\n",
      "train loss:0.03006605669436717, test loss:0.4480885979274561\n",
      "train loss:0.030037255107782195, test loss:0.44798773764803596\n",
      "train loss:0.03000844822340135, test loss:0.4478849342424443\n",
      "train loss:0.029979688223219884, test loss:0.4477927743129146\n",
      "train loss:0.029950962646906106, test loss:0.44768955722725484\n",
      "train loss:0.029922289305478734, test loss:0.44759127609409455\n",
      "train loss:0.029893632497197457, test loss:0.4474966251866398\n",
      "train loss:0.029836426169579003, test loss:0.4473053320579716\n",
      "train loss:0.02980787221758274, test loss:0.4472083859364109\n",
      "train loss:0.029779368261346025, test loss:0.44711792454154187\n",
      "train loss:0.029750882632701557, test loss:0.4470243943210356\n",
      "train loss:0.02972245072122685, test loss:0.4469283867516994\n",
      "train loss:0.02969403857110337, test loss:0.446833453587031\n",
      "train loss:0.02966565921164249, test loss:0.4467495268987863\n",
      "train loss:0.029637321606950755, test loss:0.4466544250698364\n",
      "train loss:0.02960901925493742, test loss:0.446556329956344\n",
      "train loss:0.02955253218341056, test loss:0.44638270582314465\n",
      "train loss:0.029524336477195524, test loss:0.44628840566693356\n",
      "train loss:0.029496183643342992, test loss:0.4461963079451956\n",
      "train loss:0.029468073069835125, test loss:0.4461107953036975\n",
      "train loss:0.029439982957750875, test loss:0.44601707443638205\n",
      "train loss:0.029411915141354817, test loss:0.4459180693980725\n",
      "train loss:0.029383909576463908, test loss:0.44583041287618014\n",
      "train loss:0.029355930188897, test loss:0.4457426609132106\n",
      "train loss:0.02932799373435536, test loss:0.44565083733304933\n",
      "train loss:0.0292722011273661, test loss:0.44547040408808736\n",
      "train loss:0.029244369175130347, test loss:0.44538090581348316\n",
      "train loss:0.02921655743234796, test loss:0.44528802224371156\n",
      "train loss:0.029188776615641863, test loss:0.44519351926411277\n",
      "train loss:0.029161027204880403, test loss:0.44510045995737624\n",
      "train loss:0.029133314700304936, test loss:0.4450138636478095\n",
      "train loss:0.029105643135014035, test loss:0.4449266051116152\n",
      "train loss:0.029077991195259518, test loss:0.444837807011476\n",
      "train loss:0.02905038518220388, test loss:0.44475608957946167\n",
      "train loss:0.028995260721689134, test loss:0.44457272874658493\n",
      "train loss:0.028967733866565752, test loss:0.44448445303557793\n",
      "train loss:0.02894024255528997, test loss:0.4443951225163741\n",
      "train loss:0.028912797111273873, test loss:0.44430678006727065\n",
      "train loss:0.028885378807397975, test loss:0.4442142891171165\n",
      "train loss:0.028857994312641272, test loss:0.4441304789232959\n",
      "train loss:0.028830638030426525, test loss:0.44404144920534205\n",
      "train loss:0.02880332473603439, test loss:0.4439478213347826\n",
      "train loss:0.028776040160824575, test loss:0.44385521744851697\n",
      "train loss:0.02872157494737993, test loss:0.4436703936723353\n",
      "train loss:0.02869438462571602, test loss:0.4435772918878536\n",
      "train loss:0.02866723406537758, test loss:0.4434881438355417\n",
      "train loss:0.02864010745135045, test loss:0.4433999259722296\n",
      "train loss:0.028613009010883147, test loss:0.44330877919414424\n",
      "train loss:0.0285859519471339, test loss:0.4432130582941665\n",
      "train loss:0.02855893593374546, test loss:0.4431185391426748\n",
      "train loss:0.02853193994356849, test loss:0.4430209609185484\n",
      "train loss:0.0285049887567316, test loss:0.44292324012487955\n",
      "train loss:0.028451155024310018, test loss:0.44273805167128466\n",
      "train loss:0.028424291914214313, test loss:0.44263639170394875\n",
      "train loss:0.028397472471122995, test loss:0.44253827791925693\n",
      "train loss:0.028370654211604437, test loss:0.4424448195762519\n",
      "train loss:0.028343899103691474, test loss:0.4423447640082772\n",
      "train loss:0.028317166579937093, test loss:0.4422502596134477\n",
      "train loss:0.028290463785083892, test loss:0.44215535631823677\n",
      "train loss:0.028263783214266817, test loss:0.44205742579093926\n",
      "train loss:0.028237136629187788, test loss:0.4419610142835337\n",
      "train loss:0.028183964846441582, test loss:0.44177224086370004\n",
      "train loss:0.028157430379348862, test loss:0.4416744208707738\n",
      "train loss:0.028130908929905593, test loss:0.4415710306208256\n",
      "train loss:0.028104423574819716, test loss:0.4414779079166493\n",
      "train loss:0.02807798401754206, test loss:0.4413799454236593\n",
      "train loss:0.028051554297450324, test loss:0.4412787118000981\n",
      "train loss:0.028025169960287352, test loss:0.4411898854036678\n",
      "train loss:0.027998821053215342, test loss:0.44109728494833095\n",
      "train loss:0.027972488122855715, test loss:0.44099640995751743\n",
      "train loss:0.02791993898670183, test loss:0.44081149220721666\n",
      "train loss:0.027893714713108527, test loss:0.4407155979880449\n",
      "train loss:0.027867519553894247, test loss:0.4406216570452963\n",
      "train loss:0.027841339142236618, test loss:0.4405228045291959\n",
      "train loss:0.027815218504225338, test loss:0.44042646536755786\n",
      "train loss:0.027789117674939336, test loss:0.44033882556459647\n",
      "train loss:0.027763042718232375, test loss:0.4402392542425275\n",
      "train loss:0.027736997613834004, test loss:0.44014058380138765\n",
      "train loss:0.027710990895405158, test loss:0.44004975241558286\n",
      "train loss:0.02765906327550182, test loss:0.4398495647906224\n",
      "train loss:0.027633138901343866, test loss:0.43974911700546443\n",
      "train loss:0.027607250485941946, test loss:0.43965222171115914\n",
      "train loss:0.027581381053866063, test loss:0.4395515969350095\n",
      "train loss:0.027555549065730923, test loss:0.43944996955334503\n",
      "train loss:0.02752974602672975, test loss:0.4393528172816669\n",
      "train loss:0.027503976442082648, test loss:0.4392590089658816\n",
      "train loss:0.027478244197988087, test loss:0.4391620221030041\n",
      "train loss:0.027452534416248495, test loss:0.43906122623161403\n",
      "train loss:0.027401213348070552, test loss:0.4388722454168467\n",
      "train loss:0.027375595158321938, test loss:0.4387743834239397\n",
      "train loss:0.027350025302173783, test loss:0.43868558036175426\n",
      "train loss:0.02732445585494576, test loss:0.4385970830798555\n",
      "train loss:0.027298931008568016, test loss:0.438508017669891\n",
      "train loss:0.02727343845644991, test loss:0.4384168121801643\n",
      "train loss:0.027247986917289392, test loss:0.4383278791406248\n",
      "train loss:0.027222549956817863, test loss:0.43824763864442895\n",
      "train loss:0.02719714956740708, test loss:0.4381608889576193\n",
      "train loss:0.027146437828041606, test loss:0.43798424486598364\n",
      "train loss:0.027121117594171375, test loss:0.4378995039108824\n",
      "train loss:0.02709584711855874, test loss:0.4378076267640989\n",
      "train loss:0.027070591181280374, test loss:0.4377201624557718\n",
      "train loss:0.02704537458666952, test loss:0.43763601856368256\n",
      "train loss:0.02702018851253654, test loss:0.43755171520217473\n",
      "train loss:0.02699502811668077, test loss:0.4374616596263882\n",
      "train loss:0.02696991780502444, test loss:0.43737838936106216\n",
      "train loss:0.02694482413849212, test loss:0.43729723045904667\n",
      "train loss:0.026894726440235023, test loss:0.43712274282649227\n",
      "train loss:0.026869714229206688, test loss:0.4370375114223554\n",
      "train loss:0.02684475326697346, test loss:0.4369558473036348\n",
      "train loss:0.026819811535639584, test loss:0.4368677632690751\n",
      "train loss:0.02679489651429415, test loss:0.43678150779351965\n",
      "train loss:0.026770014286395354, test loss:0.43669887467288226\n",
      "train loss:0.026745161522024604, test loss:0.4366159052598282\n",
      "train loss:0.02672033945716965, test loss:0.43653586598460253\n",
      "train loss:0.02669553778792095, test loss:0.43645069612773435\n",
      "train loss:0.02664604069620503, test loss:0.43628026994439045\n",
      "train loss:0.026621326344922494, test loss:0.43619950126287116\n",
      "train loss:0.026596647564539874, test loss:0.4361137506173494\n",
      "train loss:0.02657199122510258, test loss:0.43602075982087524\n",
      "train loss:0.026547369787259353, test loss:0.43593559919643865\n",
      "train loss:0.026522781367127962, test loss:0.43585019646572454\n",
      "train loss:0.026498220439788816, test loss:0.4357573138386538\n",
      "train loss:0.026473684949431593, test loss:0.4356709361973407\n",
      "train loss:0.026449170355795002, test loss:0.4355839475760316\n",
      "train loss:0.026400227038430275, test loss:0.435412744151863\n",
      "train loss:0.026375794876483265, test loss:0.4353303685180918\n",
      "train loss:0.026351375105369192, test loss:0.43524490613261174\n",
      "train loss:0.02632700791273474, test loss:0.43516418074611907\n",
      "train loss:0.026302652827641698, test loss:0.435083763689066\n",
      "train loss:0.026278335289662818, test loss:0.4349952441340783\n",
      "train loss:0.026254038778227656, test loss:0.4349149042175361\n",
      "train loss:0.02622977227186581, test loss:0.43483155126453876\n",
      "train loss:0.026205535103605043, test loss:0.434749841105685\n",
      "train loss:0.026157140284867264, test loss:0.43458800646267015\n",
      "train loss:0.026132981890927703, test loss:0.4345098709580672\n",
      "train loss:0.026108865691395648, test loss:0.4344244771002358\n",
      "train loss:0.02608477001166274, test loss:0.4343363598072337\n",
      "train loss:0.026060714314462136, test loss:0.4342618755187078\n",
      "train loss:0.02603668524979642, test loss:0.4341799997358321\n",
      "train loss:0.02601267315503034, test loss:0.43409023801772345\n",
      "train loss:0.02598869073072314, test loss:0.43400805503370227\n",
      "train loss:0.02596474223678451, test loss:0.433934365725827\n",
      "train loss:0.02591692611521034, test loss:0.4337660729234797\n",
      "train loss:0.02589304213275517, test loss:0.4336850403241476\n",
      "train loss:0.02586919755081074, test loss:0.43360494738668137\n",
      "train loss:0.025845398704957716, test loss:0.4335169455211904\n",
      "train loss:0.02582160818145892, test loss:0.4334348887375455\n",
      "train loss:0.025797841259315134, test loss:0.433360664421546\n",
      "train loss:0.025774100288645893, test loss:0.43327481650639416\n",
      "train loss:0.025750395060795187, test loss:0.43318877753182544\n",
      "train loss:0.02572671566392985, test loss:0.43311415503026124\n",
      "train loss:0.025679439254237064, test loss:0.43295223484737955\n",
      "train loss:0.02565583040430098, test loss:0.4328721633123373\n",
      "train loss:0.025632264560926068, test loss:0.43279921529126564\n",
      "train loss:0.025608718870843816, test loss:0.43272084296722896\n",
      "train loss:0.02558520256780953, test loss:0.4326325085964989\n",
      "train loss:0.025561713579284207, test loss:0.43255268055950685\n",
      "train loss:0.025538252801299635, test loss:0.43247989035006307\n",
      "train loss:0.025514835848286604, test loss:0.43240355863242014\n",
      "train loss:0.025491429553897254, test loss:0.43231833492232624\n",
      "train loss:0.025444696891518917, test loss:0.432170058366848\n",
      "train loss:0.025421381389376642, test loss:0.43208959643689965\n",
      "train loss:0.025398092710350803, test loss:0.43200644611990807\n",
      "train loss:0.02537482814545328, test loss:0.4319320054532775\n",
      "train loss:0.025351579215902285, test loss:0.43185352101565105\n",
      "train loss:0.025328362248368694, test loss:0.4317711669807418\n",
      "train loss:0.025305185499525783, test loss:0.4316886860120295\n",
      "train loss:0.02528202345593115, test loss:0.43160925032905656\n",
      "train loss:0.025258895348600964, test loss:0.4315292514509188\n",
      "train loss:0.02521270895426973, test loss:0.43135761593160193\n",
      "train loss:0.02518964197810789, test loss:0.4312816221318175\n",
      "train loss:0.0251666249889886, test loss:0.4312040535331574\n",
      "train loss:0.025143613901204495, test loss:0.43112125956328795\n",
      "train loss:0.025120634338011816, test loss:0.43104481599608085\n",
      "train loss:0.02509768677661256, test loss:0.4309645011726571\n",
      "train loss:0.02507476311147576, test loss:0.4308844615454488\n",
      "train loss:0.025051859764603272, test loss:0.43080551054066146\n",
      "train loss:0.02502898890888429, test loss:0.4307279350534172\n",
      "train loss:0.024983327554180555, test loss:0.4305689186125235\n",
      "train loss:0.0249605387734857, test loss:0.4304939270582227\n",
      "train loss:0.024937768202568, test loss:0.43041765541918936\n",
      "train loss:0.024915035258161557, test loss:0.4303382236314373\n",
      "train loss:0.024892309926537022, test loss:0.43026210105450907\n",
      "train loss:0.0248696228921207, test loss:0.4301837237178824\n",
      "train loss:0.024846968846119602, test loss:0.4300969994544514\n",
      "train loss:0.024824323963732846, test loss:0.4300170940157211\n",
      "train loss:0.02480170013258356, test loss:0.42993479958762865\n",
      "train loss:0.024756548451663928, test loss:0.4297730129674151\n",
      "train loss:0.024733998580151254, test loss:0.4296971123406635\n",
      "train loss:0.024711487798149318, test loss:0.4296195219129322\n",
      "train loss:0.024689002187262263, test loss:0.4295354567695359\n",
      "train loss:0.024666539903792455, test loss:0.4294593085746481\n",
      "train loss:0.024644101650639124, test loss:0.42938294961136686\n",
      "train loss:0.024621698764578107, test loss:0.42930657393623534\n",
      "train loss:0.024599316062473044, test loss:0.4292254296455081\n",
      "train loss:0.024576953472825968, test loss:0.4291487214676436\n",
      "train loss:0.024532324245468903, test loss:0.42899938096260987\n",
      "train loss:0.024510046790036755, test loss:0.428925217427216\n",
      "train loss:0.024487795981855572, test loss:0.4288453377902986\n",
      "train loss:0.024465549214883486, test loss:0.42877642477261313\n",
      "train loss:0.02444334841311972, test loss:0.4287025724194799\n",
      "train loss:0.02442116248784166, test loss:0.42863229551209475\n",
      "train loss:0.024398999846081414, test loss:0.42855763313560374\n",
      "train loss:0.024376871541686166, test loss:0.4284871767366683\n",
      "train loss:0.024354761374828376, test loss:0.42841367238284284\n",
      "train loss:0.02431062472086056, test loss:0.42827328690203154\n",
      "train loss:0.024288589868917968, test loss:0.42820373409793544\n",
      "train loss:0.024266583574393274, test loss:0.4281317009055741\n",
      "train loss:0.024244608811955344, test loss:0.4280551075388725\n",
      "train loss:0.024222641609512523, test loss:0.4279788059049798\n",
      "train loss:0.02420070857488876, test loss:0.4279069913115477\n",
      "train loss:0.024178800289770475, test loss:0.4278262711901386\n",
      "train loss:0.024156918587771314, test loss:0.4277554377782676\n",
      "train loss:0.024135056269928748, test loss:0.42768092978899785\n",
      "train loss:0.024091404066980543, test loss:0.427523254686473\n",
      "train loss:0.024069612814142244, test loss:0.4274515583177436\n",
      "train loss:0.024047850358215498, test loss:0.42737225126811645\n",
      "train loss:0.02402610516739603, test loss:0.4272948214866183\n",
      "train loss:0.024004390479759358, test loss:0.42722757293773833\n",
      "train loss:0.023982705367498948, test loss:0.42715095704659634\n",
      "train loss:0.023961019885399608, test loss:0.42707162411469957\n",
      "train loss:0.02393936742605716, test loss:0.4269900540799977\n",
      "train loss:0.02391776068394405, test loss:0.4269122725705072\n",
      "train loss:0.023874578748431456, test loss:0.42675267410446177\n",
      "train loss:0.023853038749923037, test loss:0.4266669185606296\n",
      "train loss:0.023831507749482086, test loss:0.42659130117889993\n",
      "train loss:0.023810014117649655, test loss:0.4265193712001582\n",
      "train loss:0.023788531537360994, test loss:0.4264382865468151\n",
      "train loss:0.02376707647247328, test loss:0.4263615373421032\n",
      "train loss:0.023745635648792795, test loss:0.4262931103322576\n",
      "train loss:0.02372422124511961, test loss:0.4262136126497525\n",
      "train loss:0.023702835976327316, test loss:0.4261351309590589\n",
      "train loss:0.023660120737200075, test loss:0.4259888596281544\n",
      "train loss:0.02363881995975761, test loss:0.42591504981121964\n",
      "train loss:0.023617517270037736, test loss:0.4258420217886843\n",
      "train loss:0.023596249994671293, test loss:0.4257720543548492\n",
      "train loss:0.023575006751872737, test loss:0.4256966399959394\n",
      "train loss:0.023553781737310153, test loss:0.4256210000493868\n",
      "train loss:0.023532595485146815, test loss:0.4255489761331058\n",
      "train loss:0.02351142983581542, test loss:0.4254746419262788\n",
      "train loss:0.023490287933130682, test loss:0.42539968894977126\n",
      "train loss:0.023448078734571838, test loss:0.4252519973933049\n",
      "train loss:0.023427014639018275, test loss:0.4251831715439872\n",
      "train loss:0.023405973288726522, test loss:0.42510924761729957\n",
      "train loss:0.02338496414465451, test loss:0.42503782082886593\n",
      "train loss:0.023363969160991826, test loss:0.4249692649213492\n",
      "train loss:0.023343006500752796, test loss:0.4248951294988181\n",
      "train loss:0.023322061114539203, test loss:0.4248249647106001\n",
      "train loss:0.023301146293226563, test loss:0.4247506612892732\n",
      "train loss:0.023280254993947997, test loss:0.42467347640223274\n",
      "train loss:0.02323854370928051, test loss:0.4245286051858869\n",
      "train loss:0.023217740312517225, test loss:0.4244532810183417\n",
      "train loss:0.02319694201530392, test loss:0.42438244744222886\n",
      "train loss:0.02317616950804192, test loss:0.4243145929625111\n",
      "train loss:0.023155416444163794, test loss:0.4242397921535902\n",
      "train loss:0.023134694107690267, test loss:0.4241686606959993\n",
      "train loss:0.023113990677347297, test loss:0.4241101658208254\n",
      "train loss:0.023093310781731425, test loss:0.4240419145187978\n",
      "train loss:0.023072651111326392, test loss:0.4239650701133832\n",
      "train loss:0.023031405573032366, test loss:0.4238394680648601\n",
      "train loss:0.023010815791928017, test loss:0.42376714673666677\n",
      "train loss:0.02299025303759542, test loss:0.42370070468171356\n",
      "train loss:0.02296971270952702, test loss:0.42363935940549274\n",
      "train loss:0.022949198484578635, test loss:0.42357135319859224\n",
      "train loss:0.022928700493353757, test loss:0.42350107367135165\n",
      "train loss:0.022908210546136478, test loss:0.42343660765059227\n",
      "train loss:0.022887755473208957, test loss:0.4233716499104212\n",
      "train loss:0.022867321816768274, test loss:0.4233046462528255\n",
      "train loss:0.022826533549622004, test loss:0.42317342578055617\n",
      "train loss:0.022806161395931664, test loss:0.4231106465731142\n",
      "train loss:0.02278581117461449, test loss:0.42304213644704475\n",
      "train loss:0.022765489820988014, test loss:0.42297486896583614\n",
      "train loss:0.022745197112490005, test loss:0.4229131322484569\n",
      "train loss:0.02272491609571714, test loss:0.42284696139842715\n",
      "train loss:0.022704657097096192, test loss:0.42277628799524114\n",
      "train loss:0.02268442445024583, test loss:0.42271565204609474\n",
      "train loss:0.022664218172632717, test loss:0.4226521073633436\n",
      "train loss:0.022623873059669786, test loss:0.4225227185095672\n",
      "train loss:0.022603740794639244, test loss:0.42245922226174626\n",
      "train loss:0.022583630024088808, test loss:0.4223916190127194\n",
      "train loss:0.022563529628206354, test loss:0.422326336603684\n",
      "train loss:0.022543454349213492, test loss:0.4222641921878195\n",
      "train loss:0.02252340503203815, test loss:0.4221972789877049\n",
      "train loss:0.022503370752319212, test loss:0.42213413546856066\n",
      "train loss:0.02248336642843214, test loss:0.4220682967234584\n",
      "train loss:0.022463386614607492, test loss:0.42199587931336435\n",
      "train loss:0.022423487186060957, test loss:0.42186689180798553\n",
      "train loss:0.022403575969308497, test loss:0.4218002696380762\n",
      "train loss:0.02238368794170704, test loss:0.4217374960231476\n",
      "train loss:0.022363820767346044, test loss:0.4216765641925307\n",
      "train loss:0.02234396660962944, test loss:0.42160765888305657\n",
      "train loss:0.02232415063160591, test loss:0.42154093370988943\n",
      "train loss:0.022304351685098492, test loss:0.42148255735895374\n",
      "train loss:0.022284575030926738, test loss:0.42141815446737013\n",
      "train loss:0.02226481223453845, test loss:0.4213555255476269\n",
      "train loss:0.02222537112403431, test loss:0.4212337633392407\n",
      "train loss:0.022205679878483056, test loss:0.42116493102858654\n",
      "train loss:0.022186022544212383, test loss:0.42109857634860176\n",
      "train loss:0.022166363293658373, test loss:0.42103304101659633\n",
      "train loss:0.02214675270722465, test loss:0.42096710256567577\n",
      "train loss:0.022127144146938837, test loss:0.42089958473761496\n",
      "train loss:0.022107562771687767, test loss:0.4208342703319745\n",
      "train loss:0.022088008538071306, test loss:0.4207668307956645\n",
      "train loss:0.022068477875446423, test loss:0.42069870505718027\n",
      "train loss:0.02202947413491804, test loss:0.42056014534429736\n",
      "train loss:0.022010014080260085, test loss:0.42049188121908604\n",
      "train loss:0.021990564466371938, test loss:0.420427030242911\n",
      "train loss:0.02197113335998683, test loss:0.4203573695133648\n",
      "train loss:0.021951730005877995, test loss:0.42028860133726686\n",
      "train loss:0.021932349424831497, test loss:0.42022664947657384\n",
      "train loss:0.021912977654785854, test loss:0.42015865687046605\n",
      "train loss:0.021893634605858553, test loss:0.42008981404012297\n",
      "train loss:0.021874316291356728, test loss:0.42002400716985544\n",
      "train loss:0.021835744990489306, test loss:0.4198895403855634\n",
      "train loss:0.021816467387346168, test loss:0.41982251487862515\n",
      "train loss:0.021797242372807103, test loss:0.41975398382969964\n",
      "train loss:0.021778033594802428, test loss:0.41968780986232457\n",
      "train loss:0.021758838809862423, test loss:0.41962092163246906\n",
      "train loss:0.021739671563640914, test loss:0.41954991308273126\n",
      "train loss:0.021720522830075938, test loss:0.4194862524804706\n",
      "train loss:0.021701394997564877, test loss:0.4194226135814533\n",
      "train loss:0.021682300493663088, test loss:0.41935232945010903\n",
      "train loss:0.021644154590956987, test loss:0.4192207920402016\n",
      "train loss:0.021625113253262426, test loss:0.41915562370820564\n",
      "train loss:0.021606092022113443, test loss:0.41908466227476554\n",
      "train loss:0.021587094555037382, test loss:0.419019787571665\n",
      "train loss:0.02156811543468496, test loss:0.41895292547697016\n",
      "train loss:0.021549160071148953, test loss:0.4188850469384649\n",
      "train loss:0.02153022607582122, test loss:0.4188227719435031\n",
      "train loss:0.021511297030711137, test loss:0.4187530423777305\n",
      "train loss:0.021492419861205453, test loss:0.41868395637983025\n",
      "train loss:0.02145469598077767, test loss:0.41854192791868555\n",
      "train loss:0.02143585472859542, test loss:0.4184724621954365\n",
      "train loss:0.021417041413455754, test loss:0.418410942847207\n",
      "train loss:0.021398245447603804, test loss:0.41833316604599924\n",
      "train loss:0.02137947538831801, test loss:0.4182714380632083\n",
      "train loss:0.021360728922916287, test loss:0.4182061744224054\n",
      "train loss:0.021341992438354995, test loss:0.41812772100930246\n",
      "train loss:0.021323277716690507, test loss:0.41806289265091406\n",
      "train loss:0.021304588098202345, test loss:0.41800129626188814\n",
      "train loss:0.02126727243244754, test loss:0.41785266474833516\n",
      "train loss:0.021248644985181437, test loss:0.4177959262501385\n",
      "train loss:0.021230027560027273, test loss:0.41772025331001994\n",
      "train loss:0.021211432995905232, test loss:0.41764692368115197\n",
      "train loss:0.02119286002930145, test loss:0.4175876189679857\n",
      "train loss:0.021174297187315495, test loss:0.41751010703573244\n",
      "train loss:0.021155763621346652, test loss:0.41743326137376485\n",
      "train loss:0.021137244956989685, test loss:0.4173691332510137\n",
      "train loss:0.021118760329668567, test loss:0.4172902995289736\n",
      "train loss:0.021081818572592392, test loss:0.4171430509284695\n",
      "train loss:0.021063392621760532, test loss:0.4170656224863726\n",
      "train loss:0.02104497252622762, test loss:0.4169905887270477\n",
      "train loss:0.021026579158113103, test loss:0.4169217929437488\n",
      "train loss:0.02100820551895442, test loss:0.4168441735575415\n",
      "train loss:0.02098983647816652, test loss:0.41677080355847806\n",
      "train loss:0.020971511800725384, test loss:0.41669573225224654\n",
      "train loss:0.020953193990987672, test loss:0.41662381448733\n",
      "train loss:0.020934882912715266, test loss:0.41654382129128886\n",
      "train loss:0.020898353601141853, test loss:0.41639619971168623\n",
      "train loss:0.020880100566502392, test loss:0.41632263332952385\n",
      "train loss:0.020861896014374114, test loss:0.41625649917731106\n",
      "train loss:0.020843688470305932, test loss:0.41618080627504256\n",
      "train loss:0.020825510841438365, test loss:0.4161113381157211\n",
      "train loss:0.02080735964418685, test loss:0.41604804929276984\n",
      "train loss:0.020789199386811834, test loss:0.41597595702771506\n",
      "train loss:0.020771086212252356, test loss:0.41590533922991274\n",
      "train loss:0.02075298848158108, test loss:0.4158347038146566\n",
      "train loss:0.020716834720472834, test loss:0.4156916417265438\n",
      "train loss:0.020698794627807434, test loss:0.4156181872677356\n",
      "train loss:0.020680785860419645, test loss:0.41554584027580066\n",
      "train loss:0.020662782339196373, test loss:0.41548069633975265\n",
      "train loss:0.0206447987491827, test loss:0.4154143955976675\n",
      "train loss:0.02062683871623024, test loss:0.41534885679248723\n",
      "train loss:0.020608890544083096, test loss:0.41528557956759793\n",
      "train loss:0.02059097306509397, test loss:0.4152275894566516\n",
      "train loss:0.020573066119760446, test loss:0.41515079211551636\n",
      "train loss:0.020537321690210084, test loss:0.41503520440292013\n",
      "train loss:0.02051947631062032, test loss:0.41496324202358364\n",
      "train loss:0.020501651050508173, test loss:0.4148972464976171\n",
      "train loss:0.020483840397956556, test loss:0.41484219590913024\n",
      "train loss:0.02046606346925008, test loss:0.41477404948643526\n",
      "train loss:0.020448302890703034, test loss:0.4147020618155691\n",
      "train loss:0.020430546268185152, test loss:0.4146453338320477\n",
      "train loss:0.02041283126740494, test loss:0.4145796255359272\n",
      "train loss:0.02039512004022002, test loss:0.4145082881950797\n",
      "train loss:0.020359758682726412, test loss:0.4143774941839512\n",
      "train loss:0.02034212779404538, test loss:0.41430786019211246\n",
      "train loss:0.020324508578981104, test loss:0.4142406018081258\n",
      "train loss:0.020306894766975458, test loss:0.4141740341656665\n",
      "train loss:0.02028930539094672, test loss:0.41410218057392806\n",
      "train loss:0.020271731677738564, test loss:0.41403186276927656\n",
      "train loss:0.020254183983891404, test loss:0.41396625271452386\n",
      "train loss:0.02023665630819241, test loss:0.41390051243920456\n",
      "train loss:0.020219126488168194, test loss:0.4138322245172871\n",
      "train loss:0.02018415556912219, test loss:0.4136957085045801\n",
      "train loss:0.02016670166477785, test loss:0.4136268371681796\n",
      "train loss:0.02014925002735817, test loss:0.4135619657873719\n",
      "train loss:0.02013183417378683, test loss:0.4134906086662257\n",
      "train loss:0.02011442982817395, test loss:0.41342742422403284\n",
      "train loss:0.02009704686644391, test loss:0.41335949503442154\n",
      "train loss:0.020079680879436602, test loss:0.41329341686875387\n",
      "train loss:0.020062321918919986, test loss:0.413232794129543\n",
      "train loss:0.020044997036595902, test loss:0.41316226150589175\n",
      "train loss:0.020010382654584446, test loss:0.4130324936240738\n",
      "train loss:0.019993100864358588, test loss:0.4129630032443598\n",
      "train loss:0.0199758374523857, test loss:0.41290076618288\n",
      "train loss:0.01995859307861726, test loss:0.41283598906089075\n",
      "train loss:0.019941360898312326, test loss:0.41276890057708787\n",
      "train loss:0.019924161802427353, test loss:0.41270125806340774\n",
      "train loss:0.019906980475887973, test loss:0.41263515131736717\n",
      "train loss:0.01988981041410916, test loss:0.4125708885048572\n",
      "train loss:0.01987265849192037, test loss:0.41250507555011684\n",
      "train loss:0.019838414380655877, test loss:0.4123822676715065\n",
      "train loss:0.019821310727860694, test loss:0.41231507173497756\n",
      "train loss:0.019804237466852996, test loss:0.4122502782009272\n",
      "train loss:0.01978716610544888, test loss:0.41218886378393116\n",
      "train loss:0.01977012701712437, test loss:0.41212503958197577\n",
      "train loss:0.01975310085364434, test loss:0.4120633645116752\n",
      "train loss:0.019736095135526262, test loss:0.4120046724714402\n",
      "train loss:0.019719101999564947, test loss:0.4119410188958019\n",
      "train loss:0.01970212879401527, test loss:0.4118838142678986\n",
      "train loss:0.019668245524203442, test loss:0.41176979485310156\n",
      "train loss:0.01965132030555162, test loss:0.41170586626611594\n",
      "train loss:0.019634433807707005, test loss:0.4116512079892008\n",
      "train loss:0.01961754173792553, test loss:0.4115856457516537\n",
      "train loss:0.01960068082053044, test loss:0.41152545343485186\n",
      "train loss:0.019583841591222156, test loss:0.4114725949796799\n",
      "train loss:0.01956702223289902, test loss:0.4114089319432555\n",
      "train loss:0.01955021073610755, test loss:0.41134808334901257\n",
      "train loss:0.019533414550963777, test loss:0.4113024110549492\n",
      "train loss:0.019499891295358524, test loss:0.411171460396391\n",
      "train loss:0.019483154409037255, test loss:0.4111120766867693\n",
      "train loss:0.01946643775107993, test loss:0.4110601473284791\n",
      "train loss:0.01944973650516593, test loss:0.41099434830136566\n",
      "train loss:0.01943305559513461, test loss:0.4109285435821967\n",
      "train loss:0.019416383755936684, test loss:0.41087562799522287\n",
      "train loss:0.019399736951320423, test loss:0.41081882044349083\n",
      "train loss:0.0193831151329049, test loss:0.4107556808080111\n",
      "train loss:0.019366505842981552, test loss:0.4106942573888662\n",
      "train loss:0.019333314416884833, test loss:0.41057708549474997\n",
      "train loss:0.019316755697749044, test loss:0.41051245615963833\n",
      "train loss:0.019300221163789984, test loss:0.4104534174471307\n",
      "train loss:0.01928369120303781, test loss:0.4103934633185592\n",
      "train loss:0.01926718316802804, test loss:0.41032772771429904\n",
      "train loss:0.019250683153272323, test loss:0.41027194707141257\n",
      "train loss:0.019234215320190137, test loss:0.410209506053618\n",
      "train loss:0.01921775212414178, test loss:0.4101437032471032\n",
      "train loss:0.019201320517146084, test loss:0.4100866481309516\n",
      "train loss:0.019168487444975055, test loss:0.409956611107159\n",
      "train loss:0.01915210132833099, test loss:0.4099017749690364\n",
      "train loss:0.019135729554981027, test loss:0.4098340849950511\n",
      "train loss:0.019119366870856646, test loss:0.40976627122012954\n",
      "train loss:0.019103041814499733, test loss:0.40970635458416416\n",
      "train loss:0.019086714013170504, test loss:0.40964059769621886\n",
      "train loss:0.019070406394430913, test loss:0.409570653598786\n",
      "train loss:0.0190541254543112, test loss:0.4095148458078466\n",
      "train loss:0.01903785702679571, test loss:0.4094494632038844\n",
      "train loss:0.01900536933036749, test loss:0.4093210500947982\n",
      "train loss:0.018989148882827423, test loss:0.4092592409151416\n",
      "train loss:0.01897295108236622, test loss:0.40918707046298486\n",
      "train loss:0.018956759788497694, test loss:0.4091239190509328\n",
      "train loss:0.018940581225665783, test loss:0.4090600530560596\n",
      "train loss:0.018924435810137297, test loss:0.40899394753560736\n",
      "train loss:0.018908298102513533, test loss:0.40892966987476737\n",
      "train loss:0.018892177783145697, test loss:0.40886299024717415\n",
      "train loss:0.01887608552861385, test loss:0.4088049737583743\n",
      "train loss:0.01884391917544531, test loss:0.40867528313905027\n",
      "train loss:0.01882787456352641, test loss:0.40860975346110406\n",
      "train loss:0.018811839791185638, test loss:0.40854715938155745\n",
      "train loss:0.018795820303383406, test loss:0.40848074932114714\n",
      "train loss:0.0187798245798935, test loss:0.4084187515655896\n",
      "train loss:0.01876384582338396, test loss:0.4083530239785323\n",
      "train loss:0.018747874641372453, test loss:0.4082926306947046\n",
      "train loss:0.018731932921647196, test loss:0.40822947070433296\n",
      "train loss:0.018715999370275644, test loss:0.40816575361649227\n",
      "train loss:0.018684194830535218, test loss:0.4080388999908408\n",
      "train loss:0.01866831470721666, test loss:0.4079804482925295\n",
      "train loss:0.018652454575285, test loss:0.4079133562379621\n",
      "train loss:0.018636608674291826, test loss:0.40784873171650315\n",
      "train loss:0.018620783053190988, test loss:0.4077882893333849\n",
      "train loss:0.01860497063636126, test loss:0.40772633176493017\n",
      "train loss:0.01858917589873394, test loss:0.40766177561478584\n",
      "train loss:0.01857339898073005, test loss:0.4076058297184961\n",
      "train loss:0.018557643390141067, test loss:0.40754664697176957\n",
      "train loss:0.01852619402375079, test loss:0.40742479520500385\n",
      "train loss:0.018510483867985535, test loss:0.4073617975943915\n",
      "train loss:0.01849480816389648, test loss:0.4073030244939805\n",
      "train loss:0.018479145395883245, test loss:0.4072432407503825\n",
      "train loss:0.018463479011799257, test loss:0.40717928218862837\n",
      "train loss:0.01844784370243076, test loss:0.40711592306477046\n",
      "train loss:0.018432227161887027, test loss:0.4070530375293466\n",
      "train loss:0.018416622110570373, test loss:0.40698991285062025\n",
      "train loss:0.018401039390604227, test loss:0.40692189639576304\n",
      "train loss:0.018369917153972286, test loss:0.40679682534260714\n",
      "train loss:0.0183543783396399, test loss:0.4067357727074326\n",
      "train loss:0.018338852823178556, test loss:0.40668053206106597\n",
      "train loss:0.01832334237417901, test loss:0.4066195447044792\n",
      "train loss:0.018307852141770487, test loss:0.4065593063181124\n",
      "train loss:0.01829238597143862, test loss:0.406508285534938\n",
      "train loss:0.018276925372652266, test loss:0.40644273533288877\n",
      "train loss:0.018261494048083405, test loss:0.4063849122351975\n",
      "train loss:0.018246053013672842, test loss:0.40633580194265867\n",
      "train loss:0.01821525684214079, test loss:0.40621435672849154\n",
      "train loss:0.018199884973188606, test loss:0.40616766790511444\n",
      "train loss:0.018184527258382308, test loss:0.40611465970855387\n",
      "train loss:0.01816918085850045, test loss:0.406050597558628\n",
      "train loss:0.01815385520784344, test loss:0.4059998689501133\n",
      "train loss:0.018138537285073485, test loss:0.4059515474374094\n",
      "train loss:0.01812323566572964, test loss:0.4058859829210426\n",
      "train loss:0.018107962356642714, test loss:0.40582544935707826\n",
      "train loss:0.01809269609810467, test loss:0.4057754752974881\n",
      "train loss:0.01806221170207218, test loss:0.4056503095151183\n",
      "train loss:0.01804698722933063, test loss:0.4056026519562355\n",
      "train loss:0.018031785665490414, test loss:0.40554719126235067\n",
      "train loss:0.018016594190503152, test loss:0.405483396063741\n",
      "train loss:0.018001422059510306, test loss:0.4054366364264944\n",
      "train loss:0.017986273549822823, test loss:0.40538254364502185\n",
      "train loss:0.017971125722923285, test loss:0.40532412297887266\n",
      "train loss:0.017956013507337828, test loss:0.4052720822568709\n",
      "train loss:0.01794089287710368, test loss:0.40521772933577244\n",
      "train loss:0.017910722567778207, test loss:0.4051096586602953\n",
      "train loss:0.017895662958130002, test loss:0.40506098655355044\n",
      "train loss:0.01788062304465264, test loss:0.40500689984248317\n",
      "train loss:0.017865586659860343, test loss:0.40494738443072625\n",
      "train loss:0.017850570354534166, test loss:0.40489359749387793\n",
      "train loss:0.01783556957134582, test loss:0.4048378566652775\n",
      "train loss:0.017820591176700255, test loss:0.4047795094639578\n",
      "train loss:0.01780562873624509, test loss:0.4047231849747152\n",
      "train loss:0.017790673239507303, test loss:0.4046743124809479\n",
      "train loss:0.017760815673653955, test loss:0.40455971558136467\n",
      "train loss:0.017745911438594365, test loss:0.404506105314434\n",
      "train loss:0.017731032924836836, test loss:0.4044457892601617\n",
      "train loss:0.017716171654780337, test loss:0.40438728475873087\n",
      "train loss:0.01770131729347563, test loss:0.40433102718801245\n",
      "train loss:0.017686471336649426, test loss:0.4042703681925769\n",
      "train loss:0.017671649476031143, test loss:0.4042163453931217\n",
      "train loss:0.017656844770368666, test loss:0.40415846546965556\n",
      "train loss:0.0176420506215493, test loss:0.4041013016606354\n",
      "train loss:0.017612507447732323, test loss:0.4039938189949179\n",
      "train loss:0.017597757688067466, test loss:0.40393771882178375\n",
      "train loss:0.01758302500102515, test loss:0.40388326769143984\n",
      "train loss:0.017568308454756144, test loss:0.40382534116953556\n",
      "train loss:0.017553605953519975, test loss:0.4037667663194157\n",
      "train loss:0.017538915339158915, test loss:0.4037143428641432\n",
      "train loss:0.01752424713575892, test loss:0.4036552746110295\n",
      "train loss:0.017509582883074755, test loss:0.4035971810903774\n",
      "train loss:0.017494939261951132, test loss:0.40354150310253417\n",
      "train loss:0.017465704816317235, test loss:0.4034304128736066\n",
      "train loss:0.017451109862113904, test loss:0.40337710351371886\n",
      "train loss:0.01743651590162367, test loss:0.40332211655738076\n",
      "train loss:0.017421953084147887, test loss:0.4032705454519897\n",
      "train loss:0.01740740454933146, test loss:0.4032157206266178\n",
      "train loss:0.017392870239958166, test loss:0.4031566064480353\n",
      "train loss:0.01737834527145311, test loss:0.4031001576501776\n",
      "train loss:0.017363845830255884, test loss:0.40304119812717826\n",
      "train loss:0.01734935619427384, test loss:0.40298388220120246\n",
      "train loss:0.01732042256568866, test loss:0.40287528900138386\n",
      "train loss:0.01730597604963062, test loss:0.4028142206485626\n",
      "train loss:0.017291550482890695, test loss:0.4027599508693922\n",
      "train loss:0.017277132178200715, test loss:0.40270403707278707\n",
      "train loss:0.017262731849884402, test loss:0.402647635060093\n",
      "train loss:0.01724835063646964, test loss:0.40258994105773177\n",
      "train loss:0.01723398854821231, test loss:0.4025296102680936\n",
      "train loss:0.01721961736874805, test loss:0.4024733773366711\n",
      "train loss:0.017205277807119054, test loss:0.4024142728187894\n",
      "train loss:0.017176647305160164, test loss:0.4023001817075226\n",
      "train loss:0.01716235024621344, test loss:0.402238811148599\n",
      "train loss:0.017148076956772295, test loss:0.4021839949804044\n",
      "train loss:0.017133812146357737, test loss:0.4021304039918628\n",
      "train loss:0.01711956961255911, test loss:0.4020734559077243\n",
      "train loss:0.01710533638130471, test loss:0.4020164353965837\n",
      "train loss:0.017091118377302043, test loss:0.4019681588872688\n",
      "train loss:0.01707691959716989, test loss:0.4019140701390204\n",
      "train loss:0.017062735321190202, test loss:0.4018583640539292\n",
      "train loss:0.017034394421298366, test loss:0.40175501500233496\n",
      "train loss:0.01702025124755612, test loss:0.4017005759611235\n",
      "train loss:0.017006117916623775, test loss:0.4016516159596212\n",
      "train loss:0.016992004361020437, test loss:0.40159589657213224\n",
      "train loss:0.01697790121998072, test loss:0.4015469327164203\n",
      "train loss:0.016963807120055057, test loss:0.4014990732522694\n",
      "train loss:0.016949722720419835, test loss:0.40144432215013703\n",
      "train loss:0.016935670590375672, test loss:0.40139704353537975\n",
      "train loss:0.016921615344745455, test loss:0.4013520782689923\n",
      "train loss:0.016893570308152178, test loss:0.4012485479112374\n",
      "train loss:0.016879566808846898, test loss:0.4012018187692892\n",
      "train loss:0.01686557662211227, test loss:0.40114814140341015\n",
      "train loss:0.01685161023794709, test loss:0.40109829842237427\n",
      "train loss:0.01683764205492907, test loss:0.40105425723955596\n",
      "train loss:0.016823696616298487, test loss:0.4009975255695698\n",
      "train loss:0.016809763484717903, test loss:0.40094395184664244\n",
      "train loss:0.01679584496733684, test loss:0.400896494943659\n",
      "train loss:0.016781942560982036, test loss:0.40083902168589924\n",
      "train loss:0.01675417700902463, test loss:0.4007340047160143\n",
      "train loss:0.016740319436417614, test loss:0.4006757677371493\n",
      "train loss:0.016726470074137894, test loss:0.40062368864677417\n",
      "train loss:0.01671263700214338, test loss:0.4005827272670463\n",
      "train loss:0.0166988148445592, test loss:0.4005263910581236\n",
      "train loss:0.016684997583815035, test loss:0.4004743963073331\n",
      "train loss:0.016671220820396203, test loss:0.4004266527199042\n",
      "train loss:0.016657438878428226, test loss:0.40037310948841\n",
      "train loss:0.016643680971230798, test loss:0.40031709336645616\n",
      "train loss:0.016616195965500923, test loss:0.4002203830442985\n",
      "train loss:0.016602476894507202, test loss:0.40016426268600375\n",
      "train loss:0.016588780348897516, test loss:0.40011955969250884\n",
      "train loss:0.016575082659838047, test loss:0.4000646417556421\n",
      "train loss:0.016561397844422846, test loss:0.4000106729119231\n",
      "train loss:0.01654773878555185, test loss:0.3999641602562281\n",
      "train loss:0.01653408849068389, test loss:0.3999057536269837\n",
      "train loss:0.016520456404191912, test loss:0.39985382238903755\n",
      "train loss:0.01650683685788601, test loss:0.3998083215272062\n",
      "train loss:0.016479626723338105, test loss:0.3996943813781694\n",
      "train loss:0.01646605098528132, test loss:0.3996511659564255\n",
      "train loss:0.016452488391757503, test loss:0.399588906181045\n",
      "train loss:0.016438929299981687, test loss:0.39953798423080206\n",
      "train loss:0.016425389539023654, test loss:0.39949188722904944\n",
      "train loss:0.016411866814741112, test loss:0.3994358314812323\n",
      "train loss:0.016398353031162775, test loss:0.3993872253769563\n",
      "train loss:0.01638485797704492, test loss:0.3993386077855771\n",
      "train loss:0.016371359922285254, test loss:0.3992883416853887\n",
      "train loss:0.016344425155928217, test loss:0.3991895769167555\n",
      "train loss:0.016330983067971434, test loss:0.39914251052821703\n",
      "train loss:0.016317551723104538, test loss:0.39909087400627685\n",
      "train loss:0.016304128581108787, test loss:0.39904408766476984\n",
      "train loss:0.016290718050682204, test loss:0.3990005804240167\n",
      "train loss:0.016277332937805995, test loss:0.3989465374663133\n",
      "train loss:0.016263947302597156, test loss:0.3989032187698144\n",
      "train loss:0.016250582495892167, test loss:0.39885630520755716\n",
      "train loss:0.01623723107749487, test loss:0.3988028739455614\n",
      "train loss:0.01621055805509207, test loss:0.3987128198156934\n",
      "train loss:0.0161972494607834, test loss:0.39866475027414944\n",
      "train loss:0.01618395090434389, test loss:0.3986175577460095\n",
      "train loss:0.01617066601826094, test loss:0.3985696354362559\n",
      "train loss:0.016157392694511, test loss:0.39851822309039053\n",
      "train loss:0.016144129601829538, test loss:0.3984724850530469\n",
      "train loss:0.01613087521706412, test loss:0.39841979034823255\n",
      "train loss:0.016117648355861006, test loss:0.39836689492023614\n",
      "train loss:0.01610442960725877, test loss:0.39831979521678657\n",
      "train loss:0.016078038533894872, test loss:0.3982011607309214\n",
      "train loss:0.016064854280734696, test loss:0.39815327921653976\n",
      "train loss:0.016051681513480878, test loss:0.39810027863477304\n",
      "train loss:0.016038533557527084, test loss:0.39803862520913624\n",
      "train loss:0.01602539964073798, test loss:0.3979847175565842\n",
      "train loss:0.016012273433360855, test loss:0.397935352228132\n",
      "train loss:0.01599916042180856, test loss:0.39787663773059484\n",
      "train loss:0.015986068623101805, test loss:0.39782243204525947\n",
      "train loss:0.0159729803382744, test loss:0.397769537728267\n",
      "train loss:0.015946854901457922, test loss:0.3976557969364539\n",
      "train loss:0.015933814104322968, test loss:0.39760112034097955\n",
      "train loss:0.01592077563966539, test loss:0.3975443913252823\n",
      "train loss:0.015907763736463906, test loss:0.3974848229430342\n",
      "train loss:0.015894755242802077, test loss:0.3974277770164185\n",
      "train loss:0.0158817575683514, test loss:0.39736892653668876\n",
      "train loss:0.015868780699472975, test loss:0.3973069166861086\n",
      "train loss:0.015855813454876584, test loss:0.3972515073562935\n",
      "train loss:0.015842860817781433, test loss:0.3971912677353918\n",
      "train loss:0.015816982006149716, test loss:0.3970711687643861\n",
      "train loss:0.01580406819140547, test loss:0.3970125396055073\n",
      "train loss:0.015791164980114004, test loss:0.3969530184238269\n",
      "train loss:0.01577826619766376, test loss:0.39689469880670436\n",
      "train loss:0.0157653867557017, test loss:0.39683351352418683\n",
      "train loss:0.015752516140077655, test loss:0.3967757537471197\n",
      "train loss:0.0157396647009145, test loss:0.39671621719837996\n",
      "train loss:0.015726829317554224, test loss:0.3966593118291825\n",
      "train loss:0.01571399101908029, test loss:0.3965977077193842\n",
      "train loss:0.015688376043552735, test loss:0.3964768450591921\n",
      "train loss:0.015675586871779185, test loss:0.3964175375225672\n",
      "train loss:0.015662811199828854, test loss:0.39636592910966734\n",
      "train loss:0.01565004362685055, test loss:0.39630552905664096\n",
      "train loss:0.01563728945224759, test loss:0.39624527237110063\n",
      "train loss:0.015624554743019032, test loss:0.39619182032120626\n",
      "train loss:0.015611828112249818, test loss:0.3961321962797254\n",
      "train loss:0.01559910870965149, test loss:0.3960725087619657\n",
      "train loss:0.01558641073221194, test loss:0.39601134266869736\n",
      "train loss:0.015561043455076482, test loss:0.39589392808422796\n",
      "train loss:0.015548385356402445, test loss:0.39583619786993307\n",
      "train loss:0.015535733803564326, test loss:0.39577764249089137\n",
      "train loss:0.015523097557110246, test loss:0.3957238461818594\n",
      "train loss:0.015510464911743885, test loss:0.39566532793517223\n",
      "train loss:0.015497845165907277, test loss:0.3956049021272659\n",
      "train loss:0.015485243390069297, test loss:0.395554955320763\n",
      "train loss:0.015472656268550227, test loss:0.39549382264854677\n",
      "train loss:0.015460075884355483, test loss:0.3954380280357106\n",
      "train loss:0.015434948337121973, test loss:0.3953285660467453\n",
      "train loss:0.015422410591444405, test loss:0.39527028877068815\n",
      "train loss:0.015409886640697116, test loss:0.39521906037607746\n",
      "train loss:0.01539736683104438, test loss:0.3951657663048885\n",
      "train loss:0.015384860993029516, test loss:0.39510535503903416\n",
      "train loss:0.015372375720540562, test loss:0.39505197967564964\n",
      "train loss:0.015359899177466612, test loss:0.3950036605100738\n",
      "train loss:0.015347432372325559, test loss:0.39494970394042916\n",
      "train loss:0.01533498205821023, test loss:0.39489278958369584\n",
      "train loss:0.015310119780352334, test loss:0.3947932124115581\n",
      "train loss:0.015297710409425081, test loss:0.39473649557186535\n",
      "train loss:0.015285312951867108, test loss:0.3946819532353162\n",
      "train loss:0.015272932712435289, test loss:0.3946343727985995\n",
      "train loss:0.015260547554626523, test loss:0.3945839693502831\n",
      "train loss:0.015248194823342867, test loss:0.3945269817806817\n",
      "train loss:0.015235842592789149, test loss:0.3944727716772136\n",
      "train loss:0.01522351545364931, test loss:0.39442464787335846\n",
      "train loss:0.015211183606255604, test loss:0.39436281190977335\n",
      "train loss:0.015186577587881103, test loss:0.3942621523111589\n",
      "train loss:0.01517429401530793, test loss:0.3942114651006087\n",
      "train loss:0.015162017782203278, test loss:0.3941594066244751\n",
      "train loss:0.01514975731998441, test loss:0.39411218828004463\n",
      "train loss:0.01513750502215778, test loss:0.39405946073498943\n",
      "train loss:0.015125271450673808, test loss:0.3940087082334692\n",
      "train loss:0.015113038177153816, test loss:0.39395714231625245\n",
      "train loss:0.015100833447473023, test loss:0.3939046197065752\n",
      "train loss:0.01508863793047516, test loss:0.393859383895572\n",
      "train loss:0.015064274833979721, test loss:0.39376211900833813\n",
      "train loss:0.015052108565413959, test loss:0.3937172025941968\n",
      "train loss:0.015039960530687908, test loss:0.39366796465563625\n",
      "train loss:0.015027822030706807, test loss:0.3936213430940195\n",
      "train loss:0.015015706893926369, test loss:0.3935738089135265\n",
      "train loss:0.01500358781790381, test loss:0.39352369433055867\n",
      "train loss:0.01499149385947609, test loss:0.3934803062816652\n",
      "train loss:0.014979408093414309, test loss:0.39343606094763156\n",
      "train loss:0.014967329718389313, test loss:0.39339001254044315\n",
      "train loss:0.014943207334027224, test loss:0.39329729149973675\n",
      "train loss:0.014931167241908377, test loss:0.3932468938329379\n",
      "train loss:0.014919132616433094, test loss:0.3932056149132869\n",
      "train loss:0.014907115526157528, test loss:0.3931549029300015\n",
      "train loss:0.014895104784123548, test loss:0.3931042526754125\n",
      "train loss:0.014883104734029469, test loss:0.3930634584313703\n",
      "train loss:0.014871127782666893, test loss:0.3930192063919372\n",
      "train loss:0.014859149284154619, test loss:0.39296829533398514\n",
      "train loss:0.014847182970150362, test loss:0.3929324349434569\n",
      "train loss:0.01482329083886176, test loss:0.392832933011675\n",
      "train loss:0.014811356017588118, test loss:0.3927935555455036\n",
      "train loss:0.014799448401587053, test loss:0.3927494734552816\n",
      "train loss:0.014787543218865938, test loss:0.39269412965521283\n",
      "train loss:0.014775638990176613, test loss:0.39265044216842626\n",
      "train loss:0.014763753259519933, test loss:0.3926092287396947\n",
      "train loss:0.014751884575181782, test loss:0.3925585504427743\n",
      "train loss:0.014740024227472311, test loss:0.3925127289660882\n",
      "train loss:0.014728159736438705, test loss:0.3924688451107076\n",
      "train loss:0.014704495924673957, test loss:0.3923802671591431\n",
      "train loss:0.014692674719710185, test loss:0.3923390343844221\n",
      "train loss:0.014680867713718375, test loss:0.3922965944749222\n",
      "train loss:0.01466907528227762, test loss:0.392256159647884\n",
      "train loss:0.01465729867404058, test loss:0.3922171941073862\n",
      "train loss:0.014645523246993943, test loss:0.3921709724058119\n",
      "train loss:0.014633755795052132, test loss:0.3921347749568621\n",
      "train loss:0.014622006528479952, test loss:0.39209536997026756\n",
      "train loss:0.014610267488861808, test loss:0.39205285642452237\n",
      "train loss:0.014586828760201896, test loss:0.3919723061320668\n",
      "train loss:0.014575128935547996, test loss:0.39193868991646486\n",
      "train loss:0.01456343961865382, test loss:0.3918950373079026\n",
      "train loss:0.014551759125083384, test loss:0.39185538489417826\n",
      "train loss:0.014540092849494826, test loss:0.39182244757230283\n",
      "train loss:0.014528432892452842, test loss:0.3917790900648319\n",
      "train loss:0.014516787086240062, test loss:0.3917366095780154\n",
      "train loss:0.014505144659187771, test loss:0.3916955028054663\n",
      "train loss:0.014493524509835876, test loss:0.3916493921119157\n",
      "train loss:0.01447030527400918, test loss:0.39156551808487183\n",
      "train loss:0.014458719891803785, test loss:0.3915196285969776\n",
      "train loss:0.014447143978380797, test loss:0.39147639525318423\n",
      "train loss:0.014435576367258344, test loss:0.39143321138249715\n",
      "train loss:0.01442401562983398, test loss:0.3913847737173726\n",
      "train loss:0.014412483972161275, test loss:0.3913421785756525\n",
      "train loss:0.01440095350784514, test loss:0.39128693279905863\n",
      "train loss:0.014389424921020957, test loss:0.391245549473856\n",
      "train loss:0.014377919722513273, test loss:0.3912011509415756\n",
      "train loss:0.014354943557549883, test loss:0.39110711758958083\n",
      "train loss:0.014343469727789286, test loss:0.3910618661915718\n",
      "train loss:0.014332011503747282, test loss:0.3910039112654122\n",
      "train loss:0.014320558899216044, test loss:0.39096376096961005\n",
      "train loss:0.014309120523165216, test loss:0.390926005516275\n",
      "train loss:0.014297697530320681, test loss:0.3908754849234692\n",
      "train loss:0.014286286347552229, test loss:0.39083322199482246\n",
      "train loss:0.014274873841550654, test loss:0.3907928844319183\n",
      "train loss:0.01426348105847471, test loss:0.39074777098458124\n",
      "train loss:0.014240717981168022, test loss:0.3906556134109297\n",
      "train loss:0.014229366224369521, test loss:0.39061629866349795\n",
      "train loss:0.014218015723279036, test loss:0.3905685118340617\n",
      "train loss:0.014206669883857437, test loss:0.39052452799286075\n",
      "train loss:0.01419533485402113, test loss:0.39048188196651107\n",
      "train loss:0.014184019975986738, test loss:0.3904378580851705\n",
      "train loss:0.014172714367644583, test loss:0.39039129452209115\n",
      "train loss:0.014161412612674094, test loss:0.39035690822358304\n",
      "train loss:0.014150129173617028, test loss:0.39031103766271685\n",
      "train loss:0.01412759587607545, test loss:0.3902240337849481\n",
      "train loss:0.014116339267389297, test loss:0.39017269199960936\n",
      "train loss:0.014105102827317988, test loss:0.3901301931034875\n",
      "train loss:0.014093871172630399, test loss:0.39009015516301176\n",
      "train loss:0.014082652673455685, test loss:0.39004588202407353\n",
      "train loss:0.01407144961334588, test loss:0.3900096585931265\n",
      "train loss:0.014060252168622198, test loss:0.3899590380387146\n",
      "train loss:0.01404906657454622, test loss:0.3899148503953604\n",
      "train loss:0.014037898850911975, test loss:0.3898729610159963\n",
      "train loss:0.014015580700667494, test loss:0.38977368942250373\n",
      "train loss:0.014004440666763106, test loss:0.3897324250671458\n",
      "train loss:0.013993317390170517, test loss:0.3896815799300358\n",
      "train loss:0.013982206582616114, test loss:0.38963707891586824\n",
      "train loss:0.013971091181605708, test loss:0.38959823023878287\n",
      "train loss:0.013959996579338467, test loss:0.3895484559681435\n",
      "train loss:0.013948905596432413, test loss:0.3895004499774566\n",
      "train loss:0.013937837102084974, test loss:0.38946505624051436\n",
      "train loss:0.013926775272342854, test loss:0.38941520357096165\n",
      "train loss:0.01390467986375301, test loss:0.38933554913655183\n",
      "train loss:0.013893645615628305, test loss:0.3892857188133609\n",
      "train loss:0.013882632523272553, test loss:0.38924172305051924\n",
      "train loss:0.013871627973584036, test loss:0.3892045604912089\n",
      "train loss:0.013860627391873777, test loss:0.38915306789750576\n",
      "train loss:0.013849647715319423, test loss:0.3891087491209882\n",
      "train loss:0.013838667920859542, test loss:0.3890704722592208\n",
      "train loss:0.013827710042342766, test loss:0.38902258386255706\n",
      "train loss:0.013816768569403687, test loss:0.3889825707705882\n",
      "train loss:0.01379489369687271, test loss:0.38889429803990727\n",
      "train loss:0.013783980527353419, test loss:0.38885556422824147\n",
      "train loss:0.01377306636684315, test loss:0.38881533126987683\n",
      "train loss:0.013762174368709345, test loss:0.3887704383563791\n",
      "train loss:0.013751282565493727, test loss:0.3887237407369664\n",
      "train loss:0.013740414779002418, test loss:0.3886915502536108\n",
      "train loss:0.013729544116447242, test loss:0.3886481859586631\n",
      "train loss:0.013718687422902071, test loss:0.3885981755491618\n",
      "train loss:0.013707847449608308, test loss:0.38856007345242094\n",
      "train loss:0.013686185711195158, test loss:0.3884763472018422\n",
      "train loss:0.013675359160766789, test loss:0.3884367669371308\n",
      "train loss:0.01366456247582613, test loss:0.38840124433360906\n",
      "train loss:0.013653763077306057, test loss:0.3883558376427848\n",
      "train loss:0.013642972978005926, test loss:0.38831965793597467\n",
      "train loss:0.013632201610523463, test loss:0.38828039068629727\n",
      "train loss:0.013621437509806895, test loss:0.38823256284429275\n",
      "train loss:0.013610670293866128, test loss:0.38819354466223616\n",
      "train loss:0.013599929820662512, test loss:0.38814913151755265\n",
      "train loss:0.01357846966418268, test loss:0.3880601133200692\n",
      "train loss:0.013567754014733778, test loss:0.38802231208973603\n",
      "train loss:0.013557062296077664, test loss:0.38797802766125694\n",
      "train loss:0.013546353572090946, test loss:0.3879356091569644\n",
      "train loss:0.013535674342124488, test loss:0.3878977837260894\n",
      "train loss:0.013524995338960996, test loss:0.3878528439942124\n",
      "train loss:0.013514337784870746, test loss:0.3878147966542\n",
      "train loss:0.013503686967517147, test loss:0.38777766115434714\n",
      "train loss:0.013493039433895166, test loss:0.38773081397806775\n",
      "train loss:0.013471781959115907, test loss:0.3876590608056802\n",
      "train loss:0.013461166313147447, test loss:0.3876157918353641\n",
      "train loss:0.013450557570308885, test loss:0.3875825072415138\n",
      "train loss:0.01343996829676086, test loss:0.3875461972077673\n",
      "train loss:0.013429389363824705, test loss:0.3875079921205458\n",
      "train loss:0.013418817375559508, test loss:0.3874788755599646\n",
      "train loss:0.013408249923328222, test loss:0.3874423120719913\n",
      "train loss:0.013397700910361245, test loss:0.387405754411852\n",
      "train loss:0.013387160540170693, test loss:0.3873708757843139\n",
      "train loss:0.013366099351624127, test loss:0.3872964099487313\n",
      "train loss:0.013355588406422696, test loss:0.38726198384582017\n",
      "train loss:0.01334508629187419, test loss:0.3872258830177649\n",
      "train loss:0.013334586461465955, test loss:0.38719388944462774\n",
      "train loss:0.013324106511079388, test loss:0.38715957012333907\n",
      "train loss:0.013313633141511384, test loss:0.38712552827274965\n",
      "train loss:0.01330317527872372, test loss:0.3870911086882859\n",
      "train loss:0.01329272644363427, test loss:0.38705361124405513\n",
      "train loss:0.013282286304291437, test loss:0.3870173115073848\n",
      "train loss:0.013261436084024802, test loss:0.38693898160480616\n",
      "train loss:0.013251025690690444, test loss:0.3869086071363483\n",
      "train loss:0.013240622445582481, test loss:0.38686641839584207\n",
      "train loss:0.013230235700209302, test loss:0.38682569669335803\n",
      "train loss:0.013219853915876843, test loss:0.38679459343110745\n",
      "train loss:0.013209486120524326, test loss:0.3867498793419442\n",
      "train loss:0.013199131144923328, test loss:0.38671207836774646\n",
      "train loss:0.01318876785220399, test loss:0.38667541509248515\n",
      "train loss:0.013178436890008652, test loss:0.3866363530563565\n",
      "train loss:0.013157782844312828, test loss:0.38656448637758745\n",
      "train loss:0.013147469468890031, test loss:0.3865264278478692\n",
      "train loss:0.013137173567264602, test loss:0.3864976649080911\n",
      "train loss:0.013126879979316683, test loss:0.3864603611619378\n",
      "train loss:0.01311659767605142, test loss:0.38642351667999575\n",
      "train loss:0.013106328472482576, test loss:0.3863912292671613\n",
      "train loss:0.013096068549000238, test loss:0.3863500444586706\n",
      "train loss:0.01308581877047807, test loss:0.38630932021363934\n",
      "train loss:0.013075584646236977, test loss:0.3862841806321589\n",
      "train loss:0.013055120056970406, test loss:0.3862048736206852\n",
      "train loss:0.013044912784583222, test loss:0.386176090387222\n",
      "train loss:0.013034713356341581, test loss:0.38613931760311304\n",
      "train loss:0.013024517925503762, test loss:0.38609389348099216\n",
      "train loss:0.013014338909945983, test loss:0.38606411644058347\n",
      "train loss:0.013004162822791897, test loss:0.38603334604564515\n",
      "train loss:0.01299399714355813, test loss:0.38599088650955093\n",
      "train loss:0.012983850884761795, test loss:0.3859625629885458\n",
      "train loss:0.012973711448898839, test loss:0.3859340233406877\n",
      "train loss:0.012953438795141442, test loss:0.3858621069723524\n",
      "train loss:0.012943320078133074, test loss:0.3858295749272938\n",
      "train loss:0.012933220032145355, test loss:0.3857910916220643\n",
      "train loss:0.012923122091223743, test loss:0.38575910211456055\n",
      "train loss:0.012913033310151745, test loss:0.385720836468766\n",
      "train loss:0.012902958497634153, test loss:0.3856792756891178\n",
      "train loss:0.012892887669207516, test loss:0.38564870672558993\n",
      "train loss:0.012882825211948988, test loss:0.3856135406094689\n",
      "train loss:0.01287277655669037, test loss:0.3855707213112872\n",
      "train loss:0.012852705218495682, test loss:0.38550251330661456\n",
      "train loss:0.012842690776026755, test loss:0.3854609214947348\n",
      "train loss:0.012832681836515522, test loss:0.3854267969348486\n",
      "train loss:0.012822673058218584, test loss:0.38539412696631603\n",
      "train loss:0.012812671902284847, test loss:0.38534980939965113\n",
      "train loss:0.012802695645051643, test loss:0.3853160844316602\n",
      "train loss:0.012792718665702845, test loss:0.3852808173769345\n",
      "train loss:0.01278276113603963, test loss:0.38523935638487977\n",
      "train loss:0.012772806801682888, test loss:0.38520675940944976\n",
      "train loss:0.01275291301701975, test loss:0.38513457595158596\n",
      "train loss:0.01274299131127787, test loss:0.38509991404108107\n",
      "train loss:0.01273307014166929, test loss:0.385064724324637\n",
      "train loss:0.012723161762601316, test loss:0.38503195859642303\n",
      "train loss:0.012713265045783522, test loss:0.3850005752896422\n",
      "train loss:0.012703367475649704, test loss:0.38496427098193586\n",
      "train loss:0.012693493617586725, test loss:0.38493287844914803\n",
      "train loss:0.012683617679258007, test loss:0.38489457675098365\n",
      "train loss:0.012673748235821266, test loss:0.3848653979974435\n",
      "train loss:0.012654050416676303, test loss:0.384788520476339\n",
      "train loss:0.01264421869600929, test loss:0.3847561014631495\n",
      "train loss:0.01263439975538935, test loss:0.38471812676644956\n",
      "train loss:0.012624579530943775, test loss:0.3846802726261195\n",
      "train loss:0.012614775456275536, test loss:0.38465151415620175\n",
      "train loss:0.012604975357429235, test loss:0.3846113791855548\n",
      "train loss:0.01259518410726153, test loss:0.38457472815508675\n",
      "train loss:0.012585404838914361, test loss:0.38454508039170227\n",
      "train loss:0.012575630407288034, test loss:0.38450842331127916\n",
      "train loss:0.012556118122273793, test loss:0.38443650179437827\n",
      "train loss:0.012546378711319276, test loss:0.3843979286091684\n",
      "train loss:0.012536637769334133, test loss:0.3843587630001987\n",
      "train loss:0.012526911422281312, test loss:0.3843237729568868\n",
      "train loss:0.012517195182548551, test loss:0.38428859560530193\n",
      "train loss:0.012507488544476695, test loss:0.3842487524674469\n",
      "train loss:0.012497787943040252, test loss:0.38421432319462423\n",
      "train loss:0.012488095726811225, test loss:0.3841728802399362\n",
      "train loss:0.012478421049972141, test loss:0.3841358350140668\n",
      "train loss:0.01245908477712318, test loss:0.3840585689716817\n",
      "train loss:0.012449426058056188, test loss:0.38401805230602176\n",
      "train loss:0.01243978060713467, test loss:0.38398506672171134\n",
      "train loss:0.012430146812198497, test loss:0.38394605744787325\n",
      "train loss:0.012420517636267009, test loss:0.3839066359274266\n",
      "train loss:0.012410905136738693, test loss:0.3838692585751211\n",
      "train loss:0.012401289921055271, test loss:0.3838277923861903\n",
      "train loss:0.012391696799656893, test loss:0.3837866100569312\n",
      "train loss:0.01238210524476346, test loss:0.3837504585986359\n",
      "train loss:0.012362946792401939, test loss:0.38366825837087437\n",
      "train loss:0.012353377642375608, test loss:0.3836287198592035\n",
      "train loss:0.01234382948527165, test loss:0.3835844359587856\n",
      "train loss:0.012334286816325226, test loss:0.3835448744462406\n",
      "train loss:0.012324744021161354, test loss:0.3835085569439481\n",
      "train loss:0.012315215067085064, test loss:0.3834662454288454\n",
      "train loss:0.012305697544181978, test loss:0.3834207719930413\n",
      "train loss:0.012296190291488675, test loss:0.38338521142859366\n",
      "train loss:0.012286687304915778, test loss:0.3833396179294452\n",
      "train loss:0.012267712712500017, test loss:0.3832656267434555\n",
      "train loss:0.012258243917280231, test loss:0.38322324360726595\n",
      "train loss:0.012248774981473289, test loss:0.38318619035100243\n",
      "train loss:0.012239312615813255, test loss:0.38314793701446986\n",
      "train loss:0.012229869965290048, test loss:0.38310809273519886\n",
      "train loss:0.012220432106238672, test loss:0.38307382865836315\n",
      "train loss:0.012211006223433424, test loss:0.3830290826683388\n",
      "train loss:0.012201591958267139, test loss:0.3829861671054776\n",
      "train loss:0.012192178813842894, test loss:0.3829526611194182\n",
      "train loss:0.01217338350205829, test loss:0.38286691892084196\n",
      "train loss:0.012163990152600945, test loss:0.3828281799175916\n",
      "train loss:0.012154617045494124, test loss:0.38278314756102577\n",
      "train loss:0.012145252275729093, test loss:0.38274175606748023\n",
      "train loss:0.012135896157261891, test loss:0.38270388324498317\n",
      "train loss:0.012126541512595432, test loss:0.38266249138376146\n",
      "train loss:0.012117201116712007, test loss:0.3826235489598202\n",
      "train loss:0.012107871325494565, test loss:0.38257842878720927\n",
      "train loss:0.012098541373778122, test loss:0.3825451715717181\n",
      "train loss:0.012079926800965637, test loss:0.3824662925708445\n",
      "train loss:0.012070614824247365, test loss:0.38243175848758076\n",
      "train loss:0.012061328394882859, test loss:0.38240143327241705\n",
      "train loss:0.012052048850186903, test loss:0.38236217482019363\n",
      "train loss:0.01204277386693292, test loss:0.3823196928947193\n",
      "train loss:0.012033516289627212, test loss:0.38229438396762083\n",
      "train loss:0.012024252323283243, test loss:0.3822606618445191\n",
      "train loss:0.012015009413500708, test loss:0.3822146778403607\n",
      "train loss:0.01200577057859384, test loss:0.38218189295576277\n",
      "train loss:0.011987316989305894, test loss:0.38210735885094466\n",
      "train loss:0.011978097449191883, test loss:0.38207313372840185\n",
      "train loss:0.01196889893590388, test loss:0.3820452412684787\n",
      "train loss:0.011959697837026415, test loss:0.3820092732427918\n",
      "train loss:0.01195051201913384, test loss:0.38197268516647437\n",
      "train loss:0.01194133599140851, test loss:0.38193909728604786\n",
      "train loss:0.011932164658138174, test loss:0.38190645573770654\n",
      "train loss:0.01192300000320209, test loss:0.3818678928295879\n",
      "train loss:0.01191384408905822, test loss:0.38183162416249433\n",
      "train loss:0.011895561763640601, test loss:0.38175849538009626\n",
      "train loss:0.011886442082131583, test loss:0.38173460486333194\n",
      "train loss:0.011877318070985403, test loss:0.3816989680756981\n",
      "train loss:0.011868212730921332, test loss:0.381657778945544\n",
      "train loss:0.011859106836408732, test loss:0.38162629501565926\n",
      "train loss:0.011850018757538761, test loss:0.3815910479066957\n",
      "train loss:0.011840936203788486, test loss:0.38154968588830385\n",
      "train loss:0.01183185795059097, test loss:0.3815203572087735\n",
      "train loss:0.011822798787659336, test loss:0.3814925506227889\n",
      "train loss:0.011804693077656468, test loss:0.3814214003222739\n",
      "train loss:0.011795647682927152, test loss:0.3813879856745415\n",
      "train loss:0.01178661389617962, test loss:0.38135237779300507\n",
      "train loss:0.011777592197189296, test loss:0.38131663073989763\n",
      "train loss:0.01176857149044086, test loss:0.3812811629468694\n",
      "train loss:0.01175956461207956, test loss:0.381242200175655\n",
      "train loss:0.011750566030535575, test loss:0.38120686648098345\n",
      "train loss:0.011741567112815758, test loss:0.3811721046728501\n",
      "train loss:0.011732583149815321, test loss:0.38113660374699365\n",
      "train loss:0.011714636309898942, test loss:0.3810641092188098\n",
      "train loss:0.011705667862464068, test loss:0.38102683377893215\n",
      "train loss:0.01169671612239956, test loss:0.38099076933515774\n",
      "train loss:0.011687774804910109, test loss:0.3809570744672953\n",
      "train loss:0.01167883000008143, test loss:0.3809172610757149\n",
      "train loss:0.011669904624849788, test loss:0.38088374873984276\n",
      "train loss:0.011660985503040466, test loss:0.3808499114998212\n",
      "train loss:0.011652078266519076, test loss:0.3808119110948215\n",
      "train loss:0.011643177410341174, test loss:0.3807812587797034\n",
      "train loss:0.011625398230600836, test loss:0.3807083155204577\n",
      "train loss:0.011616528831820854, test loss:0.38067760630062725\n",
      "train loss:0.011607662678710242, test loss:0.38064091625073054\n",
      "train loss:0.011598802321643889, test loss:0.38060677111527325\n",
      "train loss:0.011589952712415091, test loss:0.3805774754100546\n",
      "train loss:0.011581113516499738, test loss:0.38054242462199983\n",
      "train loss:0.011572272116369093, test loss:0.3805093892442957\n",
      "train loss:0.011563445557252904, test loss:0.38048017619843144\n",
      "train loss:0.01155462982201533, test loss:0.38044273755516683\n",
      "train loss:0.011537019382638654, test loss:0.38038644713327163\n",
      "train loss:0.011528226573482077, test loss:0.3803465628225634\n",
      "train loss:0.011519434071300773, test loss:0.3803168312455991\n",
      "train loss:0.011510663387821722, test loss:0.3802931004556768\n",
      "train loss:0.011501884823728008, test loss:0.38025057897713294\n",
      "train loss:0.011493125673828085, test loss:0.3802227185138428\n",
      "train loss:0.01148438033374239, test loss:0.3801990058059281\n",
      "train loss:0.011475629966699858, test loss:0.38015598211261614\n",
      "train loss:0.011466891891050827, test loss:0.3801269795566952\n",
      "train loss:0.011449442121795739, test loss:0.3800617116844992\n",
      "train loss:0.011440720827060512, test loss:0.38003207276160794\n",
      "train loss:0.011432018190442506, test loss:0.38000782983995324\n",
      "train loss:0.011423320096994923, test loss:0.3799739031683427\n",
      "train loss:0.011414634624981575, test loss:0.3799372212313751\n",
      "train loss:0.011405953850989255, test loss:0.3799127590042342\n",
      "train loss:0.011397274596350586, test loss:0.3798696906046549\n",
      "train loss:0.011388612226245586, test loss:0.3798413600074012\n",
      "train loss:0.011379956523176759, test loss:0.3798127061017909\n",
      "train loss:0.011362662426860924, test loss:0.3797504985429778\n",
      "train loss:0.011354033689761874, test loss:0.37972068294462513\n",
      "train loss:0.011345403724856892, test loss:0.3796882653219639\n",
      "train loss:0.011336789516747495, test loss:0.37966473491163566\n",
      "train loss:0.011328175730352604, test loss:0.3796343276885196\n",
      "train loss:0.01131958038373448, test loss:0.37960812091429075\n",
      "train loss:0.01131098025967537, test loss:0.3795806564748041\n",
      "train loss:0.01130240139194573, test loss:0.379552654396356\n",
      "train loss:0.01129381989091768, test loss:0.3795243425747441\n",
      "train loss:0.011276687469179574, test loss:0.3794671072295753\n",
      "train loss:0.011268133687423073, test loss:0.37944734196927116\n",
      "train loss:0.01125958658912586, test loss:0.37941758379333196\n",
      "train loss:0.01125104596371207, test loss:0.37938780193219085\n",
      "train loss:0.011242514286901258, test loss:0.3793657352224831\n",
      "train loss:0.011233993721048411, test loss:0.37933663698161213\n",
      "train loss:0.011225476400531613, test loss:0.3793033940028439\n",
      "train loss:0.011216966177497982, test loss:0.37928232744798845\n",
      "train loss:0.011208467984206293, test loss:0.3792597418182889\n",
      "train loss:0.011191491377146987, test loss:0.37920700208894725\n",
      "train loss:0.011183014832971943, test loss:0.37918573846481973\n",
      "train loss:0.011174552397103242, test loss:0.37915581363477113\n",
      "train loss:0.011166090842606119, test loss:0.37913811318362284\n",
      "train loss:0.011157638352489866, test loss:0.3791109159382256\n",
      "train loss:0.011149195606979705, test loss:0.37908746736833393\n",
      "train loss:0.01114076000326979, test loss:0.3790675923342483\n",
      "train loss:0.01113233278995637, test loss:0.3790379789439137\n",
      "train loss:0.011123909856687684, test loss:0.37900926510413335\n",
      "train loss:0.011107087475555014, test loss:0.3789618810635779\n",
      "train loss:0.011098692160380218, test loss:0.3789383066740983\n",
      "train loss:0.011090302294446507, test loss:0.37892123170538206\n",
      "train loss:0.011081924622507336, test loss:0.37888554482478215\n",
      "train loss:0.011073545394696856, test loss:0.37886229453514886\n",
      "train loss:0.011065179354443212, test loss:0.3788404306020381\n",
      "train loss:0.011056819918652105, test loss:0.37880695132578013\n",
      "train loss:0.0110484685726156, test loss:0.37878433697235064\n",
      "train loss:0.01104011976244432, test loss:0.37876262867103005\n",
      "train loss:0.011023446885002933, test loss:0.3787051416158921\n",
      "train loss:0.011015125415267137, test loss:0.37868393054580385\n",
      "train loss:0.011006800690556097, test loss:0.3786585000977379\n",
      "train loss:0.010998493451392406, test loss:0.37863100498486446\n",
      "train loss:0.01099018850019277, test loss:0.3786132005725275\n",
      "train loss:0.010981893095971244, test loss:0.37857877041147836\n",
      "train loss:0.010973606685350945, test loss:0.37855393084157335\n",
      "train loss:0.010965326621034948, test loss:0.3785359309304702\n",
      "train loss:0.010957043191293637, test loss:0.37850828340491466\n",
      "train loss:0.010940523907660907, test loss:0.37846627303752006\n",
      "train loss:0.010932269757345589, test loss:0.3784418358920751\n",
      "train loss:0.010924025624896581, test loss:0.3784156602375961\n",
      "train loss:0.010915783084170818, test loss:0.37839389809366725\n",
      "train loss:0.010907547684902664, test loss:0.3783710896974163\n",
      "train loss:0.010899332158705158, test loss:0.37834567312282075\n",
      "train loss:0.010891117599721643, test loss:0.37832548878775246\n",
      "train loss:0.010882898582673231, test loss:0.37830256635232223\n",
      "train loss:0.01087470317925489, test loss:0.3782731818943003\n",
      "train loss:0.010858322909784623, test loss:0.3782293143925524\n",
      "train loss:0.010850144008413404, test loss:0.37820081122054544\n",
      "train loss:0.01084196672907254, test loss:0.37818713518940594\n",
      "train loss:0.01083379534389068, test loss:0.3781609080759457\n",
      "train loss:0.010825641698377223, test loss:0.3781340896919989\n",
      "train loss:0.010817492316280075, test loss:0.3781184680334244\n",
      "train loss:0.010809347288426668, test loss:0.37810007679547664\n",
      "train loss:0.010801213251169716, test loss:0.37807079936556903\n",
      "train loss:0.010793082198279205, test loss:0.37805347203461753\n",
      "train loss:0.010776847777144537, test loss:0.37801253681883895\n",
      "train loss:0.010768737868988451, test loss:0.3779883717233089\n",
      "train loss:0.010760638378883254, test loss:0.3779726280062054\n",
      "train loss:0.010752550674540935, test loss:0.37795015417874783\n",
      "train loss:0.010744461444163413, test loss:0.37792245928862805\n",
      "train loss:0.010736392870568795, test loss:0.37790657964969515\n",
      "train loss:0.0107283224220082, test loss:0.3778863715816077\n",
      "train loss:0.01072025231155423, test loss:0.37785635139427787\n",
      "train loss:0.010712202043005754, test loss:0.3778384150768158\n",
      "train loss:0.01069611082516987, test loss:0.3777924220329552\n",
      "train loss:0.010688074355661678, test loss:0.3777740843358804\n",
      "train loss:0.010680048129797316, test loss:0.3777519563822098\n",
      "train loss:0.01067202615587297, test loss:0.3777305358211054\n",
      "train loss:0.010664012414545804, test loss:0.3777149774586759\n",
      "train loss:0.01065601233863368, test loss:0.3776905490078026\n",
      "train loss:0.010648013665199931, test loss:0.3776737776016147\n",
      "train loss:0.010640023165112011, test loss:0.37765928065471305\n",
      "train loss:0.010632035297597387, test loss:0.3776379433626997\n",
      "train loss:0.010616090167741272, test loss:0.3775986796950667\n",
      "train loss:0.010608126869101916, test loss:0.3775865223232514\n",
      "train loss:0.01060017289169251, test loss:0.377558490305617\n",
      "train loss:0.01059222620526979, test loss:0.37753734728165134\n",
      "train loss:0.010584285047234573, test loss:0.3775230310197493\n",
      "train loss:0.010576358522281568, test loss:0.37749463429288876\n",
      "train loss:0.010568423955039766, test loss:0.3774697782709534\n",
      "train loss:0.010560500969880008, test loss:0.37745619358838073\n",
      "train loss:0.01055258837517913, test loss:0.3774305127049578\n",
      "train loss:0.010536784649104857, test loss:0.3773832161933009\n",
      "train loss:0.010528890147229613, test loss:0.3773585601006218\n",
      "train loss:0.010521006392378037, test loss:0.37732759597185167\n",
      "train loss:0.010513128852997993, test loss:0.3772980477256836\n",
      "train loss:0.0105052545117364, test loss:0.37727259929959356\n",
      "train loss:0.01049738706817266, test loss:0.3772434986724327\n",
      "train loss:0.010489532116852097, test loss:0.37721241194052135\n",
      "train loss:0.010481678101451316, test loss:0.3771919728176756\n",
      "train loss:0.010473836327951591, test loss:0.3771629905864111\n",
      "train loss:0.010458168670870628, test loss:0.3771187596391343\n",
      "train loss:0.010450347723363923, test loss:0.3770894964923276\n",
      "train loss:0.01044253440679236, test loss:0.37706762693861173\n",
      "train loss:0.010434723493937398, test loss:0.37704527491628087\n",
      "train loss:0.010426921498633765, test loss:0.37701933688405337\n",
      "train loss:0.010419127801994586, test loss:0.3769916309852845\n",
      "train loss:0.010411341298816013, test loss:0.37697434160036664\n",
      "train loss:0.010403557257218282, test loss:0.3769488527698089\n",
      "train loss:0.010395784947879579, test loss:0.37692323376645015\n",
      "train loss:0.010380258134044536, test loss:0.3768770634838633\n",
      "train loss:0.010372504575090001, test loss:0.3768464675228881\n",
      "train loss:0.010364759950080241, test loss:0.3768198564541037\n",
      "train loss:0.01035701866532789, test loss:0.3768018706639065\n",
      "train loss:0.010349283924523027, test loss:0.37676944236176363\n",
      "train loss:0.010341559729670222, test loss:0.3767414431771486\n",
      "train loss:0.010333842221975543, test loss:0.37672651926322437\n",
      "train loss:0.01032612561935533, test loss:0.37669331310292886\n",
      "train loss:0.010318421122621275, test loss:0.37666543360147964\n",
      "train loss:0.010303036185347917, test loss:0.3766252119488667\n",
      "train loss:0.010295350553181435, test loss:0.3765918411304974\n",
      "train loss:0.010287678558418565, test loss:0.3765730680728916\n",
      "train loss:0.010280005434059927, test loss:0.3765481041025551\n",
      "train loss:0.01027234351108737, test loss:0.37651516473897406\n",
      "train loss:0.010264687844476372, test loss:0.37648903503467274\n",
      "train loss:0.010257042263328724, test loss:0.37647356011317684\n",
      "train loss:0.01024939915140034, test loss:0.3764420667019445\n",
      "train loss:0.010241765745744499, test loss:0.37641333160619783\n",
      "train loss:0.01022651355814826, test loss:0.37637159561864253\n",
      "train loss:0.010218904198019569, test loss:0.3763385125089405\n",
      "train loss:0.010211297025752514, test loss:0.3763133117759974\n",
      "train loss:0.010203697429910598, test loss:0.3762909717099168\n",
      "train loss:0.010196105023069309, test loss:0.3762595919630982\n",
      "train loss:0.010188524313704813, test loss:0.376235080305132\n",
      "train loss:0.010180945997916663, test loss:0.3762130457928542\n",
      "train loss:0.010173372182635552, test loss:0.3761848801410757\n",
      "train loss:0.010165806376303658, test loss:0.37616547057419325\n",
      "train loss:0.010150701112104168, test loss:0.376111703995591\n",
      "train loss:0.010143157797070325, test loss:0.37608673253512964\n",
      "train loss:0.010135623561629063, test loss:0.3760579806653296\n",
      "train loss:0.010128091503767108, test loss:0.37603291531637606\n",
      "train loss:0.01012056528064632, test loss:0.3760092452953685\n",
      "train loss:0.010113050857586237, test loss:0.37597399187617414\n",
      "train loss:0.010105535622456906, test loss:0.37595333167443207\n",
      "train loss:0.010098031568883255, test loss:0.37593062799921373\n",
      "train loss:0.010090539762755623, test loss:0.37589687467489785\n",
      "train loss:0.010075555018926834, test loss:0.3758452063566771\n",
      "train loss:0.010068078248590401, test loss:0.37581409828604156\n",
      "train loss:0.010060601020419288, test loss:0.3757873861984817\n",
      "train loss:0.010053135525597138, test loss:0.3757579073387693\n",
      "train loss:0.01004567831493803, test loss:0.37571840924862315\n",
      "train loss:0.010038222119210696, test loss:0.3756976271490774\n",
      "train loss:0.010030775380767994, test loss:0.37566932936873043\n",
      "train loss:0.010023336794394622, test loss:0.375634736972588\n",
      "train loss:0.010015897246753608, test loss:0.3756135866782879\n",
      "train loss:0.010001054361250115, test loss:0.37555636539135734\n",
      "train loss:0.009993639116804528, test loss:0.37553131305339327\n",
      "train loss:0.009986230508472334, test loss:0.3755032450742773\n",
      "train loss:0.009978832500855923, test loss:0.3754751741020322\n",
      "train loss:0.009971436486076434, test loss:0.3754448813336696\n",
      "train loss:0.009964045679122959, test loss:0.3754204798138099\n",
      "train loss:0.009956662793372713, test loss:0.3753908875709658\n",
      "train loss:0.009949284433388595, test loss:0.37536385047882204\n",
      "train loss:0.009941919936540243, test loss:0.37533418727692863\n",
      "train loss:0.009927203002808145, test loss:0.3752784665465291\n",
      "train loss:0.009919854861545322, test loss:0.3752516866544736\n",
      "train loss:0.009912512365400597, test loss:0.37522071998746725\n",
      "train loss:0.00990517425461868, test loss:0.3751959234012953\n",
      "train loss:0.009897845501534264, test loss:0.37516583788379687\n",
      "train loss:0.009890522644640626, test loss:0.3751286754435033\n",
      "train loss:0.009883208571268376, test loss:0.3751060348806318\n",
      "train loss:0.009875902597331027, test loss:0.3750652323763255\n",
      "train loss:0.009868593053556264, test loss:0.37503382295009124\n",
      "train loss:0.009854016451308395, test loss:0.37497824550497877\n",
      "train loss:0.009846725498376849, test loss:0.37494324277812846\n",
      "train loss:0.009839449431443723, test loss:0.37491846037871573\n",
      "train loss:0.009832181879966609, test loss:0.3748900255429393\n",
      "train loss:0.009824919542966529, test loss:0.37485302468765186\n",
      "train loss:0.009817663476671915, test loss:0.37482560813487836\n",
      "train loss:0.009810415644080268, test loss:0.3747995908656775\n",
      "train loss:0.009803167053585946, test loss:0.37476089737582957\n",
      "train loss:0.009795924490928806, test loss:0.3747412600223878\n",
      "train loss:0.009781475166879963, test loss:0.37468246391003335\n",
      "train loss:0.009774257029968618, test loss:0.3746615532290751\n",
      "train loss:0.009767044889328322, test loss:0.3746414938758982\n",
      "train loss:0.009759840565341849, test loss:0.3746145353941898\n",
      "train loss:0.009752636562060431, test loss:0.3745910801764508\n",
      "train loss:0.00974544632190109, test loss:0.37457216145500444\n",
      "train loss:0.009738260979496647, test loss:0.37454421814723715\n",
      "train loss:0.009731084450900249, test loss:0.3745201821803441\n",
      "train loss:0.009723906308685054, test loss:0.3744996273774742\n",
      "train loss:0.009709578856603564, test loss:0.37445549811019013\n",
      "train loss:0.009702429668072925, test loss:0.3744377273501801\n",
      "train loss:0.009695283610317407, test loss:0.37441618144809047\n",
      "train loss:0.00968813917918794, test loss:0.3743963212394704\n",
      "train loss:0.009681006505901464, test loss:0.37437567973636554\n",
      "train loss:0.009673880051467843, test loss:0.3743575539562995\n",
      "train loss:0.0096667605502797, test loss:0.3743338675986631\n",
      "train loss:0.009659647250348364, test loss:0.3743143277425329\n",
      "train loss:0.009652541029566907, test loss:0.37429623576940674\n",
      "train loss:0.009638339054451571, test loss:0.37425243879907194\n",
      "train loss:0.009631255300492601, test loss:0.3742360201485374\n",
      "train loss:0.009624168368841475, test loss:0.3742109811340654\n",
      "train loss:0.00961709050050203, test loss:0.3741929363012636\n",
      "train loss:0.009610022943717099, test loss:0.3741761595558841\n",
      "train loss:0.009602954771634835, test loss:0.3741510183042978\n",
      "train loss:0.009595903654514076, test loss:0.37412856396073196\n",
      "train loss:0.00958885139994475, test loss:0.3741116627700712\n",
      "train loss:0.009581798876738927, test loss:0.37408384265087147\n",
      "train loss:0.009567727139764228, test loss:0.3740425277561849\n",
      "train loss:0.009560700483384267, test loss:0.37401619720745877\n",
      "train loss:0.00955368668635758, test loss:0.37399744889504305\n",
      "train loss:0.009546669659452323, test loss:0.37397963056073896\n",
      "train loss:0.009539661763306185, test loss:0.37395428519387897\n",
      "train loss:0.00953265711570264, test loss:0.37392863186671244\n",
      "train loss:0.009525659834549715, test loss:0.3739115812363816\n",
      "train loss:0.009518671558513403, test loss:0.3738824152003818\n",
      "train loss:0.00951168384334287, test loss:0.37386101798795335\n",
      "train loss:0.009497742345539802, test loss:0.37381718562018174\n",
      "train loss:0.00949077455525055, test loss:0.37379344200597564\n",
      "train loss:0.00948381299187395, test loss:0.3737724159048996\n",
      "train loss:0.00947686540999033, test loss:0.37374946440227597\n",
      "train loss:0.009469911349281435, test loss:0.3737268320137988\n",
      "train loss:0.009462973752227613, test loss:0.3737067653084779\n",
      "train loss:0.009456036579492013, test loss:0.3736790885189416\n",
      "train loss:0.009449109929840546, test loss:0.37366158321476517\n",
      "train loss:0.009442187602950994, test loss:0.373639227016301\n",
      "train loss:0.009428359057937145, test loss:0.37359048948004475\n",
      "train loss:0.0094214535395605, test loss:0.37356776557794796\n",
      "train loss:0.009414551946865721, test loss:0.3735467161307877\n",
      "train loss:0.009407666287070928, test loss:0.3735236290554523\n",
      "train loss:0.009400769887899472, test loss:0.37350246859025726\n",
      "train loss:0.009393891991406188, test loss:0.37348482163115726\n",
      "train loss:0.009387010737380604, test loss:0.37345642828550646\n",
      "train loss:0.009380142654271504, test loss:0.37343593092967237\n",
      "train loss:0.009373278843775831, test loss:0.37341399039346385\n",
      "train loss:0.009359571757906545, test loss:0.37336276771934873\n",
      "train loss:0.00935271913682931, test loss:0.37333963869031334\n",
      "train loss:0.009345881539750495, test loss:0.37330995030228137\n",
      "train loss:0.009339040351022433, test loss:0.3732856521925606\n",
      "train loss:0.009332217737460879, test loss:0.3732655405393184\n",
      "train loss:0.009325394647437708, test loss:0.3732340770090927\n",
      "train loss:0.009318580276898315, test loss:0.37321052055736814\n",
      "train loss:0.009311766076167134, test loss:0.37318644831974673\n",
      "train loss:0.009304958610726097, test loss:0.3731540187375016\n",
      "train loss:0.00929137184233574, test loss:0.37310639061992246\n",
      "train loss:0.00928458111833662, test loss:0.37308021784109174\n",
      "train loss:0.009277797302406795, test loss:0.37305310687899873\n",
      "train loss:0.009271021637373427, test loss:0.37303270547515044\n",
      "train loss:0.009264252689787035, test loss:0.3730007017185279\n",
      "train loss:0.009257484238648145, test loss:0.37297231767206046\n",
      "train loss:0.009250727545131737, test loss:0.3729541760487271\n",
      "train loss:0.009243973109075142, test loss:0.37292609484035827\n",
      "train loss:0.009237221370596413, test loss:0.3728967654658949\n",
      "train loss:0.009223746286722713, test loss:0.3728574057964302\n",
      "train loss:0.009217021878618793, test loss:0.3728223940060326\n",
      "train loss:0.009210290183918708, test loss:0.37280541689903907\n",
      "train loss:0.009203572502747847, test loss:0.37278169411023593\n",
      "train loss:0.00919685730405211, test loss:0.3727482940819085\n",
      "train loss:0.009190154365318778, test loss:0.37272183003948894\n",
      "train loss:0.009183448560690779, test loss:0.3727030761479665\n",
      "train loss:0.009176751420407771, test loss:0.3726760159711751\n",
      "train loss:0.009170061760267464, test loss:0.372649488681252\n",
      "train loss:0.00915669621882583, test loss:0.3726033183546722\n",
      "train loss:0.009150020951672718, test loss:0.37257087496810015\n",
      "train loss:0.009143350194125417, test loss:0.37255088672352815\n",
      "train loss:0.009136693375336907, test loss:0.3725257463344636\n",
      "train loss:0.00913003264892606, test loss:0.37249603202547465\n",
      "train loss:0.009123378484088661, test loss:0.37247626555024943\n",
      "train loss:0.009116740969143153, test loss:0.37245262119512595\n",
      "train loss:0.009110098803585799, test loss:0.37242448629869845\n",
      "train loss:0.009103467729028078, test loss:0.3724008072639741\n",
      "train loss:0.009090222453857447, test loss:0.37235154710455687\n",
      "train loss:0.009083607428347143, test loss:0.37232076188792845\n",
      "train loss:0.009076992356891363, test loss:0.3722929769897008\n",
      "train loss:0.009070389757314744, test loss:0.3722676565192247\n",
      "train loss:0.009063790121409081, test loss:0.3722397501369223\n",
      "train loss:0.009057202199891324, test loss:0.37220758026936573\n",
      "train loss:0.009050613472406871, test loss:0.37218167055191087\n",
      "train loss:0.009044028243479283, test loss:0.37215887830413186\n",
      "train loss:0.009037456812591176, test loss:0.37212307138086353\n",
      "train loss:0.009024323552946202, test loss:0.3720770542464382\n",
      "train loss:0.009017759910663567, test loss:0.3720436151447464\n",
      "train loss:0.009011213408414433, test loss:0.3720164024667516\n",
      "train loss:0.009004666763313166, test loss:0.3719919745105928\n",
      "train loss:0.008998120806524226, test loss:0.3719655587533515\n",
      "train loss:0.008991589654375437, test loss:0.3719387625700523\n",
      "train loss:0.008985059336580236, test loss:0.3719124931439286\n",
      "train loss:0.008978536384876298, test loss:0.37188186727871064\n",
      "train loss:0.008972016272610727, test loss:0.3718593369443137\n",
      "train loss:0.008958997291139163, test loss:0.37179474048563926\n",
      "train loss:0.008952500444345465, test loss:0.37177086997110836\n",
      "train loss:0.008946004543396894, test loss:0.37174099842778235\n",
      "train loss:0.008939514303092923, test loss:0.37170867814607716\n",
      "train loss:0.008933029306761217, test loss:0.3716896361629675\n",
      "train loss:0.008926551268106344, test loss:0.3716610896137765\n",
      "train loss:0.008920080936589187, test loss:0.3716328164574713\n",
      "train loss:0.00891361308041229, test loss:0.37161105975494474\n",
      "train loss:0.008907153039272184, test loss:0.3715802737225576\n",
      "train loss:0.008894241732945288, test loss:0.3715323973788276\n",
      "train loss:0.008887796902163469, test loss:0.37150382938289245\n",
      "train loss:0.008881352765368363, test loss:0.3714757511377554\n",
      "train loss:0.00887492209213753, test loss:0.37145661809480485\n",
      "train loss:0.008868488513875509, test loss:0.3714296986275253\n",
      "train loss:0.008862066446733327, test loss:0.3714026654482011\n",
      "train loss:0.00885564365888873, test loss:0.37137563748495384\n",
      "train loss:0.008849229757001498, test loss:0.3713533276314016\n",
      "train loss:0.008842821845318233, test loss:0.37132239813269424\n",
      "train loss:0.008830018484580189, test loss:0.37127466009410226\n",
      "train loss:0.008823626316916841, test loss:0.3712479630031017\n",
      "train loss:0.00881723887250485, test loss:0.3712232405330044\n",
      "train loss:0.008810850459388278, test loss:0.3711993607341053\n",
      "train loss:0.008804474535502704, test loss:0.3711737359973773\n",
      "train loss:0.008798101765317158, test loss:0.3711503483403676\n",
      "train loss:0.008791734310404625, test loss:0.3711247955403349\n",
      "train loss:0.008785374007142775, test loss:0.37110146452295595\n",
      "train loss:0.00877900978104211, test loss:0.3710749814333375\n",
      "train loss:0.008766321845595651, test loss:0.37102013336618256\n",
      "train loss:0.008759978915636575, test loss:0.3709920829699743\n",
      "train loss:0.00875364370763379, test loss:0.3709677377310682\n",
      "train loss:0.008747317499401828, test loss:0.3709379296736724\n",
      "train loss:0.008740995324416495, test loss:0.370912138108326\n",
      "train loss:0.008734678293518483, test loss:0.3708886626544231\n",
      "train loss:0.008728360310797927, test loss:0.37086265897730314\n",
      "train loss:0.008722055943898917, test loss:0.3708375350145444\n",
      "train loss:0.008715749648185707, test loss:0.3708153512058071\n",
      "train loss:0.008703154405844364, test loss:0.3707635904252453\n",
      "train loss:0.008696868920573055, test loss:0.3707382797299525\n",
      "train loss:0.008690582736553801, test loss:0.3707099151726571\n",
      "train loss:0.008684303574343579, test loss:0.3706872100747495\n",
      "train loss:0.008678030604751691, test loss:0.37065908051348\n",
      "train loss:0.008671764434632115, test loss:0.37063642279440856\n",
      "train loss:0.008665499400264603, test loss:0.37061175603867674\n",
      "train loss:0.00865923892650896, test loss:0.37058394894192603\n",
      "train loss:0.008652984222148484, test loss:0.37056332012142557\n",
      "train loss:0.008640489528474755, test loss:0.3705048142096342\n",
      "train loss:0.008634254792078975, test loss:0.3704824718730198\n",
      "train loss:0.008628023208948774, test loss:0.37045471957104686\n",
      "train loss:0.008621788023923693, test loss:0.37043041470497484\n",
      "train loss:0.008615565596456258, test loss:0.3704101648435971\n",
      "train loss:0.00860934907403986, test loss:0.3703815148210941\n",
      "train loss:0.008603139913896387, test loss:0.37035791837917437\n",
      "train loss:0.008596928877523348, test loss:0.3703336874324718\n",
      "train loss:0.00859073411357959, test loss:0.37030397460323333\n",
      "train loss:0.008578344571914328, test loss:0.37025493657206227\n",
      "train loss:0.008572160638495219, test loss:0.37022889123330444\n",
      "train loss:0.008565981063032775, test loss:0.37020320449667155\n",
      "train loss:0.008559808774883448, test loss:0.37018176217284543\n",
      "train loss:0.008553636354151605, test loss:0.37015032908564205\n",
      "train loss:0.008547478053781659, test loss:0.3701293436734844\n",
      "train loss:0.008541320941281808, test loss:0.37010209222543966\n",
      "train loss:0.008535168252864423, test loss:0.37007281948462567\n",
      "train loss:0.008529017949981413, test loss:0.3700441881180712\n",
      "train loss:0.008516742264164719, test loss:0.36998947257169823\n",
      "train loss:0.008510603696895441, test loss:0.3699636442189863\n",
      "train loss:0.00850448087467018, test loss:0.36993893765672015\n",
      "train loss:0.008498357694580632, test loss:0.36990413726581145\n",
      "train loss:0.008492240414512218, test loss:0.3698781655748864\n",
      "train loss:0.008486127945929135, test loss:0.36984892832394944\n",
      "train loss:0.008480018587701452, test loss:0.36981744546320544\n",
      "train loss:0.0084739165628886, test loss:0.36979201117256\n",
      "train loss:0.008467819761774551, test loss:0.3697657873808594\n",
      "train loss:0.008455641299692292, test loss:0.3697165721076404\n",
      "train loss:0.008449563353297928, test loss:0.36968498870599475\n",
      "train loss:0.008443490096024502, test loss:0.36965674918947666\n",
      "train loss:0.00843741521631169, test loss:0.3696334891574136\n",
      "train loss:0.008431352800144435, test loss:0.3696014724131165\n",
      "train loss:0.008425292605902404, test loss:0.3695726571861624\n",
      "train loss:0.00841923603398037, test loss:0.36955476341144133\n",
      "train loss:0.008413184131849091, test loss:0.3695222256084539\n",
      "train loss:0.00840713868000667, test loss:0.36949797171020277\n",
      "train loss:0.008395064350995294, test loss:0.3694488550076896\n",
      "train loss:0.008389030620007475, test loss:0.3694204579882779\n",
      "train loss:0.008383010253055595, test loss:0.36939707672385264\n",
      "train loss:0.008376992971311194, test loss:0.36936890688366564\n",
      "train loss:0.008370976996875584, test loss:0.36933962134384635\n",
      "train loss:0.008364967119786895, test loss:0.36931927847593987\n",
      "train loss:0.008358961435776593, test loss:0.36929230553305165\n",
      "train loss:0.0083529659576914, test loss:0.3692664813643949\n",
      "train loss:0.00834696904747765, test loss:0.3692444609875159\n",
      "train loss:0.008334992569490291, test loss:0.3691894850040678\n",
      "train loss:0.008329018442434773, test loss:0.36916740678263144\n",
      "train loss:0.008323042627057364, test loss:0.3691352399946502\n",
      "train loss:0.008317073790040543, test loss:0.3691074789546608\n",
      "train loss:0.008311110834203798, test loss:0.369085628190091\n",
      "train loss:0.008305154618609294, test loss:0.3690569469761457\n",
      "train loss:0.00829919789079783, test loss:0.369029223195155\n",
      "train loss:0.008293255124861797, test loss:0.36900535590692857\n",
      "train loss:0.008287307059471371, test loss:0.3689806186654711\n",
      "train loss:0.008275440599792268, test loss:0.36892408245301855\n",
      "train loss:0.008269508847552537, test loss:0.3688992897649675\n",
      "train loss:0.008263588795503464, test loss:0.3688651367677815\n",
      "train loss:0.008257671186011816, test loss:0.3688436936590132\n",
      "train loss:0.008251763396601925, test loss:0.3688199394957642\n",
      "train loss:0.00824585228116246, test loss:0.36878959706596326\n",
      "train loss:0.008239955068400282, test loss:0.36876791879510784\n",
      "train loss:0.008234056304403497, test loss:0.3687453092148216\n",
      "train loss:0.0082281630044848, test loss:0.3687138055778224\n",
      "train loss:0.008216397684777948, test loss:0.3686693666985605\n",
      "train loss:0.008210518828064227, test loss:0.3686448328351514\n",
      "train loss:0.00820464100783409, test loss:0.36861587437911797\n",
      "train loss:0.008198779321540537, test loss:0.36859744759286817\n",
      "train loss:0.008192912491293034, test loss:0.36857242572771876\n",
      "train loss:0.008187052998000518, test loss:0.3685430808133497\n",
      "train loss:0.008181200514417633, test loss:0.36852750894302017\n",
      "train loss:0.008175356423157084, test loss:0.36850385797287916\n",
      "train loss:0.008169510205972718, test loss:0.3684797599055597\n",
      "train loss:0.008157831173585971, test loss:0.3684379303768058\n",
      "train loss:0.008152008949399376, test loss:0.3684170318359638\n",
      "train loss:0.008146183851268671, test loss:0.3683944907710665\n",
      "train loss:0.008140358679617201, test loss:0.3683724886785433\n",
      "train loss:0.008134547348225017, test loss:0.36835128781981846\n",
      "train loss:0.008128735409562224, test loss:0.3683263208776689\n",
      "train loss:0.008122930837470002, test loss:0.36830305238315547\n",
      "train loss:0.008117129085591781, test loss:0.36828349851565934\n",
      "train loss:0.008111327363149413, test loss:0.3682614519749105\n",
      "train loss:0.008099751964420409, test loss:0.3682151282269929\n",
      "train loss:0.008093970655611446, test loss:0.3681985268413242\n",
      "train loss:0.008088194849845017, test loss:0.36817560063641314\n",
      "train loss:0.008082420319272112, test loss:0.3681540503567296\n",
      "train loss:0.008076657019158866, test loss:0.36814228056273485\n",
      "train loss:0.008070893306327882, test loss:0.3681181385394114\n",
      "train loss:0.008065138440487177, test loss:0.36809736740045584\n",
      "train loss:0.008059383495521787, test loss:0.36808388665344255\n",
      "train loss:0.008053634780953436, test loss:0.3680598718978177\n",
      "train loss:0.008042154431962448, test loss:0.36802530497849284\n",
      "train loss:0.008036422595379723, test loss:0.36799625938582253\n",
      "train loss:0.008030697690488175, test loss:0.3679772162682174\n",
      "train loss:0.00802497833215524, test loss:0.367965062675942\n",
      "train loss:0.008019254762989841, test loss:0.3679393950130669\n",
      "train loss:0.008013544681861494, test loss:0.3679225488659048\n",
      "train loss:0.0080078390406041, test loss:0.36790829119000507\n",
      "train loss:0.008002132548822831, test loss:0.36788375925786626\n",
      "train loss:0.007996433952210715, test loss:0.3678651120525672\n",
      "train loss:0.00798505806607598, test loss:0.3678257487647929\n",
      "train loss:0.007979373325347544, test loss:0.3678066057324337\n",
      "train loss:0.007973697000285475, test loss:0.36778895102132986\n",
      "train loss:0.007968021674301584, test loss:0.3677723727668967\n",
      "train loss:0.007962349167739285, test loss:0.3677503755801874\n",
      "train loss:0.007956684320305307, test loss:0.3677302527715627\n",
      "train loss:0.007951022688278062, test loss:0.3677170902399435\n",
      "train loss:0.007945365757980691, test loss:0.36769662258037505\n",
      "train loss:0.00793971668423268, test loss:0.36768284679114605\n",
      "train loss:0.007928426470930313, test loss:0.3676468148560079\n",
      "train loss:0.007922791975925542, test loss:0.36762735549632397\n",
      "train loss:0.007917158017535923, test loss:0.36761152573151046\n",
      "train loss:0.007911532835774069, test loss:0.3675897845108788\n",
      "train loss:0.007905909520687508, test loss:0.36757603555481805\n",
      "train loss:0.007900294088777952, test loss:0.36756397612214203\n",
      "train loss:0.00789467726471176, test loss:0.36754100045646537\n",
      "train loss:0.007889067879052442, test loss:0.3675287741396795\n",
      "train loss:0.007883468167666472, test loss:0.36751063878733786\n",
      "train loss:0.007872275641095597, test loss:0.3674740043638369\n",
      "train loss:0.007866687028730283, test loss:0.3674626756994125\n",
      "train loss:0.007861099770564161, test loss:0.36744413883845334\n",
      "train loss:0.007855522026968417, test loss:0.3674243600520109\n",
      "train loss:0.007849945311638263, test loss:0.3674163064906092\n",
      "train loss:0.007844379608489658, test loss:0.3673962081356951\n",
      "train loss:0.00783881352367315, test loss:0.3673765333660443\n",
      "train loss:0.00783325359377741, test loss:0.36736609311509083\n",
      "train loss:0.007827698616469307, test loss:0.36734614604478044\n",
      "train loss:0.007816602058118962, test loss:0.3673200799363111\n",
      "train loss:0.007811057585770061, test loss:0.3672969066966414\n",
      "train loss:0.007805520253302958, test loss:0.36727970834993534\n",
      "train loss:0.007799996003263599, test loss:0.36727366344267537\n",
      "train loss:0.007794462656873848, test loss:0.36725400079458176\n",
      "train loss:0.00778894288709348, test loss:0.3672357907660269\n",
      "train loss:0.007783423340079302, test loss:0.3672286213939107\n",
      "train loss:0.007777910157401466, test loss:0.3672110407202379\n",
      "train loss:0.0077724001064209715, test loss:0.36719084527182355\n",
      "train loss:0.00776139923723373, test loss:0.3671679086320398\n",
      "train loss:0.007755900523862917, test loss:0.36714475282680514\n",
      "train loss:0.007750407167776162, test loss:0.3671343821277407\n",
      "train loss:0.007744925135088102, test loss:0.36712243963466307\n",
      "train loss:0.007739436545694229, test loss:0.36710462487877726\n",
      "train loss:0.007733955489167369, test loss:0.36709848758825186\n",
      "train loss:0.007728485146505782, test loss:0.3670791973897518\n",
      "train loss:0.007723012651013529, test loss:0.3670655787911464\n",
      "train loss:0.007717550229484588, test loss:0.36705566550186547\n",
      "train loss:0.007706631716241304, test loss:0.367023592769791\n",
      "train loss:0.007701176642686159, test loss:0.3670140417043151\n",
      "train loss:0.007695727647262146, test loss:0.3669872810614801\n",
      "train loss:0.007690282411461893, test loss:0.36697813956097886\n",
      "train loss:0.007684842962513237, test loss:0.36696125333572127\n",
      "train loss:0.007679407473418439, test loss:0.36694327372266355\n",
      "train loss:0.00767397925472952, test loss:0.3669281190561645\n",
      "train loss:0.007668552021833699, test loss:0.366907364746613\n",
      "train loss:0.007663128483227402, test loss:0.36689236582824003\n",
      "train loss:0.007652289998340638, test loss:0.3668523482869607\n",
      "train loss:0.007646878705033432, test loss:0.36683531510675144\n",
      "train loss:0.007641478036669112, test loss:0.36681099723873795\n",
      "train loss:0.007636078900886243, test loss:0.3667931765205067\n",
      "train loss:0.007630681355731287, test loss:0.3667736258038403\n",
      "train loss:0.007625291286822511, test loss:0.36675305710981343\n",
      "train loss:0.007619905351630666, test loss:0.366737861022853\n",
      "train loss:0.007614520542978423, test loss:0.36671359303229284\n",
      "train loss:0.007609141807012789, test loss:0.3666936962009431\n",
      "train loss:0.007598402213289603, test loss:0.36665010021005384\n",
      "train loss:0.007593035681436809, test loss:0.3666407485618094\n",
      "train loss:0.007587680999679551, test loss:0.36662556635300214\n",
      "train loss:0.007582321931518505, test loss:0.3666050017710466\n",
      "train loss:0.007576969634058573, test loss:0.36659374486664686\n",
      "train loss:0.007571625988031367, test loss:0.3665802337694409\n",
      "train loss:0.007566283012832438, test loss:0.3665592237771757\n",
      "train loss:0.007560944109564784, test loss:0.36654845706292183\n",
      "train loss:0.007555614929113826, test loss:0.3665341182091416\n",
      "train loss:0.0075449569145596435, test loss:0.36650364012264286\n",
      "train loss:0.007539644522431947, test loss:0.3664848901429489\n",
      "train loss:0.007534324308754459, test loss:0.3664700242102314\n",
      "train loss:0.00752901460824132, test loss:0.3664615548954283\n",
      "train loss:0.007523704548803264, test loss:0.3664411146031534\n",
      "train loss:0.00751840500033199, test loss:0.36642962330609796\n",
      "train loss:0.007513104345582683, test loss:0.3664157054669765\n",
      "train loss:0.007507812118170572, test loss:0.36639380048796366\n",
      "train loss:0.007502523313226966, test loss:0.3663864318151812\n",
      "train loss:0.007491956446494888, test loss:0.3663498999532059\n",
      "train loss:0.007486679166276377, test loss:0.36634346695812214\n",
      "train loss:0.007481406447428278, test loss:0.3663262008812658\n",
      "train loss:0.007476140174425556, test loss:0.3663080790691497\n",
      "train loss:0.007470876842141812, test loss:0.3662957764741788\n",
      "train loss:0.007465615970949783, test loss:0.36627515679171535\n",
      "train loss:0.007460365333430101, test loss:0.36626006153826834\n",
      "train loss:0.007455112804773364, test loss:0.36624553337980403\n",
      "train loss:0.007449864908917681, test loss:0.36622975272907793\n",
      "train loss:0.007439383035776594, test loss:0.3661932785185402\n",
      "train loss:0.007434151003224786, test loss:0.3661710401610966\n",
      "train loss:0.007428922394884266, test loss:0.3661546125027034\n",
      "train loss:0.0074236954267082305, test loss:0.36612929652326853\n",
      "train loss:0.007418477356912939, test loss:0.36611085602719745\n",
      "train loss:0.00741326343195232, test loss:0.3660921401150112\n",
      "train loss:0.007408048075068594, test loss:0.3660709706751186\n",
      "train loss:0.007402838675794408, test loss:0.36605209208901307\n",
      "train loss:0.007397634681001901, test loss:0.36603332131636956\n",
      "train loss:0.00738724453921572, test loss:0.3659903753126284\n",
      "train loss:0.007382051030554145, test loss:0.36597149118159444\n",
      "train loss:0.007376869687675739, test loss:0.36594755665684736\n",
      "train loss:0.007371687177393872, test loss:0.3659322984755319\n",
      "train loss:0.0073665073483977685, test loss:0.36591680496622375\n",
      "train loss:0.007361340899721401, test loss:0.3658916546197069\n",
      "train loss:0.007356171312390909, test loss:0.36588024668461927\n",
      "train loss:0.007351007031212355, test loss:0.36586124114207635\n",
      "train loss:0.007345855256683436, test loss:0.36583723729830203\n",
      "train loss:0.007335548384470486, test loss:0.3658066385470896\n",
      "train loss:0.007330402377869324, test loss:0.3657985494616781\n",
      "train loss:0.007325267515230426, test loss:0.3657816604819734\n",
      "train loss:0.007320129728900009, test loss:0.3657664536059162\n",
      "train loss:0.007314995021753593, test loss:0.3657603822500831\n",
      "train loss:0.007309866874319651, test loss:0.3657401934626203\n",
      "train loss:0.007304746252048036, test loss:0.365722571810444\n",
      "train loss:0.0072996303926078695, test loss:0.36571047898267595\n",
      "train loss:0.007294513538059889, test loss:0.3656893255669851\n",
      "train loss:0.0072843009434076875, test loss:0.365668693676366\n",
      "train loss:0.0072792010295403145, test loss:0.3656527038437254\n",
      "train loss:0.0072741015996439645, test loss:0.36563853277546265\n",
      "train loss:0.007269007170687323, test loss:0.365632045034355\n",
      "train loss:0.007263923193051682, test loss:0.36561272694837954\n",
      "train loss:0.007258839589877865, test loss:0.3655944413290013\n",
      "train loss:0.007253758706430751, test loss:0.365586745093386\n",
      "train loss:0.007248683690392926, test loss:0.36556816134047254\n",
      "train loss:0.007243611098645541, test loss:0.3655516641616021\n",
      "train loss:0.007233480442652866, test loss:0.3655209907785477\n",
      "train loss:0.007228421676167344, test loss:0.3655063462583409\n",
      "train loss:0.007223369806921023, test loss:0.3654894616516162\n",
      "train loss:0.0072183150829121445, test loss:0.3654699638261587\n",
      "train loss:0.00721327182377931, test loss:0.3654567728425084\n",
      "train loss:0.00720823928492006, test loss:0.36543666723966234\n",
      "train loss:0.007203195014190284, test loss:0.36542492112002983\n",
      "train loss:0.007198166699432385, test loss:0.3654104393620429\n",
      "train loss:0.007193134321176174, test loss:0.36538818415682967\n",
      "train loss:0.007183086085745778, test loss:0.36535704421180093\n",
      "train loss:0.007178072360334219, test loss:0.3653380230846468\n",
      "train loss:0.007173057783094753, test loss:0.36532090806402095\n",
      "train loss:0.007168049869600606, test loss:0.3653019755685584\n",
      "train loss:0.0071630449637005815, test loss:0.36528219703577247\n",
      "train loss:0.00715804635071176, test loss:0.3652714509527558\n",
      "train loss:0.007153047524023674, test loss:0.36524769494688314\n",
      "train loss:0.007148055706653711, test loss:0.36523620820293323\n",
      "train loss:0.007143069224239363, test loss:0.3652167836997927\n",
      "train loss:0.007133098483508073, test loss:0.3651807567153763\n",
      "train loss:0.007128128912150418, test loss:0.36515766958581636\n",
      "train loss:0.007123155644302969, test loss:0.36514533698256235\n",
      "train loss:0.007118187577929341, test loss:0.36511842092088226\n",
      "train loss:0.007113227352382946, test loss:0.365106854212033\n",
      "train loss:0.0071082720597221575, test loss:0.3650878962464714\n",
      "train loss:0.0071033129067334935, test loss:0.3650662979554118\n",
      "train loss:0.0070983675492305165, test loss:0.36505711162601645\n",
      "train loss:0.007093413464711625, test loss:0.3650348392246004\n",
      "train loss:0.007083536447940333, test loss:0.36499846100679706\n",
      "train loss:0.007078601435029056, test loss:0.364981686597465\n",
      "train loss:0.007073669369129717, test loss:0.36496171537229344\n",
      "train loss:0.007068742982300423, test loss:0.36494917072589317\n",
      "train loss:0.007063827153894901, test loss:0.3649242879266541\n",
      "train loss:0.0070589067739147745, test loss:0.36491290433599644\n",
      "train loss:0.007053986011053059, test loss:0.3648903250160084\n",
      "train loss:0.0070490763838232965, test loss:0.3648747209583326\n",
      "train loss:0.007044169795768928, test loss:0.3648640184868818\n",
      "train loss:0.0070343693448701354, test loss:0.3648329235804317\n",
      "train loss:0.007029476857296858, test loss:0.36481830610206994\n",
      "train loss:0.0070245875302856154, test loss:0.3647910882905708\n",
      "train loss:0.0070196989154964655, test loss:0.36478674686509677\n",
      "train loss:0.007014811760335909, test loss:0.36477411924312436\n",
      "train loss:0.007009936634675578, test loss:0.36474880432649837\n",
      "train loss:0.007005062225038889, test loss:0.36473841609887614\n",
      "train loss:0.007000190450164929, test loss:0.3647259364864689\n",
      "train loss:0.006995325518289996, test loss:0.3646978398092917\n",
      "train loss:0.006985603123372379, test loss:0.364676988479035\n",
      "train loss:0.006980747835364595, test loss:0.3646567796324208\n",
      "train loss:0.006975898579830825, test loss:0.3646454470071758\n",
      "train loss:0.006971050998988584, test loss:0.36462556589570844\n",
      "train loss:0.006966206821080496, test loss:0.3646125641616515\n",
      "train loss:0.006961369028226487, test loss:0.36459382499530285\n",
      "train loss:0.006956534281066523, test loss:0.3645798914924067\n",
      "train loss:0.0069517012949461944, test loss:0.3645729229263028\n",
      "train loss:0.006946875093657208, test loss:0.36454398449651904\n",
      "train loss:0.006937228225430857, test loss:0.36451762651310626\n",
      "train loss:0.006932408837719366, test loss:0.3644910422531593\n",
      "train loss:0.006927596187199417, test loss:0.3644834230042876\n",
      "train loss:0.006922789573080108, test loss:0.3644571480687676\n",
      "train loss:0.006917980923816147, test loss:0.36443369797826003\n",
      "train loss:0.006913182060233481, test loss:0.3644290716019998\n",
      "train loss:0.006908384349253243, test loss:0.3643980850508765\n",
      "train loss:0.006903589804940564, test loss:0.3643772387604284\n",
      "train loss:0.006898804567687587, test loss:0.3643690437200519\n",
      "train loss:0.006889228826199536, test loss:0.36431466881552704\n",
      "train loss:0.006884451113672356, test loss:0.36431162102810216\n",
      "train loss:0.006879673249658278, test loss:0.36427886032589246\n",
      "train loss:0.006874905300167038, test loss:0.36426500652368055\n",
      "train loss:0.006870136283291308, test loss:0.3642533212215123\n",
      "train loss:0.006865369034126576, test loss:0.3642298930561858\n",
      "train loss:0.006860612850423014, test loss:0.36420889413910046\n",
      "train loss:0.006855853046637627, test loss:0.36419424180242516\n",
      "train loss:0.006851100841597692, test loss:0.3641660992015465\n",
      "train loss:0.006841608902245782, test loss:0.36413409734512203\n",
      "train loss:0.006836863145854242, test loss:0.36411187780188115\n",
      "train loss:0.0068321233798638095, test loss:0.36410387904608954\n",
      "train loss:0.006827388926912504, test loss:0.3640741106282054\n",
      "train loss:0.006822657167897907, test loss:0.3640591008271953\n",
      "train loss:0.006817931066973974, test loss:0.36404802668030434\n",
      "train loss:0.006813209611951335, test loss:0.3640156486860137\n",
      "train loss:0.0068084914318009, test loss:0.36400074627145174\n",
      "train loss:0.006803772132065118, test loss:0.36398166086305755\n",
      "train loss:0.006794355325630014, test loss:0.3639476356930852\n",
      "train loss:0.0067896495464035665, test loss:0.3639204205538309\n",
      "train loss:0.006784950140166064, test loss:0.36389472677718504\n",
      "train loss:0.006780256839086609, test loss:0.36388715834877083\n",
      "train loss:0.006775557983583856, test loss:0.36385847961855955\n",
      "train loss:0.006770872228072604, test loss:0.36384255751000816\n",
      "train loss:0.006766183393095105, test loss:0.3638283070332036\n",
      "train loss:0.006761500528037573, test loss:0.363802619867223\n",
      "train loss:0.006756824718956926, test loss:0.36378740656563074\n",
      "train loss:0.006747474474665555, test loss:0.36374957062323365\n",
      "train loss:0.006742810947853521, test loss:0.36373137848859877\n",
      "train loss:0.00673814499957962, test loss:0.36371157078073\n",
      "train loss:0.006733485703460604, test loss:0.3636900880547436\n",
      "train loss:0.006728833426180034, test loss:0.36367175881948693\n",
      "train loss:0.006724174999084412, test loss:0.3636487964616936\n",
      "train loss:0.006719527688508242, test loss:0.36363483328809104\n",
      "train loss:0.006714884411255645, test loss:0.3636134735151232\n",
      "train loss:0.0067102396762038505, test loss:0.363595821425459\n",
      "train loss:0.006700967517000464, test loss:0.3635622249366633\n",
      "train loss:0.006696337821541552, test loss:0.3635420249043671\n",
      "train loss:0.006691709244364895, test loss:0.363534124110465\n",
      "train loss:0.006687083320475157, test loss:0.36351294028241415\n",
      "train loss:0.006682463707364868, test loss:0.3634938423570098\n",
      "train loss:0.006677846181774115, test loss:0.3634839006608072\n",
      "train loss:0.006673231968527727, test loss:0.36345760691909174\n",
      "train loss:0.006668622000873708, test loss:0.36344418297159636\n",
      "train loss:0.006664016299231525, test loss:0.36343112227767116\n",
      "train loss:0.006654819241903947, test loss:0.3634029419782874\n",
      "train loss:0.00665022111843078, test loss:0.3633834518233268\n",
      "train loss:0.006645634636690235, test loss:0.36336176205955917\n",
      "train loss:0.006641048564483843, test loss:0.3633549300017627\n",
      "train loss:0.006636463576974188, test loss:0.3633403643657907\n",
      "train loss:0.006631885645275553, test loss:0.36331468856263827\n",
      "train loss:0.006627309288523764, test loss:0.3633164780361015\n",
      "train loss:0.006622738538293132, test loss:0.3632955269510688\n",
      "train loss:0.00661817227068113, test loss:0.3632759790761179\n",
      "train loss:0.006609050277281494, test loss:0.36324759927225253\n",
      "train loss:0.006604489966213106, test loss:0.3632309859825092\n",
      "train loss:0.0065999370578062495, test loss:0.3632239459496015\n",
      "train loss:0.006595389608369329, test loss:0.363201497475436\n",
      "train loss:0.00659084482175219, test loss:0.36318161301118534\n",
      "train loss:0.006586305174886579, test loss:0.36318256568461565\n",
      "train loss:0.006581762249741401, test loss:0.36315188632535783\n",
      "train loss:0.006577227271415799, test loss:0.36313904160156685\n",
      "train loss:0.0065726983508090486, test loss:0.3631329241257528\n",
      "train loss:0.006563640310806295, test loss:0.3630880744325017\n",
      "train loss:0.006559126993526438, test loss:0.3630844431103025\n",
      "train loss:0.0065546080368413554, test loss:0.36305297771100836\n",
      "train loss:0.006550093407784745, test loss:0.3630346708928659\n",
      "train loss:0.006545582385328839, test loss:0.3630285024490828\n",
      "train loss:0.006541074442385074, test loss:0.36300088170062683\n",
      "train loss:0.006536571219833026, test loss:0.3629800920571655\n",
      "train loss:0.006532075073903735, test loss:0.36297342882644396\n",
      "train loss:0.006527578242894892, test loss:0.3629447258568838\n",
      "train loss:0.006518598628539694, test loss:0.36291371578142506\n",
      "train loss:0.006514114858826309, test loss:0.3628811556390235\n",
      "train loss:0.006509633138442036, test loss:0.3628659650880576\n",
      "train loss:0.006505151417398927, test loss:0.362852532273081\n",
      "train loss:0.006500679054191141, test loss:0.3628207623282642\n",
      "train loss:0.006496210383154324, test loss:0.3628142385337079\n",
      "train loss:0.006491738481460619, test loss:0.3627976243737457\n",
      "train loss:0.0064872782258672745, test loss:0.36277091076929785\n",
      "train loss:0.006482815505187674, test loss:0.3627603732693654\n",
      "train loss:0.00647390423760555, test loss:0.3627128927334337\n",
      "train loss:0.006469454361872509, test loss:0.36270050146595695\n",
      "train loss:0.0064650044599907485, test loss:0.36268985354283567\n",
      "train loss:0.006460562269237566, test loss:0.36265544544004064\n",
      "train loss:0.006456117406157688, test loss:0.3626526304403671\n",
      "train loss:0.00645168326975479, test loss:0.36264243486075187\n",
      "train loss:0.006447246016589342, test loss:0.3626131380670021\n",
      "train loss:0.006442818313511089, test loss:0.3626076732794308\n",
      "train loss:0.006438391310041764, test loss:0.3625949243973847\n",
      "train loss:0.0064295505460402495, test loss:0.36256220848541326\n",
      "train loss:0.006425132265611084, test loss:0.3625430924014475\n",
      "train loss:0.006420717875078664, test loss:0.3625263171525874\n",
      "train loss:0.006416315085428467, test loss:0.3625122277327517\n",
      "train loss:0.006411901813304222, test loss:0.36249098255618667\n",
      "train loss:0.0064075001636964115, test loss:0.3624767654271569\n",
      "train loss:0.00640310327378612, test loss:0.36245861485273606\n",
      "train loss:0.006398712362727903, test loss:0.36244641286124185\n",
      "train loss:0.0063943186418820375, test loss:0.36242705134824793\n",
      "train loss:0.0063855492743551765, test loss:0.3623948947387522\n",
      "train loss:0.006381163052592926, test loss:0.362376567648124\n",
      "train loss:0.006376787190038467, test loss:0.3623628694404901\n",
      "train loss:0.006372409088963512, test loss:0.36234034641216695\n",
      "train loss:0.0063680417906256965, test loss:0.3623325169796466\n",
      "train loss:0.006363671690742609, test loss:0.3623108033152073\n",
      "train loss:0.006359303498450818, test loss:0.3623012533241586\n",
      "train loss:0.0063549482855398435, test loss:0.3622831032740258\n",
      "train loss:0.006350589462616581, test loss:0.36227660395340994\n",
      "train loss:0.0063418797131670705, test loss:0.36223933825606336\n",
      "train loss:0.006337532516148261, test loss:0.3622348432818662\n",
      "train loss:0.00633319025246892, test loss:0.36220839403722005\n",
      "train loss:0.006328848625574317, test loss:0.36219970404092966\n",
      "train loss:0.006324508780210186, test loss:0.36218814425591084\n",
      "train loss:0.006320174424409471, test loss:0.3621679682659705\n",
      "train loss:0.006315847616253907, test loss:0.3621626149371073\n",
      "train loss:0.006311513950926952, test loss:0.36214519558711206\n",
      "train loss:0.006307192511397842, test loss:0.36213774764574047\n",
      "train loss:0.0062985587550760044, test loss:0.36211578842526465\n",
      "train loss:0.006294243559526793, test loss:0.3620979460992611\n",
      "train loss:0.006289932630791337, test loss:0.36209616034394354\n",
      "train loss:0.006285624739401962, test loss:0.3620746700345225\n",
      "train loss:0.0062813215139292744, test loss:0.3620660734592717\n",
      "train loss:0.006277021387793711, test loss:0.36205806338552404\n",
      "train loss:0.006272722990890696, test loss:0.36204085001444747\n",
      "train loss:0.006268430916094652, test loss:0.3620364182574906\n",
      "train loss:0.006264142567694281, test loss:0.36201125937548007\n",
      "train loss:0.006255571481588744, test loss:0.36199250864699134\n",
      "train loss:0.006251296648407493, test loss:0.3619657907265384\n",
      "train loss:0.0062470205717466655, test loss:0.36196347522172\n",
      "train loss:0.006242745880533848, test loss:0.36194442532174037\n",
      "train loss:0.006238475118685493, test loss:0.36192579741473524\n",
      "train loss:0.0062342107889230705, test loss:0.3619165889293189\n",
      "train loss:0.006229948224538531, test loss:0.36189772926966773\n",
      "train loss:0.006225691800100182, test loss:0.36188085187532576\n",
      "train loss:0.00622144000886634, test loss:0.36187073229790667\n",
      "train loss:0.00621293586737631, test loss:0.3618324965327334\n",
      "train loss:0.0062086934247887636, test loss:0.361823605899236\n",
      "train loss:0.006204453276837451, test loss:0.36179925321931405\n",
      "train loss:0.006200214457808056, test loss:0.36178503685926594\n",
      "train loss:0.0061959816667059185, test loss:0.36177419001483024\n",
      "train loss:0.00619175104616839, test loss:0.3617485051571987\n",
      "train loss:0.006187526977915217, test loss:0.3617390443480016\n",
      "train loss:0.006183302326273111, test loss:0.36171655382567397\n",
      "train loss:0.006179082055419402, test loss:0.36170270099593554\n",
      "train loss:0.006170652967738352, test loss:0.3616682335783252\n",
      "train loss:0.0061664378562118925, test loss:0.3616469618580979\n",
      "train loss:0.006162235274002671, test loss:0.3616325614505602\n",
      "train loss:0.006158026796902159, test loss:0.36161006076979557\n",
      "train loss:0.006153827794327995, test loss:0.3615941932293319\n",
      "train loss:0.006149632354688764, test loss:0.3615716415668972\n",
      "train loss:0.00614544123918164, test loss:0.361561889946787\n",
      "train loss:0.006141247435221369, test loss:0.3615401527329119\n",
      "train loss:0.0061370620226458995, test loss:0.3615261697163804\n",
      "train loss:0.006128696924205018, test loss:0.361488721734324\n",
      "train loss:0.006124522167528514, test loss:0.36148147425757116\n",
      "train loss:0.006120348528051944, test loss:0.36145438565379934\n",
      "train loss:0.0061161784354157285, test loss:0.36144026080153807\n",
      "train loss:0.006112009608161005, test loss:0.3614253360671123\n",
      "train loss:0.006107847715703334, test loss:0.36140023928385806\n",
      "train loss:0.006103690266442449, test loss:0.36138649117747906\n",
      "train loss:0.006099528558943973, test loss:0.3613733306710236\n",
      "train loss:0.006095379178327738, test loss:0.3613455873703115\n",
      "train loss:0.006087075606172379, test loss:0.36131398477292676\n",
      "train loss:0.006082932755746851, test loss:0.3612956993465044\n",
      "train loss:0.0060787921808395096, test loss:0.36127987301229314\n",
      "train loss:0.006074657215058896, test loss:0.3612640649871932\n",
      "train loss:0.006070522382117195, test loss:0.3612468677376985\n",
      "train loss:0.006066391309346863, test loss:0.3612345075237458\n",
      "train loss:0.006062258042381593, test loss:0.3612089706053702\n",
      "train loss:0.006058138178415844, test loss:0.36120169734265795\n",
      "train loss:0.006054014957799442, test loss:0.36118183848111735\n",
      "train loss:0.006045780271728175, test loss:0.36115664441392004\n",
      "train loss:0.006041669972892068, test loss:0.3611304859988295\n",
      "train loss:0.006037561452472436, test loss:0.36112600910628473\n",
      "train loss:0.006033455205861794, test loss:0.36110908356862975\n",
      "train loss:0.006029351682615779, test loss:0.36108617481617145\n",
      "train loss:0.006025253779939448, test loss:0.36107668612189014\n",
      "train loss:0.006021156159880228, test loss:0.36106273199628985\n",
      "train loss:0.006017062880162087, test loss:0.36104116494092003\n",
      "train loss:0.006012974638103806, test loss:0.3610352773266657\n",
      "train loss:0.006004804603786432, test loss:0.3610024692389916\n",
      "train loss:0.006000720754346207, test loss:0.36099631746904604\n",
      "train loss:0.0059966476541296375, test loss:0.36097139102520226\n",
      "train loss:0.005992570224691955, test loss:0.360962775552336\n",
      "train loss:0.005988497480168512, test loss:0.360955204210122\n",
      "train loss:0.005984428317577238, test loss:0.36092654439563465\n",
      "train loss:0.005980362105866902, test loss:0.3609223973729955\n",
      "train loss:0.0059762994123715414, test loss:0.36091413858542\n",
      "train loss:0.0059722427888723876, test loss:0.3608890665195794\n",
      "train loss:0.005964128939518912, test loss:0.3608618402646568\n",
      "train loss:0.005960079607898011, test loss:0.3608420314418481\n",
      "train loss:0.005956035165779747, test loss:0.3608257778092312\n",
      "train loss:0.005951988602012179, test loss:0.36081700931063637\n",
      "train loss:0.00594794889545562, test loss:0.3607949776776276\n",
      "train loss:0.005943912215031886, test loss:0.3607888222413889\n",
      "train loss:0.005939880834838675, test loss:0.360773986260439\n",
      "train loss:0.005935847362095929, test loss:0.36075277429203967\n",
      "train loss:0.005931818972749174, test loss:0.3607420258974108\n",
      "train loss:0.005923776303418808, test loss:0.360702050374191\n",
      "train loss:0.005919756832532839, test loss:0.36068814852926384\n",
      "train loss:0.0059157399619010065, test loss:0.360672126197918\n",
      "train loss:0.005911728501877178, test loss:0.36064775722414283\n",
      "train loss:0.005907717989196712, test loss:0.3606392657690619\n",
      "train loss:0.005903715191076267, test loss:0.3606168602987547\n",
      "train loss:0.00589971052199458, test loss:0.36059792975667887\n",
      "train loss:0.005895712379593868, test loss:0.3605862255183068\n",
      "train loss:0.005891719114958154, test loss:0.3605700887360217\n",
      "train loss:0.005883733591567184, test loss:0.3605422113675502\n",
      "train loss:0.005879749888603969, test loss:0.360522258485339\n",
      "train loss:0.005875768343383173, test loss:0.36050464755764555\n",
      "train loss:0.005871789813196433, test loss:0.3604874085760107\n",
      "train loss:0.005867813531407556, test loss:0.3604626935881319\n",
      "train loss:0.005863841384004841, test loss:0.3604557284977069\n",
      "train loss:0.0058598710582188135, test loss:0.3604339989086199\n",
      "train loss:0.005855900744499155, test loss:0.3604168702884664\n",
      "train loss:0.005851939963465257, test loss:0.3604043034049889\n",
      "train loss:0.00584402003867905, test loss:0.3603728990587226\n",
      "train loss:0.0058400651718857645, test loss:0.3603526172296893\n",
      "train loss:0.005836115903089718, test loss:0.3603373552275556\n",
      "train loss:0.0058321660778153555, test loss:0.36031618639020996\n",
      "train loss:0.005828224132660469, test loss:0.36030182559985596\n",
      "train loss:0.005824280655926321, test loss:0.3602820991530092\n",
      "train loss:0.005820341997651798, test loss:0.3602712356974442\n",
      "train loss:0.005816406704008362, test loss:0.36024430717830835\n",
      "train loss:0.005812473671247513, test loss:0.3602354337538807\n",
      "train loss:0.00580461295586653, test loss:0.3601894152332597\n",
      "train loss:0.005800694104534733, test loss:0.36017869872076114\n",
      "train loss:0.005796772435373637, test loss:0.36015282393448694\n",
      "train loss:0.005792853500712531, test loss:0.3601393096765784\n",
      "train loss:0.0057889366360871, test loss:0.3601222022164343\n",
      "train loss:0.005785027013899986, test loss:0.36009686425870696\n",
      "train loss:0.005781117656592003, test loss:0.3600902479814548\n",
      "train loss:0.00577721863293577, test loss:0.3600644604586681\n",
      "train loss:0.005773311346040444, test loss:0.3600521796783414\n",
      "train loss:0.005765519586659919, test loss:0.36001905586193655\n",
      "train loss:0.005761626827141193, test loss:0.3600188655194114\n",
      "train loss:0.0057577376729692304, test loss:0.35999373886402924\n",
      "train loss:0.005753848452427668, test loss:0.3599790052936303\n",
      "train loss:0.005749966301843202, test loss:0.35997009607052893\n",
      "train loss:0.005746084465587537, test loss:0.3599467068643961\n",
      "train loss:0.005742209927845207, test loss:0.3599400942693233\n",
      "train loss:0.005738335802986352, test loss:0.3599266149470272\n",
      "train loss:0.005734463556108704, test loss:0.3599114630032869\n",
      "train loss:0.00572673045628871, test loss:0.359876679619125\n",
      "train loss:0.005722872168487155, test loss:0.35987704410586147\n",
      "train loss:0.00571900535067071, test loss:0.3598619467199691\n",
      "train loss:0.00571515498900053, test loss:0.35983258469764295\n",
      "train loss:0.005711298461305483, test loss:0.3598267275075628\n",
      "train loss:0.005707450598453709, test loss:0.359819531803259\n",
      "train loss:0.005703603900544999, test loss:0.3597868146599554\n",
      "train loss:0.005699761006228204, test loss:0.35978382348973953\n",
      "train loss:0.005695918887411582, test loss:0.3597773901871002\n",
      "train loss:0.005688245121357451, test loss:0.3597391029620043\n",
      "train loss:0.00568441017071861, test loss:0.3597346899335888\n",
      "train loss:0.005680582364361222, test loss:0.3597019819334047\n",
      "train loss:0.00567675126273663, test loss:0.35969399850782474\n",
      "train loss:0.005672928173718669, test loss:0.359690932592948\n",
      "train loss:0.005669107067583914, test loss:0.35966031814867916\n",
      "train loss:0.005665285914607108, test loss:0.3596519663342218\n",
      "train loss:0.005661470325069575, test loss:0.3596434950435907\n",
      "train loss:0.005657658738311813, test loss:0.3596141736687929\n",
      "train loss:0.005650042159144849, test loss:0.35959810128735653\n",
      "train loss:0.005646238266373958, test loss:0.35957090511020967\n",
      "train loss:0.005642434994429145, test loss:0.3595659638210795\n",
      "train loss:0.005638639037573613, test loss:0.35955627861910544\n",
      "train loss:0.0056348412558776696, test loss:0.3595289895589174\n",
      "train loss:0.0056310502087493835, test loss:0.35952306507902904\n",
      "train loss:0.005627258753085951, test loss:0.3595140214016044\n",
      "train loss:0.005623471414311834, test loss:0.3594893666307879\n",
      "train loss:0.005619687167409256, test loss:0.3594856471139411\n",
      "train loss:0.005612126870063049, test loss:0.35944978365210706\n",
      "train loss:0.005608350573173538, test loss:0.359444820186425\n",
      "train loss:0.0056045746789951225, test loss:0.35942538382055816\n",
      "train loss:0.005600810778518687, test loss:0.3594083071215313\n",
      "train loss:0.005597040963848151, test loss:0.3593961676374405\n",
      "train loss:0.005593272746760204, test loss:0.3593832299906812\n",
      "train loss:0.005589514071554962, test loss:0.3593694496721071\n",
      "train loss:0.005585752007989901, test loss:0.359361941930458\n",
      "train loss:0.005581999723609834, test loss:0.35934422418403683\n",
      "train loss:0.005574492597833606, test loss:0.35932057307231446\n",
      "train loss:0.005570743426951691, test loss:0.3593136903539764\n",
      "train loss:0.00556699921439124, test loss:0.3592998859564948\n",
      "train loss:0.00556325875158669, test loss:0.35928370186910913\n",
      "train loss:0.005559518566800497, test loss:0.35927449758344565\n",
      "train loss:0.005555779677912749, test loss:0.35926001229266835\n",
      "train loss:0.0055520473496224534, test loss:0.3592523523005917\n",
      "train loss:0.005548314068451443, test loss:0.35924000904925046\n",
      "train loss:0.005544589170734314, test loss:0.35923264942922184\n",
      "train loss:0.005537138821652712, test loss:0.3592120739215164\n",
      "train loss:0.005533420743799662, test loss:0.3592040224396487\n",
      "train loss:0.005529702941342004, test loss:0.35918700520600044\n",
      "train loss:0.005525986775011875, test loss:0.35918523341020137\n",
      "train loss:0.005522276547500005, test loss:0.3591644197017935\n",
      "train loss:0.005518567535920136, test loss:0.3591572987700113\n",
      "train loss:0.005514857240679362, test loss:0.3591514442424237\n",
      "train loss:0.005511155785836926, test loss:0.35914095252733536\n",
      "train loss:0.005507456406221766, test loss:0.35913658460667497\n",
      "train loss:0.0055000622908708535, test loss:0.3591172138478477\n",
      "train loss:0.005496370954934105, test loss:0.3591178755799146\n",
      "train loss:0.005492684384421278, test loss:0.35909707441613553\n",
      "train loss:0.0054889956053502924, test loss:0.3590902885572891\n",
      "train loss:0.005485310311611532, test loss:0.3590794641162049\n",
      "train loss:0.005481633018743703, test loss:0.3590680782288109\n",
      "train loss:0.005477956634322159, test loss:0.3590649066793298\n",
      "train loss:0.0054742792302542274, test loss:0.35904731529573014\n",
      "train loss:0.0054706084170241505, test loss:0.35904586966817365\n",
      "train loss:0.005463271053693346, test loss:0.3590182220944865\n",
      "train loss:0.005459606514098161, test loss:0.3590109754920274\n",
      "train loss:0.005455944244291325, test loss:0.3589948267986934\n",
      "train loss:0.005452284798531493, test loss:0.3589903000825841\n",
      "train loss:0.005448633080264452, test loss:0.35897601982589566\n",
      "train loss:0.0054449776257126615, test loss:0.3589709189098662\n",
      "train loss:0.005441325989383622, test loss:0.3589619404984338\n",
      "train loss:0.005437679696610522, test loss:0.358944820837045\n",
      "train loss:0.005434036868525444, test loss:0.3589435043999287\n",
      "train loss:0.005426751762769866, test loss:0.3589106673654627\n",
      "train loss:0.005423112423959755, test loss:0.3589022401448427\n",
      "train loss:0.005419483157144082, test loss:0.3588830549856648\n",
      "train loss:0.005415850337572511, test loss:0.35888441714161323\n",
      "train loss:0.005412218446749477, test loss:0.3588693284057141\n",
      "train loss:0.0054085960013379964, test loss:0.35885907558451924\n",
      "train loss:0.005404971829977498, test loss:0.35885437612727716\n",
      "train loss:0.005401351014661657, test loss:0.35883098804313346\n",
      "train loss:0.005397733836250636, test loss:0.3588205285163899\n",
      "train loss:0.005390506037179717, test loss:0.35878415988367257\n",
      "train loss:0.005386895365112345, test loss:0.35877962816132936\n",
      "train loss:0.005383290558133725, test loss:0.3587629383061387\n",
      "train loss:0.00537968635116415, test loss:0.35873832351309104\n",
      "train loss:0.005376085400039383, test loss:0.35873559142565054\n",
      "train loss:0.005372489097577353, test loss:0.3587145987636406\n",
      "train loss:0.005368894969660238, test loss:0.35869288698344903\n",
      "train loss:0.005365299622720421, test loss:0.3586820356871269\n",
      "train loss:0.005361707434945975, test loss:0.3586577708232594\n",
      "train loss:0.005354538165377602, test loss:0.3586265463878316\n",
      "train loss:0.005350955377141931, test loss:0.358603196183869\n",
      "train loss:0.005347376761751027, test loss:0.3585876776393205\n",
      "train loss:0.005343798213983166, test loss:0.35856660135091606\n",
      "train loss:0.005340223412666473, test loss:0.3585497899732202\n",
      "train loss:0.00533665276943805, test loss:0.3585377155607757\n",
      "train loss:0.005333084839624026, test loss:0.35851568826327085\n",
      "train loss:0.0053295115262285255, test loss:0.35851134145742763\n",
      "train loss:0.005325949734357091, test loss:0.35849120683303415\n",
      "train loss:0.005318830817393978, test loss:0.358463489959747\n",
      "train loss:0.005315277323402649, test loss:0.3584455512588153\n",
      "train loss:0.005311722564377588, test loss:0.3584296828495018\n",
      "train loss:0.005308173623548034, test loss:0.35842419703842743\n",
      "train loss:0.005304625667919916, test loss:0.358401786217818\n",
      "train loss:0.0053010834600945396, test loss:0.35839927930684556\n",
      "train loss:0.005297541817908091, test loss:0.35837494247347235\n",
      "train loss:0.005294009212915247, test loss:0.35836483072680525\n",
      "train loss:0.005290468665294194, test loss:0.3583494171742669\n",
      "train loss:0.005283402651402129, test loss:0.3583267761820754\n",
      "train loss:0.005279875388848951, test loss:0.35831555015457855\n",
      "train loss:0.005276346516354331, test loss:0.3583041290417815\n",
      "train loss:0.005272822656562134, test loss:0.35829139947151056\n",
      "train loss:0.005269305656172424, test loss:0.3582793000551737\n",
      "train loss:0.0052657889657188, test loss:0.3582634145836646\n",
      "train loss:0.005262271985023891, test loss:0.35825341723176\n",
      "train loss:0.005258757225130834, test loss:0.35823598974671267\n",
      "train loss:0.005255248392723178, test loss:0.35822956520059146\n",
      "train loss:0.0052482380501043474, test loss:0.358206558616415\n",
      "train loss:0.0052447308124789534, test loss:0.35819248584628854\n",
      "train loss:0.005241233596018266, test loss:0.3581789970395696\n",
      "train loss:0.005237739294948026, test loss:0.358168039995624\n",
      "train loss:0.00523424118487705, test loss:0.3581481719937952\n",
      "train loss:0.005230749599698047, test loss:0.35813326783521926\n",
      "train loss:0.0052272610315134945, test loss:0.35811979393397436\n",
      "train loss:0.005223770563902965, test loss:0.35809892366286844\n",
      "train loss:0.005220290786929915, test loss:0.3580899094305778\n",
      "train loss:0.005213332587395287, test loss:0.358053829362218\n",
      "train loss:0.005209851959078669, test loss:0.35804924865771737\n",
      "train loss:0.005206380019935383, test loss:0.35803125488068993\n",
      "train loss:0.005202909176666277, test loss:0.35800824458841807\n",
      "train loss:0.005199438793017942, test loss:0.3580004912950663\n",
      "train loss:0.005195974615769102, test loss:0.35798214015966195\n",
      "train loss:0.00519250962243927, test loss:0.3579667205638354\n",
      "train loss:0.005189049764824982, test loss:0.3579569537129348\n",
      "train loss:0.005185595774593188, test loss:0.35793510246276966\n",
      "train loss:0.005178687374706478, test loss:0.3579082300650537\n",
      "train loss:0.00517523872961575, test loss:0.35788117065845104\n",
      "train loss:0.005171789941065522, test loss:0.35787188238355155\n",
      "train loss:0.0051683473102446895, test loss:0.357852197215983\n",
      "train loss:0.005164904237920798, test loss:0.3578346920822043\n",
      "train loss:0.005161464188657436, test loss:0.3578246465030098\n",
      "train loss:0.005158027263041836, test loss:0.3578068503918856\n",
      "train loss:0.005154593594768367, test loss:0.3577924665302228\n",
      "train loss:0.005151165752188988, test loss:0.3577745883993119\n",
      "train loss:0.005144307083907692, test loss:0.35774215941631554\n",
      "train loss:0.005140884436902638, test loss:0.3577235636452048\n",
      "train loss:0.005137464717097865, test loss:0.3577141466176557\n",
      "train loss:0.005134044472650469, test loss:0.3576954915794389\n",
      "train loss:0.0051306284493773295, test loss:0.3576849970479169\n",
      "train loss:0.005127215200179966, test loss:0.3576723006299642\n",
      "train loss:0.005123804038592472, test loss:0.35765371744997215\n",
      "train loss:0.005120397774101177, test loss:0.3576483497378317\n",
      "train loss:0.005116991205231075, test loss:0.35762652538170514\n",
      "train loss:0.005110190821971956, test loss:0.35760428407844175\n",
      "train loss:0.005106792138203689, test loss:0.3575903167757473\n",
      "train loss:0.005103397492768078, test loss:0.3575857408878628\n",
      "train loss:0.005100007831728573, test loss:0.3575587778897011\n",
      "train loss:0.005096617467556359, test loss:0.357555246820802\n",
      "train loss:0.005093230788870348, test loss:0.3575393587043186\n",
      "train loss:0.00508984479663173, test loss:0.3575237086929342\n",
      "train loss:0.005086463663752381, test loss:0.3575141291893611\n",
      "train loss:0.005083084411300161, test loss:0.35749794485781616\n",
      "train loss:0.005076336285573472, test loss:0.35747463577215755\n",
      "train loss:0.005072965599612699, test loss:0.35746303795163087\n",
      "train loss:0.005069598739972492, test loss:0.35744349255179275\n",
      "train loss:0.0050662297944034, test loss:0.35744613637897854\n",
      "train loss:0.005062869571602946, test loss:0.3574238489662616\n",
      "train loss:0.005059504952869196, test loss:0.35742148515989913\n",
      "train loss:0.005056148435638617, test loss:0.3574138584063572\n",
      "train loss:0.00505278888299766, test loss:0.35739209834855806\n",
      "train loss:0.005049434653624538, test loss:0.3573930117385515\n",
      "train loss:0.005042735387944309, test loss:0.3573655149671635\n",
      "train loss:0.005039390279928271, test loss:0.35735804338473676\n",
      "train loss:0.005036046518153466, test loss:0.3573464959817812\n",
      "train loss:0.005032705370696501, test loss:0.3573370784373699\n",
      "train loss:0.005029366302503642, test loss:0.3573250540255957\n",
      "train loss:0.005026029150077663, test loss:0.35731921139325157\n",
      "train loss:0.0050226965264763765, test loss:0.35730865859162825\n",
      "train loss:0.005019361943725288, test loss:0.3573018085899248\n",
      "train loss:0.005016035902129152, test loss:0.3572888817315902\n",
      "train loss:0.00500938722523064, test loss:0.3572667523533364\n",
      "train loss:0.0050060644384233586, test loss:0.35726591945406977\n",
      "train loss:0.005002747455490768, test loss:0.35725705926300155\n",
      "train loss:0.00499942976519843, test loss:0.357239666964373\n",
      "train loss:0.004996116353163913, test loss:0.3572444062796384\n",
      "train loss:0.004992802719938342, test loss:0.3572281087579643\n",
      "train loss:0.004989498288856563, test loss:0.35722163663101064\n",
      "train loss:0.004986192024326372, test loss:0.3572189316377486\n",
      "train loss:0.004982886512324155, test loss:0.3571968711767398\n",
      "train loss:0.004976284095419535, test loss:0.3571849726931153\n",
      "train loss:0.0049729901756986016, test loss:0.3571676109390451\n",
      "train loss:0.004969698164639797, test loss:0.3571681405378884\n",
      "train loss:0.00496640333684939, test loss:0.357151537336484\n",
      "train loss:0.004963113469732224, test loss:0.35713911301382945\n",
      "train loss:0.004959826730107071, test loss:0.357135972709537\n",
      "train loss:0.004956542460701688, test loss:0.35711372264676355\n",
      "train loss:0.0049532603753383595, test loss:0.35710709951823955\n",
      "train loss:0.004949983631212989, test loss:0.35709794852988125\n",
      "train loss:0.004943432028400032, test loss:0.35707726839161047\n",
      "train loss:0.004940159719887607, test loss:0.3570618361389605\n",
      "train loss:0.004936888277788155, test loss:0.35705238321370164\n",
      "train loss:0.004933620769734153, test loss:0.35704073729315206\n",
      "train loss:0.004930353849480414, test loss:0.3570287503289708\n",
      "train loss:0.0049270929386280434, test loss:0.35701348058567745\n",
      "train loss:0.004923836181860008, test loss:0.35700524043209053\n",
      "train loss:0.00492057802632316, test loss:0.35698403955201063\n",
      "train loss:0.00491732196085064, test loss:0.3569838573631931\n",
      "train loss:0.004910820208728805, test loss:0.35695559727343024\n",
      "train loss:0.0049075730373861325, test loss:0.35694869651443195\n",
      "train loss:0.00490432875614704, test loss:0.356923368307325\n",
      "train loss:0.004901086380776858, test loss:0.35692145025138416\n",
      "train loss:0.004897842545113239, test loss:0.3569071332464521\n",
      "train loss:0.004894603473936772, test loss:0.3568899533054177\n",
      "train loss:0.0048913660146547204, test loss:0.3568842632054488\n",
      "train loss:0.0048881317290120204, test loss:0.35686831333813174\n",
      "train loss:0.004884902855503133, test loss:0.3568524313662459\n",
      "train loss:0.0048784442756044264, test loss:0.35682736130873366\n",
      "train loss:0.004875220913682787, test loss:0.3568208434456473\n",
      "train loss:0.004871999763427405, test loss:0.35681341542086314\n",
      "train loss:0.004868781375532428, test loss:0.3567897640273946\n",
      "train loss:0.004865563351899263, test loss:0.3567908416724334\n",
      "train loss:0.004862347382806163, test loss:0.35676585998061794\n",
      "train loss:0.004859135411990665, test loss:0.3567554663485319\n",
      "train loss:0.004855922025720212, test loss:0.35675051520892576\n",
      "train loss:0.0048527159170163505, test loss:0.35672571946203735\n",
      "train loss:0.0048463092015591175, test loss:0.35670858528461125\n",
      "train loss:0.004843106468793241, test loss:0.3566984994091945\n",
      "train loss:0.004839910338278769, test loss:0.35668590281241364\n",
      "train loss:0.004836710121158953, test loss:0.35667493192647876\n",
      "train loss:0.004833521686213425, test loss:0.35665806443275405\n",
      "train loss:0.0048303299504694145, test loss:0.35664906793898343\n",
      "train loss:0.004827142674231853, test loss:0.3566250147708093\n",
      "train loss:0.0048239545798668725, test loss:0.3566210189361965\n",
      "train loss:0.004820771495684998, test loss:0.356603667293211\n",
      "train loss:0.0048144119960415194, test loss:0.3565841125478993\n",
      "train loss:0.00481123324297615, test loss:0.3565578384576319\n",
      "train loss:0.004808059584328558, test loss:0.35654926333250453\n",
      "train loss:0.0048048894171484014, test loss:0.3565421301392019\n",
      "train loss:0.00480172025323691, test loss:0.35651413851578206\n",
      "train loss:0.004798553947774651, test loss:0.3565050231673005\n",
      "train loss:0.004795386450393138, test loss:0.3564982323595825\n",
      "train loss:0.004792229317842749, test loss:0.3564673152222454\n",
      "train loss:0.004789066197120031, test loss:0.3564674268778509\n",
      "train loss:0.004782756508553782, test loss:0.3564267525716179\n",
      "train loss:0.00477960442843335, test loss:0.3564272594744565\n",
      "train loss:0.004776452895930664, test loss:0.35640543070805814\n",
      "train loss:0.0047733052495509, test loss:0.3563815589476727\n",
      "train loss:0.004770163533321254, test loss:0.356380322813249\n",
      "train loss:0.004767021860206352, test loss:0.35635451851420774\n",
      "train loss:0.004763878514512257, test loss:0.35633718588826724\n",
      "train loss:0.004760743925847, test loss:0.35633504546050715\n",
      "train loss:0.0047576094804688196, test loss:0.3563041944609405\n",
      "train loss:0.004751342299844092, test loss:0.35628607061802503\n",
      "train loss:0.004748215328703172, test loss:0.3562646187007948\n",
      "train loss:0.00474509459914523, test loss:0.35626095646608075\n",
      "train loss:0.004741969835760225, test loss:0.3562385467461485\n",
      "train loss:0.00473884823390532, test loss:0.3562249724223057\n",
      "train loss:0.004735728851535679, test loss:0.35621569726272\n",
      "train loss:0.0047326100436878675, test loss:0.3561944698621073\n",
      "train loss:0.00472950051487897, test loss:0.3561914488667648\n",
      "train loss:0.004726387614895093, test loss:0.3561640368512965\n",
      "train loss:0.004720169617165987, test loss:0.356141762897911\n",
      "train loss:0.004717064166636632, test loss:0.35612350220644073\n",
      "train loss:0.004713962358212042, test loss:0.35612352073891335\n",
      "train loss:0.0047108589418250755, test loss:0.35609596793405074\n",
      "train loss:0.004707762113179066, test loss:0.35609314548249327\n",
      "train loss:0.0047046625798888774, test loss:0.3560817727026617\n",
      "train loss:0.0047015710978649565, test loss:0.356060261033419\n",
      "train loss:0.00469847379073581, test loss:0.3560593095899709\n",
      "train loss:0.004695386897920033, test loss:0.3560363923470478\n",
      "train loss:0.004689213260938517, test loss:0.3560190577315769\n",
      "train loss:0.004686129027765344, test loss:0.3559960834651055\n",
      "train loss:0.00468304685128913, test loss:0.35599078362917586\n",
      "train loss:0.004679970244097179, test loss:0.35597687801942435\n",
      "train loss:0.004676892105251471, test loss:0.3559595295063543\n",
      "train loss:0.004673816970609386, test loss:0.35595589332819033\n",
      "train loss:0.0046707436040827445, test loss:0.355928311971151\n",
      "train loss:0.004667671518213091, test loss:0.3559197605297331\n",
      "train loss:0.004664607346063934, test loss:0.35591123579390416\n",
      "train loss:0.0046584768400393835, test loss:0.35588483003185034\n",
      "train loss:0.004655412320066005, test loss:0.355869742201283\n",
      "train loss:0.004652356017733985, test loss:0.35585078801719194\n",
      "train loss:0.0046492990294741115, test loss:0.35584351630002803\n",
      "train loss:0.004646244984394427, test loss:0.3558230768388375\n",
      "train loss:0.0046431969378716315, test loss:0.35581105823953346\n",
      "train loss:0.004640142316339114, test loss:0.3557973143873565\n",
      "train loss:0.004637100995481834, test loss:0.35578205093802534\n",
      "train loss:0.004634049888234094, test loss:0.35577125871596016\n",
      "train loss:0.004627967513642447, test loss:0.35574567024028086\n",
      "train loss:0.004624931659057526, test loss:0.3557236994212164\n",
      "train loss:0.004621896474324143, test loss:0.3557174967695772\n",
      "train loss:0.0046188634666068625, test loss:0.35570424480010354\n",
      "train loss:0.0046158294467918285, test loss:0.3556902554504446\n",
      "train loss:0.004612801413658143, test loss:0.3556823135716264\n",
      "train loss:0.004609772038684509, test loss:0.3556675239295615\n",
      "train loss:0.004606746907619176, test loss:0.35565477018187325\n",
      "train loss:0.004603727346740127, test loss:0.35564250235963113\n",
      "train loss:0.004597686367637519, test loss:0.35562197819689173\n",
      "train loss:0.004594670421243042, test loss:0.355601307339982\n",
      "train loss:0.004591655294890872, test loss:0.3556012585239779\n",
      "train loss:0.0045886435133855735, test loss:0.3555834676054922\n",
      "train loss:0.004585634837556237, test loss:0.35557322156045296\n",
      "train loss:0.004582626741321522, test loss:0.3555688376680838\n",
      "train loss:0.004579620272798476, test loss:0.35554768905618445\n",
      "train loss:0.004576616068297944, test loss:0.3555431711563363\n",
      "train loss:0.00457361319885656, test loss:0.3555316331786413\n",
      "train loss:0.004567619030589517, test loss:0.3555044002024124\n",
      "train loss:0.004564621101143766, test loss:0.3554882009553761\n",
      "train loss:0.004561630887519155, test loss:0.3554753096225867\n",
      "train loss:0.004558638092639535, test loss:0.35545741159310784\n",
      "train loss:0.0045556495632898255, test loss:0.35544539668089764\n",
      "train loss:0.004552660011004427, test loss:0.35542927548512043\n",
      "train loss:0.004549678964477266, test loss:0.35541312082617105\n",
      "train loss:0.004546696907182822, test loss:0.3554008168753869\n",
      "train loss:0.004543716449630006, test loss:0.355380071413161\n",
      "train loss:0.004537763602446519, test loss:0.35535293662174794\n",
      "train loss:0.00453478900749758, test loss:0.35533681682968526\n",
      "train loss:0.004531816588398181, test loss:0.35532270528590254\n",
      "train loss:0.004528849163924514, test loss:0.35530382043477143\n",
      "train loss:0.0045258786217228395, test loss:0.35529021174378395\n",
      "train loss:0.004522919037893585, test loss:0.3552676417539493\n",
      "train loss:0.004519951549618845, test loss:0.3552553517644138\n",
      "train loss:0.00451699267642819, test loss:0.3552393155038227\n",
      "train loss:0.004514034705958663, test loss:0.355219609689666\n",
      "train loss:0.004508124411953465, test loss:0.35518647745554766\n",
      "train loss:0.004505173718204832, test loss:0.35517650835982545\n",
      "train loss:0.0045022234269363704, test loss:0.355151557226974\n",
      "train loss:0.004499273636653258, test loss:0.3551439320624237\n",
      "train loss:0.00449632932742982, test loss:0.3551242905517842\n",
      "train loss:0.004493385287197514, test loss:0.3551089559201095\n",
      "train loss:0.00449044576195496, test loss:0.3550920837078935\n",
      "train loss:0.004487503442717829, test loss:0.3550773889859999\n",
      "train loss:0.0044845708784084365, test loss:0.3550633905340887\n",
      "train loss:0.004478705355033462, test loss:0.3550391860273261\n",
      "train loss:0.004475774691113845, test loss:0.3550187338716033\n",
      "train loss:0.004472847253995811, test loss:0.35500984967198035\n",
      "train loss:0.004469922493389655, test loss:0.35499242626447314\n",
      "train loss:0.004467000523896662, test loss:0.35497880226055406\n",
      "train loss:0.004464077574821807, test loss:0.3549659160617062\n",
      "train loss:0.004461160871440545, test loss:0.35494975737507783\n",
      "train loss:0.004458246497577584, test loss:0.35494160815218023\n",
      "train loss:0.004455330511194781, test loss:0.3549251956578128\n",
      "train loss:0.004449511302949268, test loss:0.35490164412299086\n",
      "train loss:0.0044466034830636745, test loss:0.35488022607650643\n",
      "train loss:0.004443699675427423, test loss:0.35487436672896183\n",
      "train loss:0.004440791637158463, test loss:0.35484628945691976\n",
      "train loss:0.00443789332268439, test loss:0.3548440101885482\n",
      "train loss:0.004434994968056989, test loss:0.3548268363511441\n",
      "train loss:0.004432097269491229, test loss:0.3548063180999027\n",
      "train loss:0.004429200831096872, test loss:0.35480051511417876\n",
      "train loss:0.00442630669732843, test loss:0.3547862941271873\n",
      "train loss:0.00442053037589158, test loss:0.3547604019132666\n",
      "train loss:0.004417638731586016, test loss:0.3547411735730861\n",
      "train loss:0.004414755667332409, test loss:0.3547237997027385\n",
      "train loss:0.004411873371292164, test loss:0.354717629695937\n",
      "train loss:0.004408992450091639, test loss:0.3546935322092012\n",
      "train loss:0.004406116855222016, test loss:0.354687457201508\n",
      "train loss:0.004403236293770516, test loss:0.3546770027436429\n",
      "train loss:0.004400363852495255, test loss:0.35465258047809656\n",
      "train loss:0.004397490339504429, test loss:0.3546466764510265\n",
      "train loss:0.004391753979340069, test loss:0.3546086612273243\n",
      "train loss:0.004388885987922313, test loss:0.3546003921598224\n",
      "train loss:0.004386025134202241, test loss:0.3545907453242331\n",
      "train loss:0.004383162057795339, test loss:0.35456235526204566\n",
      "train loss:0.004380306055612295, test loss:0.3545502488060066\n",
      "train loss:0.004377446029850219, test loss:0.3545428100881282\n",
      "train loss:0.004374593472044559, test loss:0.3545094774136072\n",
      "train loss:0.004371738702590355, test loss:0.3545020985817262\n",
      "train loss:0.0043688885912642, test loss:0.35449251355997136\n",
      "train loss:0.004363193374066298, test loss:0.3544519832948681\n",
      "train loss:0.004360347661230269, test loss:0.3544457158725401\n",
      "train loss:0.00435750452625799, test loss:0.354421102595867\n",
      "train loss:0.004354665077218212, test loss:0.35441295071090567\n",
      "train loss:0.004351826936587709, test loss:0.35440679919337625\n",
      "train loss:0.004348988730513584, test loss:0.3543791936887006\n",
      "train loss:0.004346152610104999, test loss:0.3543733147635403\n",
      "train loss:0.004343320385514179, test loss:0.3543673596091165\n",
      "train loss:0.0043404895885708365, test loss:0.3543354878725068\n",
      "train loss:0.004334837502539672, test loss:0.35432299224342384\n",
      "train loss:0.004332010041243639, test loss:0.3542917533429287\n",
      "train loss:0.004329190434772494, test loss:0.35428203544503756\n",
      "train loss:0.004326370608672542, test loss:0.3542778059424626\n",
      "train loss:0.0043235513028761635, test loss:0.3542471956490226\n",
      "train loss:0.004320733011034376, test loss:0.3542435853706061\n",
      "train loss:0.004317917644243156, test loss:0.3542353077774328\n",
      "train loss:0.004315106856078767, test loss:0.35420659601747206\n",
      "train loss:0.0043122973336268385, test loss:0.35419939439328296\n",
      "train loss:0.004306682064705175, test loss:0.3541602953748684\n",
      "train loss:0.004303877599958003, test loss:0.35414818273592874\n",
      "train loss:0.0043010792763305185, test loss:0.35413735227557125\n",
      "train loss:0.004298281245312745, test loss:0.3541093723752847\n",
      "train loss:0.00429547931509352, test loss:0.35409667662961325\n",
      "train loss:0.004292684179644205, test loss:0.3540931212179135\n",
      "train loss:0.004289890577944856, test loss:0.354064490814487\n",
      "train loss:0.004287100692374624, test loss:0.354051952543253\n",
      "train loss:0.004284308283802513, test loss:0.3540471788108042\n",
      "train loss:0.004278737745859175, test loss:0.353998828402826\n",
      "train loss:0.004275956332659178, test loss:0.3539961509768591\n",
      "train loss:0.004273173293895692, test loss:0.35396977871936436\n",
      "train loss:0.004270394280867575, test loss:0.3539467981364849\n",
      "train loss:0.004267618374889222, test loss:0.35394522828565217\n",
      "train loss:0.004264843231193757, test loss:0.3539193466983178\n",
      "train loss:0.004262068739497972, test loss:0.35390053096984303\n",
      "train loss:0.004259297734743729, test loss:0.35389781873169884\n",
      "train loss:0.0042565311665523865, test loss:0.3538692118347573\n",
      "train loss:0.004250997675824328, test loss:0.35384624761722594\n",
      "train loss:0.004248233395681508, test loss:0.3538194062236738\n",
      "train loss:0.004245476227156258, test loss:0.3537971781345193\n",
      "train loss:0.004242714435939562, test loss:0.3537959194427222\n",
      "train loss:0.0042399598699209345, test loss:0.3537697706381824\n",
      "train loss:0.004237203453101958, test loss:0.35375429829910315\n",
      "train loss:0.004234452284155429, test loss:0.3537506221976469\n",
      "train loss:0.004231702882445973, test loss:0.35371912421739016\n",
      "train loss:0.0042289538036550485, test loss:0.35370927105752553\n",
      "train loss:0.00422345971963525, test loss:0.35366665591563834\n",
      "train loss:0.004220718426341906, test loss:0.353664674251431\n",
      "train loss:0.004217977084912304, test loss:0.3536534867622845\n",
      "train loss:0.004215240678834044, test loss:0.35362377579837806\n",
      "train loss:0.004212503051630627, test loss:0.35362607557891124\n",
      "train loss:0.0042097677402062016, test loss:0.353615906788026\n",
      "train loss:0.004207036023879484, test loss:0.3535864689693551\n",
      "train loss:0.0042043069460944065, test loss:0.353585173899357\n",
      "train loss:0.004201578142461033, test loss:0.353579481955503\n",
      "train loss:0.004196125002597055, test loss:0.353544489638888\n",
      "train loss:0.004193402620007698, test loss:0.3535421064365537\n",
      "train loss:0.004190683854389486, test loss:0.35351563936872177\n",
      "train loss:0.004187962506531482, test loss:0.35351067379805967\n",
      "train loss:0.004185246998458874, test loss:0.35350588281616036\n",
      "train loss:0.0041825309773480795, test loss:0.35348040053943286\n",
      "train loss:0.004179816979746982, test loss:0.3534786631666576\n",
      "train loss:0.004177104815124362, test loss:0.35346957398888407\n",
      "train loss:0.004174396205902172, test loss:0.3534453710990489\n",
      "train loss:0.004168985309695562, test loss:0.35343530977416004\n",
      "train loss:0.0041662833137268655, test loss:0.3534175378747244\n",
      "train loss:0.004163582534976067, test loss:0.3534147934583377\n",
      "train loss:0.004160881987701949, test loss:0.3533982878733716\n",
      "train loss:0.004158182848831031, test loss:0.3533855874246085\n",
      "train loss:0.00415548939021485, test loss:0.3533766019690446\n",
      "train loss:0.004152796654178741, test loss:0.35335608688348635\n",
      "train loss:0.004150104265267199, test loss:0.3533509734006716\n",
      "train loss:0.004147414794568622, test loss:0.3533412391592109\n",
      "train loss:0.0041420435468255915, test loss:0.3533226554894005\n",
      "train loss:0.004139358413572571, test loss:0.35329781227315654\n",
      "train loss:0.00413667619872089, test loss:0.35329385642974415\n",
      "train loss:0.004133998353479804, test loss:0.3532833569358667\n",
      "train loss:0.004131319429185166, test loss:0.3532611589836486\n",
      "train loss:0.004128642967779669, test loss:0.35326477289117286\n",
      "train loss:0.004125970830774578, test loss:0.3532481000162887\n",
      "train loss:0.004123298179086101, test loss:0.35323413047207863\n",
      "train loss:0.004120627650833036, test loss:0.35324096790941695\n",
      "train loss:0.004115289858963267, test loss:0.35320820366482275\n",
      "train loss:0.004112627327483202, test loss:0.35320988662161\n",
      "train loss:0.004109964560518761, test loss:0.353187346068441\n",
      "train loss:0.004107302069188869, test loss:0.3531815673165563\n",
      "train loss:0.004104644282790119, test loss:0.35317525424986296\n",
      "train loss:0.0041019883856526305, test loss:0.3531625880646616\n",
      "train loss:0.00409933287512581, test loss:0.35315433370358457\n",
      "train loss:0.004096681239999518, test loss:0.3531465478642542\n",
      "train loss:0.004094032082051975, test loss:0.3531305303641097\n",
      "train loss:0.0040887342467190715, test loss:0.3531072471487497\n",
      "train loss:0.004086090907812458, test loss:0.3531054881201429\n",
      "train loss:0.004083447003753513, test loss:0.35308459064225023\n",
      "train loss:0.00408080560173576, test loss:0.3530831395031604\n",
      "train loss:0.004078168237943299, test loss:0.3530690896725467\n",
      "train loss:0.004075530876118198, test loss:0.3530534580347258\n",
      "train loss:0.004072894945108248, test loss:0.35305934772544245\n",
      "train loss:0.004070260724347007, test loss:0.353032691560516\n",
      "train loss:0.004067630961108543, test loss:0.3530321967403657\n",
      "train loss:0.004062373296517162, test loss:0.35299358733383196\n",
      "train loss:0.004059748918553615, test loss:0.35300042313094565\n",
      "train loss:0.004057119702415412, test loss:0.35298226865197063\n",
      "train loss:0.0040545010573752975, test loss:0.35296592372608576\n",
      "train loss:0.00405188157821332, test loss:0.3529703583884442\n",
      "train loss:0.004049262156320811, test loss:0.35295143077164465\n",
      "train loss:0.004046646214997881, test loss:0.35294132009203216\n",
      "train loss:0.00404402856446683, test loss:0.3529400332309836\n",
      "train loss:0.004041419057706205, test loss:0.3529176115470268\n",
      "train loss:0.004036201603135362, test loss:0.3529082047397171\n",
      "train loss:0.004033596192780804, test loss:0.3528810624677916\n",
      "train loss:0.004030993292553399, test loss:0.3528838134753339\n",
      "train loss:0.004028387629577894, test loss:0.35286704195583196\n",
      "train loss:0.004025788236036323, test loss:0.3528442911458414\n",
      "train loss:0.004023188488475681, test loss:0.35284697247744695\n",
      "train loss:0.004020591733942226, test loss:0.352818697252907\n",
      "train loss:0.00401799905730571, test loss:0.35281090471257587\n",
      "train loss:0.004015408454186837, test loss:0.352802672737252\n",
      "train loss:0.004010226054841066, test loss:0.3527830717598948\n",
      "train loss:0.004007638978172879, test loss:0.3527673678195405\n",
      "train loss:0.00400505325809859, test loss:0.3527511224437409\n",
      "train loss:0.004002471758877161, test loss:0.3527480267325518\n",
      "train loss:0.003999888337434047, test loss:0.35273133592621053\n",
      "train loss:0.003997310682906683, test loss:0.3527248371935608\n",
      "train loss:0.003994734186626355, test loss:0.3527090017449631\n",
      "train loss:0.0039921562728759494, test loss:0.3527016023930942\n",
      "train loss:0.00398958195427611, test loss:0.35268870125595925\n",
      "train loss:0.003984441691463543, test loss:0.35266399692665246\n",
      "train loss:0.0039818740865099865, test loss:0.3526480683185103\n",
      "train loss:0.003979309655984902, test loss:0.352642358807705\n",
      "train loss:0.003976743476237289, test loss:0.35262574584405143\n",
      "train loss:0.0039741807627795165, test loss:0.3526158343683309\n",
      "train loss:0.003971621523385929, test loss:0.35260054409877606\n",
      "train loss:0.003969062260394194, test loss:0.3525917533514572\n",
      "train loss:0.003966503133606761, test loss:0.352569406549089\n",
      "train loss:0.003963950497296583, test loss:0.3525676539820294\n",
      "train loss:0.003958846251994222, test loss:0.35253917687334907\n",
      "train loss:0.0039562967122116115, test loss:0.35252474568131886\n",
      "train loss:0.003953748001385923, test loss:0.35251208626991004\n",
      "train loss:0.003951203207853728, test loss:0.3525002256669321\n",
      "train loss:0.003948658331860496, test loss:0.35248971184225325\n",
      "train loss:0.003946116964886716, test loss:0.3524750267529961\n",
      "train loss:0.003943578240399729, test loss:0.35246042780062614\n",
      "train loss:0.003941041871689734, test loss:0.3524562111579806\n",
      "train loss:0.003938506242506756, test loss:0.35243339435550675\n",
      "train loss:0.003933437657275349, test loss:0.3524228487681803\n",
      "train loss:0.003930905162221785, test loss:0.35240935861024075\n",
      "train loss:0.003928377781422524, test loss:0.3524139324739808\n",
      "train loss:0.003925849891852427, test loss:0.35238864416520366\n",
      "train loss:0.003923324534744348, test loss:0.3523786175899168\n",
      "train loss:0.003920803010107691, test loss:0.35238339778490646\n",
      "train loss:0.003918280904681803, test loss:0.35236171913021574\n",
      "train loss:0.003915760127281084, test loss:0.3523553099537127\n",
      "train loss:0.003913243084792096, test loss:0.35235392222459155\n",
      "train loss:0.003908210563787919, test loss:0.3523304183883479\n",
      "train loss:0.003905700434483233, test loss:0.3523235859276098\n",
      "train loss:0.0039031898235501873, test loss:0.35230525768270393\n",
      "train loss:0.0039006825935205217, test loss:0.3523099981665977\n",
      "train loss:0.003898175720597632, test loss:0.35229192103717116\n",
      "train loss:0.003895669187975439, test loss:0.3522737846725974\n",
      "train loss:0.003893166507226905, test loss:0.3522807799860577\n",
      "train loss:0.0038906662375512192, test loss:0.35225565541277526\n",
      "train loss:0.003888166302871011, test loss:0.352240221807909\n",
      "train loss:0.0038831711328185217, test loss:0.35222513358185387\n",
      "train loss:0.0038806768514996976, test loss:0.35221074134873787\n",
      "train loss:0.003878181947440996, test loss:0.35221901504922043\n",
      "train loss:0.003875692062517946, test loss:0.3521955234014812\n",
      "train loss:0.0038732033869454817, test loss:0.3521870040343099\n",
      "train loss:0.003870715630281539, test loss:0.3521916584861515\n",
      "train loss:0.0038682284785375486, test loss:0.35216450776059477\n",
      "train loss:0.003865746603641508, test loss:0.3521517117022993\n",
      "train loss:0.003863266479883307, test loss:0.3521590836950933\n",
      "train loss:0.003858304772733677, test loss:0.3521203451030002\n",
      "train loss:0.003855827188149134, test loss:0.35212697526928677\n",
      "train loss:0.003853351778099897, test loss:0.35211229287988605\n",
      "train loss:0.0038508790939093215, test loss:0.3520941875344383\n",
      "train loss:0.0038484084001882595, test loss:0.3520972951628367\n",
      "train loss:0.00384593740224736, test loss:0.3520817848864596\n",
      "train loss:0.0038434704495798073, test loss:0.35206230420609963\n",
      "train loss:0.003841002703036648, test loss:0.3520664842427396\n",
      "train loss:0.003838538912713197, test loss:0.35204637661164934\n",
      "train loss:0.0038336162154594903, test loss:0.3520368564733568\n",
      "train loss:0.0038311575681732463, test loss:0.35201322792743545\n",
      "train loss:0.0038287018820662155, test loss:0.35201039093919806\n",
      "train loss:0.0038262435078191197, test loss:0.3520034881186205\n",
      "train loss:0.0038237911944223528, test loss:0.35197851238981165\n",
      "train loss:0.003821337951908362, test loss:0.35197767976925176\n",
      "train loss:0.003818888200337556, test loss:0.35196884144527857\n",
      "train loss:0.0038164402066735896, test loss:0.35194796730608474\n",
      "train loss:0.0038139925332610703, test loss:0.35194962363690185\n",
      "train loss:0.0038091035077174058, test loss:0.3519196368285317\n",
      "train loss:0.0038066644147217612, test loss:0.3519194291279544\n",
      "train loss:0.00380422589182973, test loss:0.3519008603278473\n",
      "train loss:0.0038017858397767805, test loss:0.35189148845214724\n",
      "train loss:0.003799349872507368, test loss:0.35188768838544315\n",
      "train loss:0.0037969151821300663, test loss:0.3518697558492209\n",
      "train loss:0.0037944843566965077, test loss:0.3518720845918722\n",
      "train loss:0.00379204954113162, test loss:0.35185741134950194\n",
      "train loss:0.003789620612786564, test loss:0.3518460990941199\n",
      "train loss:0.0037847675842240104, test loss:0.3518247601384188\n",
      "train loss:0.0037823445581470004, test loss:0.35181809331305225\n",
      "train loss:0.0037799206335320354, test loss:0.35181425163461494\n",
      "train loss:0.0037775004942231802, test loss:0.3517933849392572\n",
      "train loss:0.003775082910817525, test loss:0.3517935799223969\n",
      "train loss:0.0037726655049010764, test loss:0.35178080074071827\n",
      "train loss:0.0037702496632688533, test loss:0.3517712768504402\n",
      "train loss:0.003767837053979548, test loss:0.35176617735057214\n",
      "train loss:0.003765427022064963, test loss:0.35174303424000075\n",
      "train loss:0.003760610329319472, test loss:0.35172832379740365\n",
      "train loss:0.003758200720387191, test loss:0.3517132209249898\n",
      "train loss:0.0037557972897697047, test loss:0.3517088630694047\n",
      "train loss:0.0037533943069201366, test loss:0.35169087010202243\n",
      "train loss:0.0037509938056194923, test loss:0.3516885733474133\n",
      "train loss:0.0037485966979148577, test loss:0.3516730055769412\n",
      "train loss:0.0037461982366389377, test loss:0.3516576069663481\n",
      "train loss:0.0037438014691595274, test loss:0.3516576671511093\n",
      "train loss:0.0037414052026332506, test loss:0.35164105398240986\n",
      "train loss:0.0037366247905017027, test loss:0.35162994440805256\n",
      "train loss:0.0037342363635514257, test loss:0.35160648416144424\n",
      "train loss:0.0037318460287863544, test loss:0.35159406579816926\n",
      "train loss:0.0037294606649400496, test loss:0.35158759487904484\n",
      "train loss:0.003727078328688254, test loss:0.3515706455317757\n",
      "train loss:0.0037246936641945486, test loss:0.35155607839208325\n",
      "train loss:0.0037223140008997807, test loss:0.3515566285500521\n",
      "train loss:0.0037199350646054466, test loss:0.35153276888742424\n",
      "train loss:0.0037175571876284113, test loss:0.35152705089072245\n",
      "train loss:0.0037128054831088273, test loss:0.35149605711870746\n",
      "train loss:0.0037104370513793227, test loss:0.35149039984984864\n",
      "train loss:0.0037080654447958074, test loss:0.3514681777349132\n",
      "train loss:0.0037056967445041867, test loss:0.3514677823076181\n",
      "train loss:0.0037033252830540064, test loss:0.35145478600478836\n",
      "train loss:0.003700960912669031, test loss:0.3514383968532509\n",
      "train loss:0.0036985977364741205, test loss:0.3514387684729935\n",
      "train loss:0.003696236632258995, test loss:0.3514240468241821\n",
      "train loss:0.0036938741779156264, test loss:0.35141351731122106\n",
      "train loss:0.003689159917562841, test loss:0.35139706350560423\n",
      "train loss:0.0036868028376203952, test loss:0.35139545319847915\n",
      "train loss:0.0036844499339535996, test loss:0.3513972295117528\n",
      "train loss:0.003682097544267407, test loss:0.3513762862105726\n",
      "train loss:0.0036797468768737227, test loss:0.35138017286654477\n",
      "train loss:0.0036773973497305004, test loss:0.35137469037950037\n",
      "train loss:0.003675049735855777, test loss:0.35135682068977275\n",
      "train loss:0.003672703657621243, test loss:0.35136274681558305\n",
      "train loss:0.003670362121530587, test loss:0.35135017234095045\n",
      "train loss:0.0036656769927874773, test loss:0.3513397155022555\n",
      "train loss:0.0036633408627068514, test loss:0.351330548921449\n",
      "train loss:0.0036610032751983727, test loss:0.3513286714972881\n",
      "train loss:0.003658670182022573, test loss:0.35132288493778646\n",
      "train loss:0.0036563333130377267, test loss:0.35131189601856494\n",
      "train loss:0.0036540007759108656, test loss:0.3513184452719338\n",
      "train loss:0.0036516717155121517, test loss:0.35129696467323196\n",
      "train loss:0.0036493430207825816, test loss:0.35129781148245437\n",
      "train loss:0.003647016678343108, test loss:0.3512943422587857\n",
      "train loss:0.0036423685613215474, test loss:0.3512824514615965\n",
      "train loss:0.003640046014267396, test loss:0.3512770862941732\n",
      "train loss:0.0036377281078836038, test loss:0.35126303387018276\n",
      "train loss:0.0036354078500738696, test loss:0.35126298859920035\n",
      "train loss:0.003633091038327536, test loss:0.3512613994089911\n",
      "train loss:0.0036307768428789247, test loss:0.35125092786180734\n",
      "train loss:0.0036284604722145336, test loss:0.35124547612877244\n",
      "train loss:0.0036261498959464546, test loss:0.35124408711139016\n",
      "train loss:0.0036238401752533896, test loss:0.3512303749902506\n",
      "train loss:0.003619223339580569, test loss:0.3512203668565193\n",
      "train loss:0.0036169192359940252, test loss:0.3512076008125629\n",
      "train loss:0.0036146157840396625, test loss:0.35121010975374606\n",
      "train loss:0.003612311606992241, test loss:0.35119828787628826\n",
      "train loss:0.0036100168858095006, test loss:0.35119324460863904\n",
      "train loss:0.003607714348562979, test loss:0.35119265942797934\n",
      "train loss:0.003605417726605287, test loss:0.351181986385974\n",
      "train loss:0.003603124219729217, test loss:0.3511835236478846\n",
      "train loss:0.0036008306863184007, test loss:0.35117480352810465\n",
      "train loss:0.003596243813293622, test loss:0.35116745695582835\n",
      "train loss:0.0035939550540358104, test loss:0.35115767565334005\n",
      "train loss:0.003591669506696014, test loss:0.35115544713773517\n",
      "train loss:0.0035893861492340812, test loss:0.35114932246546476\n",
      "train loss:0.0035870993580082566, test loss:0.3511420487271211\n",
      "train loss:0.0035848188093491108, test loss:0.3511458875513159\n",
      "train loss:0.003582537734113605, test loss:0.3511273629522196\n",
      "train loss:0.0035802594309566286, test loss:0.3511334034120189\n",
      "train loss:0.0035779810685239014, test loss:0.3511242882233182\n",
      "train loss:0.0035734320194483636, test loss:0.35111650539453554\n",
      "train loss:0.0035711619847693613, test loss:0.3511022184227734\n",
      "train loss:0.0035688883802459702, test loss:0.351087177360985\n",
      "train loss:0.003566617916483535, test loss:0.35109361757473156\n",
      "train loss:0.0035643544886810596, test loss:0.3510834922310697\n",
      "train loss:0.00356208945373381, test loss:0.35106360700484945\n",
      "train loss:0.0035598219824324083, test loss:0.3510685289739735\n",
      "train loss:0.0035575624534292653, test loss:0.351062732241867\n",
      "train loss:0.003555300537305555, test loss:0.35103934069213044\n",
      "train loss:0.0035507833015784532, test loss:0.3510422746155169\n",
      "train loss:0.0035485271037814237, test loss:0.3510219554361466\n",
      "train loss:0.003546271764871987, test loss:0.35102493479726654\n",
      "train loss:0.0035440200780635033, test loss:0.351018094043965\n",
      "train loss:0.0035417672332907725, test loss:0.35099949549201254\n",
      "train loss:0.0035395176169174297, test loss:0.35100062288525574\n",
      "train loss:0.0035372713142314055, test loss:0.35099070154727996\n",
      "train loss:0.0035350238354038346, test loss:0.35097762286019457\n",
      "train loss:0.003532778225869135, test loss:0.35097942289988393\n",
      "train loss:0.0035282909903780577, test loss:0.35096176218503555\n",
      "train loss:0.0035260538628955813, test loss:0.35096541028444067\n",
      "train loss:0.003523816127840967, test loss:0.3509491983710483\n",
      "train loss:0.0035215787031629663, test loss:0.3509503553422236\n",
      "train loss:0.0035193441966953815, test loss:0.350942350538266\n",
      "train loss:0.0035171097054539783, test loss:0.35093060654485764\n",
      "train loss:0.003514877489938419, test loss:0.3509302680417523\n",
      "train loss:0.003512646535747908, test loss:0.350921282792796\n",
      "train loss:0.0035104178197578333, test loss:0.35091554386868373\n",
      "train loss:0.003505965347628558, test loss:0.3509046272274531\n",
      "train loss:0.003503738574382515, test loss:0.35090088345644765\n",
      "train loss:0.0035015162540187926, test loss:0.3509011555925436\n",
      "train loss:0.0034992944412651477, test loss:0.35089281835508457\n",
      "train loss:0.0034970777743976245, test loss:0.3508892570305485\n",
      "train loss:0.0034948572303479314, test loss:0.35089044719386675\n",
      "train loss:0.003492642866827492, test loss:0.3508772953147691\n",
      "train loss:0.0034904268440817264, test loss:0.35088430585173463\n",
      "train loss:0.0034882123230720718, test loss:0.3508697212545377\n",
      "train loss:0.0034837893946410388, test loss:0.3508619249218946\n",
      "train loss:0.00348158042379459, test loss:0.3508534385554574\n",
      "train loss:0.003479371671441635, test loss:0.3508475398056146\n",
      "train loss:0.003477167075947582, test loss:0.35085121566078026\n",
      "train loss:0.003474963792969012, test loss:0.35083990608042653\n",
      "train loss:0.003472758488200041, test loss:0.3508394369973356\n",
      "train loss:0.003470559427960005, test loss:0.3508384139969078\n",
      "train loss:0.003468358101563258, test loss:0.35082537876630726\n",
      "train loss:0.0034661611665305703, test loss:0.35082806678108036\n",
      "train loss:0.003461766849756828, test loss:0.35081065425339064\n",
      "train loss:0.0034595761888243605, test loss:0.3508077897312858\n",
      "train loss:0.003457383105856225, test loss:0.35079999460387573\n",
      "train loss:0.003455191864605758, test loss:0.35079596457848367\n",
      "train loss:0.003453005210488609, test loss:0.3507941455881524\n",
      "train loss:0.003450819250542551, test loss:0.3507853017675721\n",
      "train loss:0.0034486317156705837, test loss:0.3507862406690778\n",
      "train loss:0.0034464477438900898, test loss:0.35078242748484206\n",
      "train loss:0.003444267408535258, test loss:0.3507796860751816\n",
      "train loss:0.003439909876753078, test loss:0.3507718323960977\n",
      "train loss:0.003437731315825968, test loss:0.35077213516017947\n",
      "train loss:0.0034355567035562547, test loss:0.35075942902645485\n",
      "train loss:0.0034333831053672053, test loss:0.3507554311109001\n",
      "train loss:0.0034312103098146658, test loss:0.35075513776440126\n",
      "train loss:0.003429038221768766, test loss:0.35074016548431153\n",
      "train loss:0.003426869980744239, test loss:0.35074643060315225\n",
      "train loss:0.003424701163660233, test loss:0.3507358715677341\n",
      "train loss:0.003422536473423921, test loss:0.3507283335233762\n",
      "train loss:0.00341820677400389, test loss:0.35071847507583975\n",
      "train loss:0.0034160468619139996, test loss:0.350713889997227\n",
      "train loss:0.0034138833659307782, test loss:0.3507059741348344\n",
      "train loss:0.0034117238309097635, test loss:0.35070993086111535\n",
      "train loss:0.003409567068607327, test loss:0.35070005269680304\n",
      "train loss:0.0034074089781055187, test loss:0.3507049986445077\n",
      "train loss:0.003405254821533306, test loss:0.35070104856922407\n",
      "train loss:0.0034031000758469607, test loss:0.3506973467283156\n",
      "train loss:0.0034009519127611585, test loss:0.3506958703910402\n",
      "train loss:0.003396651917399831, test loss:0.35069181615807815\n",
      "train loss:0.003394505734213564, test loss:0.3506736337127608\n",
      "train loss:0.0033923603471984753, test loss:0.3506771470564435\n",
      "train loss:0.0033902125264457786, test loss:0.3506774829602097\n",
      "train loss:0.003388072300900547, test loss:0.3506625232714638\n",
      "train loss:0.0033859305653708557, test loss:0.35067069502953535\n",
      "train loss:0.0033837906795230106, test loss:0.3506629795905637\n",
      "train loss:0.0033816541123013206, test loss:0.35065443398006624\n",
      "train loss:0.0033795151376338575, test loss:0.35065776147183797\n",
      "train loss:0.0033752447977066897, test loss:0.3506409140220758\n",
      "train loss:0.003373114013083777, test loss:0.35063897261690075\n",
      "train loss:0.00337098486635134, test loss:0.3506303534044595\n",
      "train loss:0.0033688539149749623, test loss:0.35063106376463027\n",
      "train loss:0.0033667261727451285, test loss:0.35062373393041346\n",
      "train loss:0.003364599067263854, test loss:0.3506207672711626\n",
      "train loss:0.0033624726945939987, test loss:0.350614460893139\n",
      "train loss:0.003360350864166886, test loss:0.3506071323530943\n",
      "train loss:0.0033582291800708934, test loss:0.35060868054338595\n",
      "train loss:0.0033539882523748346, test loss:0.3505960891674778\n",
      "train loss:0.003351872098968751, test loss:0.35059220869090807\n",
      "train loss:0.003349755291408746, test loss:0.3505778066928881\n",
      "train loss:0.0033476402385028245, test loss:0.3505793315645188\n",
      "train loss:0.0033455261108585503, test loss:0.35057520355379523\n",
      "train loss:0.003343416095041132, test loss:0.3505570989286183\n",
      "train loss:0.0033413064412624633, test loss:0.3505629808182709\n",
      "train loss:0.0033391982011522756, test loss:0.3505614313410472\n",
      "train loss:0.003337091008742295, test loss:0.35054386890428024\n",
      "train loss:0.003332878640287861, test loss:0.35054594332789446\n",
      "train loss:0.0033307772185699794, test loss:0.35053776218217947\n",
      "train loss:0.003328676047926552, test loss:0.35053611121489\n",
      "train loss:0.0033265769779570794, test loss:0.3505292894261102\n",
      "train loss:0.003324479266005012, test loss:0.35051994723843455\n",
      "train loss:0.0033223816430673213, test loss:0.3505170022734766\n",
      "train loss:0.0033202855545274015, test loss:0.3505019459627074\n",
      "train loss:0.003318193058468047, test loss:0.35050441427449286\n",
      "train loss:0.0033160996986215576, test loss:0.35049240161032347\n",
      "train loss:0.0033119182938668515, test loss:0.3504866227246037\n",
      "train loss:0.0033098286716689742, test loss:0.3504742248560086\n",
      "train loss:0.0033077448598970464, test loss:0.3504816268711415\n",
      "train loss:0.003305655872047705, test loss:0.3504647944825826\n",
      "train loss:0.0033035708642560864, test loss:0.3504677680714102\n",
      "train loss:0.0033014903547987783, test loss:0.3504617939610544\n",
      "train loss:0.003299408380811151, test loss:0.3504479203842755\n",
      "train loss:0.0032973277217313054, test loss:0.35045133209775503\n",
      "train loss:0.0032952487537448475, test loss:0.35044202777043176\n",
      "train loss:0.003291099362964147, test loss:0.35044361152656417\n",
      "train loss:0.0032890236267324926, test loss:0.3504264184668311\n",
      "train loss:0.0032869502684278427, test loss:0.35042451506960254\n",
      "train loss:0.0032848800508771324, test loss:0.35042782023106733\n",
      "train loss:0.0032828092908754503, test loss:0.3504145979321127\n",
      "train loss:0.0032807431877851, test loss:0.3504198811974644\n",
      "train loss:0.0032786750379768213, test loss:0.35041607341595743\n",
      "train loss:0.003276607209055431, test loss:0.35041242799226335\n",
      "train loss:0.003274546683003938, test loss:0.3504120206758396\n",
      "train loss:0.003270421166358522, test loss:0.3504076645588485\n",
      "train loss:0.0032683621871904154, test loss:0.35040858542843434\n",
      "train loss:0.003266302634983398, test loss:0.35040154757591835\n",
      "train loss:0.0032642452117920854, test loss:0.35041149174180064\n",
      "train loss:0.0032621899574852655, test loss:0.35039958850987396\n",
      "train loss:0.0032601373849920436, test loss:0.3504096679262625\n",
      "train loss:0.0032580836056050966, test loss:0.3504096484731143\n",
      "train loss:0.003256033637180266, test loss:0.35040382531456615\n",
      "train loss:0.0032539816873103557, test loss:0.35041375219034127\n",
      "train loss:0.0032498879272910793, test loss:0.350407888416579\n",
      "train loss:0.003247842855267476, test loss:0.3504064685910882\n",
      "train loss:0.0032457973947600657, test loss:0.3504007189116999\n",
      "train loss:0.003243752215679623, test loss:0.3504072902745101\n",
      "train loss:0.0032417150394730304, test loss:0.3503947772212802\n",
      "train loss:0.0032396748074116103, test loss:0.3503997428318054\n",
      "train loss:0.003237636357804375, test loss:0.3503979277174392\n",
      "train loss:0.003235597172180544, test loss:0.3503919769369075\n",
      "train loss:0.003233562838397854, test loss:0.3504014783642973\n",
      "train loss:0.0032294969128835588, test loss:0.3503985689421349\n",
      "train loss:0.003227463463166467, test loss:0.35038352250015786\n",
      "train loss:0.0032254342498808926, test loss:0.3503824147034107\n",
      "train loss:0.003223407522539614, test loss:0.3503843596443625\n",
      "train loss:0.0032213784722775985, test loss:0.35037238295008943\n",
      "train loss:0.0032193542239959665, test loss:0.3503835468815482\n",
      "train loss:0.0032173294540226043, test loss:0.3503631355743136\n",
      "train loss:0.0032153061900863033, test loss:0.3503694572624196\n",
      "train loss:0.0032132848764182998, test loss:0.3503675711808731\n",
      "train loss:0.0032092436755325874, test loss:0.35036073268020984\n",
      "train loss:0.003207226654297486, test loss:0.3503446905814516\n",
      "train loss:0.003205210496393419, test loss:0.3503438360280502\n",
      "train loss:0.0032031952260967887, test loss:0.35034488054719565\n",
      "train loss:0.0032011814082636095, test loss:0.3503322356958307\n",
      "train loss:0.0031991705183564557, test loss:0.35034108184351626\n",
      "train loss:0.003197157195282923, test loss:0.3503245500504533\n",
      "train loss:0.0031951486516946467, test loss:0.3503344656980462\n",
      "train loss:0.003193142216134398, test loss:0.3503252907352578\n",
      "train loss:0.003189128747816837, test loss:0.3503222748663858\n",
      "train loss:0.003187122960381436, test loss:0.35031955456785696\n",
      "train loss:0.0031851191806746707, test loss:0.35032414271464674\n",
      "train loss:0.00318312003160061, test loss:0.35031169409464913\n",
      "train loss:0.0031811193069875535, test loss:0.3503328530760412\n",
      "train loss:0.003179120749382277, test loss:0.3503119875261015\n",
      "train loss:0.0031771224429682256, test loss:0.35031810785231343\n",
      "train loss:0.003175125885441141, test loss:0.35032770662690016\n",
      "train loss:0.0031731324037776233, test loss:0.35030205227800054\n",
      "train loss:0.003169143549240584, test loss:0.3503176078217207\n",
      "train loss:0.003167154463214844, test loss:0.35029965499897\n",
      "train loss:0.0031651642032578696, test loss:0.35030801224342145\n",
      "train loss:0.0031631747999055016, test loss:0.35030794311293945\n",
      "train loss:0.0031611876158938773, test loss:0.35029556628444974\n",
      "train loss:0.0031592040909453056, test loss:0.3502987277460191\n",
      "train loss:0.0031572215357607417, test loss:0.3503018090316608\n",
      "train loss:0.003155236606766275, test loss:0.3502836133025306\n",
      "train loss:0.0031532551534144164, test loss:0.35030274187573984\n",
      "train loss:0.0031492963973514024, test loss:0.3502888634120529\n",
      "train loss:0.0031473171269926965, test loss:0.3502989343137245\n",
      "train loss:0.00314534260662874, test loss:0.3502845870598016\n",
      "train loss:0.003143366835230913, test loss:0.3502888364632946\n",
      "train loss:0.0031413937696722423, test loss:0.3502878695948047\n",
      "train loss:0.0031394208127411886, test loss:0.35028110691688824\n",
      "train loss:0.003137450138499056, test loss:0.3502827421879217\n",
      "train loss:0.003135480597360613, test loss:0.3502696735213155\n",
      "train loss:0.0031335128212713792, test loss:0.35027876140255226\n",
      "train loss:0.0031295799238392173, test loss:0.3502699126242081\n",
      "train loss:0.003127615428655523, test loss:0.3502622149013715\n",
      "train loss:0.0031256516679144123, test loss:0.3502610312842297\n",
      "train loss:0.0031236932478042982, test loss:0.35025744401388137\n",
      "train loss:0.003121732174181023, test loss:0.35025559399248635\n",
      "train loss:0.003119773891907661, test loss:0.35024831373797405\n",
      "train loss:0.003117815882516023, test loss:0.35024710795544506\n",
      "train loss:0.0031158596350116556, test loss:0.35024015131525077\n",
      "train loss:0.003113905453909963, test loss:0.35023873353261376\n",
      "train loss:0.00310999929680544, test loss:0.3502357981204745\n",
      "train loss:0.003108050490525248, test loss:0.35021809623450845\n",
      "train loss:0.003106098676333601, test loss:0.3502278003811592\n",
      "train loss:0.0031041516936483634, test loss:0.35021752714460586\n",
      "train loss:0.0031022061292367872, test loss:0.3502153329774785\n",
      "train loss:0.0031002607621998034, test loss:0.3502216388881721\n",
      "train loss:0.003098316574549977, test loss:0.35020955642201973\n",
      "train loss:0.0030963733003052064, test loss:0.35021297365236526\n",
      "train loss:0.003094433248195404, test loss:0.35020448053736697\n",
      "train loss:0.003090557008529045, test loss:0.35019763679102206\n",
      "train loss:0.003088620686728213, test loss:0.3502036115661527\n",
      "train loss:0.0030866845034177026, test loss:0.3501910761331811\n",
      "train loss:0.0030847493584544765, test loss:0.3501999831038964\n",
      "train loss:0.0030828176185048357, test loss:0.3501864259761368\n",
      "train loss:0.0030808846046474652, test loss:0.35018850936319607\n",
      "train loss:0.0030789559508599576, test loss:0.3501883535675583\n",
      "train loss:0.003077025007371614, test loss:0.35017281553213137\n",
      "train loss:0.003075098232752321, test loss:0.350188206911908\n",
      "train loss:0.003071245995698815, test loss:0.3501670139149915\n",
      "train loss:0.0030693239874339984, test loss:0.3501633808452212\n",
      "train loss:0.0030674028143808717, test loss:0.35014319736068095\n",
      "train loss:0.003065482457187683, test loss:0.35014925841346967\n",
      "train loss:0.003063560595963519, test loss:0.3501434098165824\n",
      "train loss:0.0030616422942887425, test loss:0.35012409874424166\n",
      "train loss:0.003059725317868285, test loss:0.35013527344450374\n",
      "train loss:0.0030578091424777382, test loss:0.3501186460560948\n",
      "train loss:0.003055894552377067, test loss:0.350115760240261\n",
      "train loss:0.0030520699229450543, test loss:0.3500989204336703\n",
      "train loss:0.003050158594725854, test loss:0.3501060442939956\n",
      "train loss:0.0030482480383383376, test loss:0.3500978241275002\n",
      "train loss:0.003046341596465222, test loss:0.35009126102013155\n",
      "train loss:0.003044433849081255, test loss:0.3500927102447422\n",
      "train loss:0.003042529126195722, test loss:0.3500842041045077\n",
      "train loss:0.0030406231044114106, test loss:0.3500853586275651\n",
      "train loss:0.003038721397662691, test loss:0.3500803303471949\n",
      "train loss:0.0030368191841299488, test loss:0.3500770583325212\n",
      "train loss:0.003033018125730852, test loss:0.3500679171147863\n",
      "train loss:0.0030311199707793862, test loss:0.35007555126053824\n",
      "train loss:0.003029225673653143, test loss:0.3500625164088025\n",
      "train loss:0.003027329235512236, test loss:0.35006984703770777\n",
      "train loss:0.003025434136268191, test loss:0.3500535943944911\n",
      "train loss:0.003023541036795533, test loss:0.35006050495297386\n",
      "train loss:0.0030216484781721226, test loss:0.35005333654205223\n",
      "train loss:0.003019759130141177, test loss:0.35004536344231263\n",
      "train loss:0.003017869482790556, test loss:0.35004820466730435\n",
      "train loss:0.003014096303965045, test loss:0.35004265898922576\n",
      "train loss:0.0030122085762428245, test loss:0.3500275973949907\n",
      "train loss:0.0030103241337741137, test loss:0.3500374179602671\n",
      "train loss:0.003008442374191525, test loss:0.350022775445221\n",
      "train loss:0.0030065612271074752, test loss:0.3500253266002307\n",
      "train loss:0.00300467992175623, test loss:0.35001977010527996\n",
      "train loss:0.0030027997966907277, test loss:0.3500061966939981\n",
      "train loss:0.0030009237118021376, test loss:0.35001504984885556\n",
      "train loss:0.0029990472406068603, test loss:0.3499906309360891\n",
      "train loss:0.00299529620169303, test loss:0.34999268385987065\n",
      "train loss:0.002993426774994123, test loss:0.34996953073120374\n",
      "train loss:0.002991552546866379, test loss:0.34998020073658004\n",
      "train loss:0.0029896812301389917, test loss:0.34996905069977585\n",
      "train loss:0.002987811479266423, test loss:0.34995820588291243\n",
      "train loss:0.002985943661928499, test loss:0.3499628436658045\n",
      "train loss:0.002984075906816638, test loss:0.3499397253342013\n",
      "train loss:0.0029822100592739836, test loss:0.34994690900197933\n",
      "train loss:0.0029803467891874273, test loss:0.3499312215612896\n",
      "train loss:0.002976621021116514, test loss:0.34992537381490424\n",
      "train loss:0.0029747598572208906, test loss:0.3499040912603044\n",
      "train loss:0.002972901192422913, test loss:0.34991707581755793\n",
      "train loss:0.0029710439058196416, test loss:0.3498925046325651\n",
      "train loss:0.0029691869277759146, test loss:0.3498928973976986\n",
      "train loss:0.0029673300817722376, test loss:0.3498947606503775\n",
      "train loss:0.002965476200434994, test loss:0.34987236471749994\n",
      "train loss:0.0029636229001979673, test loss:0.3498772764197587\n",
      "train loss:0.0029617721625864774, test loss:0.3498737346393009\n",
      "train loss:0.002958072475558439, test loss:0.3498604332488373\n",
      "train loss:0.002956224814514695, test loss:0.3498473760158535\n",
      "train loss:0.0029543817870350724, test loss:0.3498293004844121\n",
      "train loss:0.002952533433139361, test loss:0.34983859361981284\n",
      "train loss:0.0029506894272234964, test loss:0.3498174183770087\n",
      "train loss:0.0029488454305586385, test loss:0.349818594322448\n",
      "train loss:0.0029470043928093103, test loss:0.3498182545674634\n",
      "train loss:0.0029451653474779804, test loss:0.3497983660554256\n",
      "train loss:0.0029433261108379677, test loss:0.3498083524957102\n",
      "train loss:0.002939649810920032, test loss:0.34979063241623287\n",
      "train loss:0.0029378168825909858, test loss:0.34978964739083007\n",
      "train loss:0.002935983055384734, test loss:0.34977514311927027\n",
      "train loss:0.002934150262184259, test loss:0.34978227661458133\n",
      "train loss:0.0029323192844084925, test loss:0.34976291371755713\n",
      "train loss:0.002930487631537212, test loss:0.3497665984432287\n",
      "train loss:0.002928661188328604, test loss:0.34975939218897545\n",
      "train loss:0.0029268322667952897, test loss:0.34974387835584225\n",
      "train loss:0.002925008525915153, test loss:0.3497492782003986\n",
      "train loss:0.002921359270847098, test loss:0.3497304656475873\n",
      "train loss:0.0029195376022323792, test loss:0.3497240294622366\n",
      "train loss:0.002917715670313934, test loss:0.34970937336810415\n",
      "train loss:0.0029158988507127152, test loss:0.3497137604907572\n",
      "train loss:0.002914079664671101, test loss:0.3496882614940213\n",
      "train loss:0.002912261684731176, test loss:0.3496971516582065\n",
      "train loss:0.002910445327226604, test loss:0.3496804996145962\n",
      "train loss:0.0029086317351185733, test loss:0.34967101221068675\n",
      "train loss:0.0029068182830695732, test loss:0.34966994411426594\n",
      "train loss:0.002903197639869607, test loss:0.34965525717466683\n",
      "train loss:0.0029013875487054466, test loss:0.3496362209298073\n",
      "train loss:0.0028995789761861487, test loss:0.3496285775915022\n",
      "train loss:0.002897773101521191, test loss:0.34963378794254213\n",
      "train loss:0.002895966839071036, test loss:0.3496107942516837\n",
      "train loss:0.0028941627503976516, test loss:0.3496149340308466\n",
      "train loss:0.0028923594152605535, test loss:0.3496012309935286\n",
      "train loss:0.002890556764346732, test loss:0.34959062887839\n",
      "train loss:0.0028887564845054394, test loss:0.3495896768646397\n",
      "train loss:0.0028851603193151575, test loss:0.3495769231284798\n",
      "train loss:0.002883362279233567, test loss:0.3495551592108599\n",
      "train loss:0.0028815667345133203, test loss:0.3495512598572432\n",
      "train loss:0.002879774054596573, test loss:0.349548436991574\n",
      "train loss:0.0028779799021307227, test loss:0.3495274054349121\n",
      "train loss:0.002876189618948977, test loss:0.349531766747177\n",
      "train loss:0.002874397656733259, test loss:0.3495118354347491\n",
      "train loss:0.0028726065617780197, test loss:0.3495050959173507\n",
      "train loss:0.0028708212561747745, test loss:0.34951007325195876\n",
      "train loss:0.002867249374941948, test loss:0.3494909314248853\n",
      "train loss:0.0028654642161681757, test loss:0.3494743889124038\n",
      "train loss:0.0028636811983894007, test loss:0.3494626502338432\n",
      "train loss:0.002861898029829671, test loss:0.3494626421239627\n",
      "train loss:0.00286011760255722, test loss:0.34944321627661407\n",
      "train loss:0.0028583385728982017, test loss:0.3494429632247444\n",
      "train loss:0.002856562168875109, test loss:0.34943072272583314\n",
      "train loss:0.0028547829506507414, test loss:0.34942418390469016\n",
      "train loss:0.0028530086750583536, test loss:0.34941501206646564\n",
      "train loss:0.0028494607109545017, test loss:0.3493922391667521\n",
      "train loss:0.002847689444678529, test loss:0.3494050537427834\n",
      "train loss:0.00284591598908217, test loss:0.34938605704729775\n",
      "train loss:0.002844145195800302, test loss:0.34937731855538007\n",
      "train loss:0.0028423774124025757, test loss:0.34938771648094225\n",
      "train loss:0.002840609859986173, test loss:0.3493639156525856\n",
      "train loss:0.0028388447954905134, test loss:0.34935841940416257\n",
      "train loss:0.0028370801154823325, test loss:0.34936392986927106\n",
      "train loss:0.002835315228785149, test loss:0.3493427259634406\n",
      "train loss:0.0028317934954325747, test loss:0.3493371023187904\n",
      "train loss:0.002830030467688562, test loss:0.34931950702769454\n",
      "train loss:0.0028282729099697174, test loss:0.3493204713216017\n",
      "train loss:0.002826513334175163, test loss:0.3493109640666559\n",
      "train loss:0.002824758021334989, test loss:0.34930838061324304\n",
      "train loss:0.0028229998408669955, test loss:0.3492977107293021\n",
      "train loss:0.0028212462349297422, test loss:0.3493012247340333\n",
      "train loss:0.002819491704183474, test loss:0.3492791192340591\n",
      "train loss:0.00281773854350667, test loss:0.3492833459660566\n",
      "train loss:0.002814237924331285, test loss:0.34926414195947547\n",
      "train loss:0.002812488776518799, test loss:0.349258293068955\n",
      "train loss:0.0028107408650369817, test loss:0.3492439034943112\n",
      "train loss:0.002808995157865134, test loss:0.34925084807501316\n",
      "train loss:0.0028072471827380167, test loss:0.3492316027090726\n",
      "train loss:0.0028055028041802173, test loss:0.34924217851845324\n",
      "train loss:0.0028037605838252557, test loss:0.3492284320253747\n",
      "train loss:0.002802017387893321, test loss:0.34922318428736776\n",
      "train loss:0.00280027869430171, test loss:0.34922597441494085\n",
      "train loss:0.0027967996815802007, test loss:0.3492169645018932\n",
      "train loss:0.002795062500332787, test loss:0.34921087121247607\n",
      "train loss:0.0027933292044468754, test loss:0.34920272314520373\n",
      "train loss:0.0027915948051436316, test loss:0.3492155011098577\n",
      "train loss:0.0027898601967747184, test loss:0.3491964493904154\n",
      "train loss:0.0027881267916438006, test loss:0.3492059223367129\n",
      "train loss:0.0027863938943880526, test loss:0.3492025222182046\n",
      "train loss:0.002784665056169445, test loss:0.34919593334115356\n",
      "train loss:0.0027829369122509616, test loss:0.349198044705294\n",
      "train loss:0.002779480452020577, test loss:0.3491881211918382\n",
      "train loss:0.002777755438222508, test loss:0.34919787961006543\n",
      "train loss:0.002776033042563709, test loss:0.349174956048914\n",
      "train loss:0.0027743085415683722, test loss:0.34918949125609955\n",
      "train loss:0.002772586812747364, test loss:0.34917760152469485\n",
      "train loss:0.0027708656326135327, test loss:0.34916800504074474\n",
      "train loss:0.0027691457914623603, test loss:0.3491755684349769\n",
      "train loss:0.0027674281167927492, test loss:0.3491630881764214\n",
      "train loss:0.002765709327460697, test loss:0.34915752876949724\n",
      "train loss:0.0027622787875613178, test loss:0.34914826604421956\n",
      "train loss:0.0027605652243162895, test loss:0.3491517236464955\n",
      "train loss:0.0027588533461244216, test loss:0.3491504743440057\n",
      "train loss:0.0027571423795581876, test loss:0.3491304608886728\n",
      "train loss:0.002755433941345864, test loss:0.34914455379126963\n",
      "train loss:0.0027537249601121668, test loss:0.34912468731576374\n",
      "train loss:0.002752017796368034, test loss:0.34911756407817685\n",
      "train loss:0.0027503117973950715, test loss:0.34912915385158216\n",
      "train loss:0.002748606454982921, test loss:0.3491104604200776\n",
      "train loss:0.0027451997238263914, test loss:0.3491151955298023\n",
      "train loss:0.002743500701730776, test loss:0.34910046286363566\n",
      "train loss:0.002741799118555027, test loss:0.3491054567676358\n",
      "train loss:0.0027400996245915492, test loss:0.34910179416597237\n",
      "train loss:0.0027384044738866372, test loss:0.34909405086821027\n",
      "train loss:0.0027367059791641698, test loss:0.3491034275832632\n",
      "train loss:0.0027350118301794524, test loss:0.34908457077010957\n",
      "train loss:0.0027333152846483713, test loss:0.34909416040172286\n",
      "train loss:0.0027316243784219294, test loss:0.3490817272667038\n",
      "train loss:0.0027282406538219318, test loss:0.3490826672948003\n",
      "train loss:0.0027265516497349683, test loss:0.3490685515144458\n",
      "train loss:0.0027248625778928362, test loss:0.3490798691896067\n",
      "train loss:0.0027231745171232007, test loss:0.34906969672242705\n",
      "train loss:0.0027214869611846084, test loss:0.3490668925008419\n",
      "train loss:0.0027198036798220756, test loss:0.34907353435988747\n",
      "train loss:0.002718119261569185, test loss:0.34905331329207934\n",
      "train loss:0.002716435355809703, test loss:0.34906135409668226\n",
      "train loss:0.0027147541485840164, test loss:0.3490566174798811\n",
      "train loss:0.0027113944389915183, test loss:0.3490526283517494\n",
      "train loss:0.0027097142366005063, test loss:0.3490406185533208\n",
      "train loss:0.0027080373337719293, test loss:0.3490359647911684\n",
      "train loss:0.002706358751638595, test loss:0.3490410506347943\n",
      "train loss:0.0027046856593052333, test loss:0.3490257696127493\n",
      "train loss:0.0027030108449057457, test loss:0.34902853613224527\n",
      "train loss:0.0027013360403706, test loss:0.3490176302916692\n",
      "train loss:0.0026996643640579522, test loss:0.3490196178874025\n",
      "train loss:0.002697993577268977, test loss:0.34901066939350667\n",
      "train loss:0.0026946549459653597, test loss:0.3489992875328725\n",
      "train loss:0.00269298792556073, test loss:0.3490077932125286\n",
      "train loss:0.0026913218944587163, test loss:0.3489931767502269\n",
      "train loss:0.002689656276198615, test loss:0.34898824292098846\n",
      "train loss:0.002687990304286376, test loss:0.3489858435687739\n",
      "train loss:0.0026863291028184693, test loss:0.3489720184537114\n",
      "train loss:0.002684666711651435, test loss:0.3489788629936698\n",
      "train loss:0.002683005212767371, test loss:0.34895846352179627\n",
      "train loss:0.0026813458593471103, test loss:0.3489673537093916\n",
      "train loss:0.0026780284265173876, test loss:0.3489510977349665\n",
      "train loss:0.002676372777400012, test loss:0.3489547306202348\n",
      "train loss:0.00267471812627084, test loss:0.3489386144791958\n",
      "train loss:0.0026730647112464694, test loss:0.3489547458179985\n",
      "train loss:0.0026714094115742517, test loss:0.34893732077499456\n",
      "train loss:0.002669755087208416, test loss:0.34893985035176917\n",
      "train loss:0.002668102939813304, test loss:0.34894319897303117\n",
      "train loss:0.002666453565530318, test loss:0.34893114655668866\n",
      "train loss:0.002664803559265662, test loss:0.3489349519922986\n",
      "train loss:0.002661509697228303, test loss:0.34892567355737897\n",
      "train loss:0.002659863893556017, test loss:0.34892646636801405\n",
      "train loss:0.0026582197526407786, test loss:0.3489157774873167\n",
      "train loss:0.0026565768896213097, test loss:0.34893248058163134\n",
      "train loss:0.002654933001364579, test loss:0.3489111001745946\n",
      "train loss:0.0026532918811017886, test loss:0.3489200478714605\n",
      "train loss:0.002651652075819209, test loss:0.34891448415081167\n",
      "train loss:0.002650012146628426, test loss:0.3489037859356902\n",
      "train loss:0.0026483753633036143, test loss:0.348907960388315\n",
      "train loss:0.002645102237643737, test loss:0.34889986269495715\n",
      "train loss:0.0026434673258994884, test loss:0.3488916155876579\n",
      "train loss:0.002641835749551183, test loss:0.3488915995966314\n",
      "train loss:0.0026402032613151924, test loss:0.3488866961683181\n",
      "train loss:0.002638573257641955, test loss:0.34888518795363566\n",
      "train loss:0.0026369431572993572, test loss:0.34887608526276115\n",
      "train loss:0.0026353149999868574, test loss:0.3488808850235876\n",
      "train loss:0.0026336890707162, test loss:0.3488568769517734\n",
      "train loss:0.0026320619187073396, test loss:0.3488758387578356\n",
      "train loss:0.002628810472805877, test loss:0.34884955299376663\n",
      "train loss:0.0026271879804745644, test loss:0.34885716359906427\n",
      "train loss:0.0026255666275809904, test loss:0.3488340518407034\n",
      "train loss:0.0026239436855828003, test loss:0.3488340048244105\n",
      "train loss:0.0026223234983698456, test loss:0.34883652889633243\n",
      "train loss:0.0026207053003706924, test loss:0.3488160641877972\n",
      "train loss:0.0026190870529193533, test loss:0.34882886231489774\n",
      "train loss:0.0026174710161321058, test loss:0.34881734680026794\n",
      "train loss:0.0026158551769435603, test loss:0.348810886396233\n",
      "train loss:0.002612628765209248, test loss:0.3487960472482403\n",
      "train loss:0.00261101699876338, test loss:0.3488043782478102\n",
      "train loss:0.0026094052999601266, test loss:0.34879794520986207\n",
      "train loss:0.0026077962958095647, test loss:0.34877761764064363\n",
      "train loss:0.0026061862110743077, test loss:0.3487893951591281\n",
      "train loss:0.002604579082049758, test loss:0.34877455949601854\n",
      "train loss:0.0026029708745204647, test loss:0.34876855773244286\n",
      "train loss:0.0026013668113680625, test loss:0.34878429063885974\n",
      "train loss:0.0025997607995727925, test loss:0.3487546361259484\n",
      "train loss:0.0025965548434300914, test loss:0.34875969849015165\n",
      "train loss:0.0025949542938697788, test loss:0.3487368382138668\n",
      "train loss:0.0025933542142144573, test loss:0.3487483701902599\n",
      "train loss:0.002591755077994377, test loss:0.348729480735817\n",
      "train loss:0.002590155102637423, test loss:0.3487290305304867\n",
      "train loss:0.0025885590177398914, test loss:0.348726316217542\n",
      "train loss:0.0025869633297846304, test loss:0.3487097116452767\n",
      "train loss:0.002585368960904689, test loss:0.34871630437479106\n",
      "train loss:0.002583773749172884, test loss:0.3486952685615083\n",
      "train loss:0.0025805896493089463, test loss:0.3486867486212576\n",
      "train loss:0.0025789990116340403, test loss:0.3486848528792755\n",
      "train loss:0.0025774089926825007, test loss:0.3486755956820939\n",
      "train loss:0.002575820794574615, test loss:0.34867437443924654\n",
      "train loss:0.0025742341713978364, test loss:0.3486644118168081\n",
      "train loss:0.002572646867828174, test loss:0.34866745479614747\n",
      "train loss:0.002571060792023099, test loss:0.34865302550838073\n",
      "train loss:0.0025694778858620635, test loss:0.3486594766519965\n",
      "train loss:0.002567894658593632, test loss:0.34864630165717897\n",
      "train loss:0.002564730261071365, test loss:0.34864003472041954\n",
      "train loss:0.002563149850928514, test loss:0.3486326158131897\n",
      "train loss:0.0025615697095357206, test loss:0.34863684777708065\n",
      "train loss:0.0025599916430197787, test loss:0.3486175129931972\n",
      "train loss:0.002558413152615524, test loss:0.3486327560142357\n",
      "train loss:0.0025568394086612953, test loss:0.3486187352246137\n",
      "train loss:0.0025552614802424553, test loss:0.3486173859926194\n",
      "train loss:0.0025536885124491596, test loss:0.34862355415000057\n",
      "train loss:0.002552117819718038, test loss:0.3486059234908618\n",
      "train loss:0.0025489735643597264, test loss:0.34859762781963194\n",
      "train loss:0.002547403588341669, test loss:0.34860577476160837\n",
      "train loss:0.002545834357403987, test loss:0.34860407641653407\n",
      "train loss:0.002544268168545037, test loss:0.34858975379505525\n",
      "train loss:0.002542700078593893, test loss:0.348604397227299\n",
      "train loss:0.0025411375529892253, test loss:0.34858723135972475\n",
      "train loss:0.002539571379386733, test loss:0.34859154782119217\n",
      "train loss:0.0025380083905114418, test loss:0.34859781077214874\n",
      "train loss:0.002536446071502546, test loss:0.34858054811317185\n",
      "train loss:0.0025333261457801816, test loss:0.3485851005418727\n",
      "train loss:0.002531765686127233, test loss:0.3485855713079054\n",
      "train loss:0.0025302080299540726, test loss:0.3485923921899236\n",
      "train loss:0.002528649354515398, test loss:0.34858076815170624\n",
      "train loss:0.0025270946141004464, test loss:0.34859471588059915\n",
      "train loss:0.0025255385719860552, test loss:0.34858261438239907\n",
      "train loss:0.0025239849021001567, test loss:0.3485800605995301\n",
      "train loss:0.002522432957795384, test loss:0.3485935625672494\n",
      "train loss:0.0025208803733136127, test loss:0.34857455190817827\n",
      "train loss:0.0025177798474441604, test loss:0.34858772027614854\n",
      "train loss:0.0025162321024903185, test loss:0.34857702389923967\n",
      "train loss:0.0025146872047010316, test loss:0.3486005092025662\n",
      "train loss:0.002513138249509854, test loss:0.34858037126428043\n",
      "train loss:0.0025115909923365703, test loss:0.348589704495593\n",
      "train loss:0.0025100476780048737, test loss:0.34859433248868754\n",
      "train loss:0.00250850637700281, test loss:0.3485787349252834\n",
      "train loss:0.002506962772914002, test loss:0.34859250804276853\n",
      "train loss:0.002505420206365649, test loss:0.3485827628042188\n",
      "train loss:0.002502340152293765, test loss:0.348591823873687\n",
      "train loss:0.0025008022584052728, test loss:0.348571674917009\n",
      "train loss:0.0024992658451711324, test loss:0.34858976416496473\n",
      "train loss:0.0024977277212597446, test loss:0.3485811510297072\n",
      "train loss:0.0024961930656933608, test loss:0.3485773091241907\n",
      "train loss:0.0024946598858140265, test loss:0.34859400929582224\n",
      "train loss:0.0024931259193693776, test loss:0.3485774572944776\n",
      "train loss:0.002491593449128969, test loss:0.34858055998386406\n",
      "train loss:0.0024900621703129416, test loss:0.34858184380658364\n",
      "train loss:0.002487002570176925, test loss:0.3485744822344829\n",
      "train loss:0.0024854759504338267, test loss:0.34856361127933105\n",
      "train loss:0.002483947664337076, test loss:0.34857068803968116\n",
      "train loss:0.0024824227055755595, test loss:0.3485531846220846\n",
      "train loss:0.0024808970448650628, test loss:0.3485710529564687\n",
      "train loss:0.0024793722821443605, test loss:0.34854785834215074\n",
      "train loss:0.0024778492778823034, test loss:0.3485584644339376\n",
      "train loss:0.002476327152433101, test loss:0.34855124359067946\n",
      "train loss:0.0024748063670998028, test loss:0.3485387599399149\n",
      "train loss:0.002471767181613067, test loss:0.3485357862561076\n",
      "train loss:0.002470247587830769, test loss:0.34853116469982487\n",
      "train loss:0.002468729863463611, test loss:0.34852920979174706\n",
      "train loss:0.002467215141287344, test loss:0.34851767702342495\n",
      "train loss:0.0024657007161632987, test loss:0.3485217910499053\n",
      "train loss:0.0024641865084625548, test loss:0.34850451896431245\n",
      "train loss:0.002462673481644543, test loss:0.3485147163022836\n",
      "train loss:0.002461160932282076, test loss:0.3484959958702693\n",
      "train loss:0.0024596520827266594, test loss:0.3485061697031523\n",
      "train loss:0.0024566317750362348, test loss:0.3484935739978896\n",
      "train loss:0.0024551237423049497, test loss:0.34848241996841633\n",
      "train loss:0.0024536178484297275, test loss:0.34847446146541705\n",
      "train loss:0.0024521112351264694, test loss:0.3484795345963544\n",
      "train loss:0.0024506066679563917, test loss:0.34846010679312894\n",
      "train loss:0.0024491050934211792, test loss:0.3484701976986\n",
      "train loss:0.0024476005248818366, test loss:0.34845029512385\n",
      "train loss:0.0024460999346872995, test loss:0.34845879880035785\n",
      "train loss:0.0024445994654799263, test loss:0.3484443100043519\n",
      "train loss:0.002441599944836264, test loss:0.34843964759681434\n",
      "train loss:0.002440103197483181, test loss:0.3484425511869891\n",
      "train loss:0.0024386075295840044, test loss:0.3484402829803071\n",
      "train loss:0.0024371106142073887, test loss:0.348432506091801\n",
      "train loss:0.00243561774499942, test loss:0.34843946404735243\n",
      "train loss:0.00243412357420149, test loss:0.3484212400256851\n",
      "train loss:0.0024326314586997537, test loss:0.34843865384666506\n",
      "train loss:0.002431141110469059, test loss:0.348416473832403\n",
      "train loss:0.002429649938777839, test loss:0.34842251847619804\n",
      "train loss:0.0024266712240189163, test loss:0.34840451234251774\n",
      "train loss:0.0024251850337361847, test loss:0.3484178197428363\n",
      "train loss:0.0024236969858016095, test loss:0.34839751653870293\n",
      "train loss:0.0024222132498482435, test loss:0.34840150823245825\n",
      "train loss:0.002420729134691132, test loss:0.34839726770119334\n",
      "train loss:0.002419246349234036, test loss:0.34838980430523714\n",
      "train loss:0.0024177627294612, test loss:0.348398400192996\n",
      "train loss:0.002416281494622388, test loss:0.3483808183045429\n",
      "train loss:0.0024148011378570962, test loss:0.34839429945139605\n",
      "train loss:0.0024118421795753164, test loss:0.34838041058394814\n",
      "train loss:0.002410364775412695, test loss:0.3483772886812241\n",
      "train loss:0.0024088891065210225, test loss:0.3483729737537284\n",
      "train loss:0.0024074133707579154, test loss:0.3483708641863447\n",
      "train loss:0.002405937428961021, test loss:0.34836577340657165\n",
      "train loss:0.0024044622288356557, test loss:0.34836338827361674\n",
      "train loss:0.002402990165091149, test loss:0.34835424653440267\n",
      "train loss:0.002401518373332504, test loss:0.34836498055823834\n",
      "train loss:0.0024000476280480773, test loss:0.34833781042043904\n",
      "train loss:0.00239710836733185, test loss:0.3483492392640426\n",
      "train loss:0.0023956407086126474, test loss:0.3483260245382479\n",
      "train loss:0.002394173526557622, test loss:0.348340422985309\n",
      "train loss:0.0023927071762648243, test loss:0.34833145457611914\n",
      "train loss:0.0023912438242804555, test loss:0.348318794710158\n",
      "train loss:0.002389778695151633, test loss:0.3483311433132722\n",
      "train loss:0.002388314850174392, test loss:0.3483113697396946\n",
      "train loss:0.0023868511192842085, test loss:0.3483057525396915\n",
      "train loss:0.002385391139979606, test loss:0.3483142868812588\n",
      "train loss:0.0023824710325207835, test loss:0.3482992443146703\n",
      "train loss:0.002381011802055772, test loss:0.34829642017576756\n",
      "train loss:0.002379555502568328, test loss:0.3482773453392337\n",
      "train loss:0.002378096662585711, test loss:0.3482898135362703\n",
      "train loss:0.002376643041460762, test loss:0.3482801017050823\n",
      "train loss:0.0023751863609093806, test loss:0.3482707993521445\n",
      "train loss:0.002373734226159731, test loss:0.34827307894487963\n",
      "train loss:0.0023722807946123707, test loss:0.3482508256140936\n",
      "train loss:0.002370830410061103, test loss:0.34825658549318905\n",
      "train loss:0.0023679288871790902, test loss:0.3482339485286439\n",
      "train loss:0.002366479682148885, test loss:0.34823317952107036\n",
      "train loss:0.0023650318251593324, test loss:0.34821842053514623\n",
      "train loss:0.002363586603067864, test loss:0.34821568737867187\n",
      "train loss:0.002362139989264167, test loss:0.348200827449423\n",
      "train loss:0.0023606957191408826, test loss:0.34820499411481104\n",
      "train loss:0.0023592500474406163, test loss:0.348184858511489\n",
      "train loss:0.0023578083985085064, test loss:0.3481878266474805\n",
      "train loss:0.0023563654051122827, test loss:0.34817992230363687\n",
      "train loss:0.002353485020697978, test loss:0.3481668108066465\n",
      "train loss:0.002352045233825599, test loss:0.3481441922131185\n",
      "train loss:0.002350607490262301, test loss:0.3481388675747381\n",
      "train loss:0.0023491697993773173, test loss:0.3481321474124278\n",
      "train loss:0.0023477338883946186, test loss:0.34811884704429313\n",
      "train loss:0.0023462992191957324, test loss:0.34812276743830295\n",
      "train loss:0.0023448637274378903, test loss:0.3480990491110947\n",
      "train loss:0.0023434297604967343, test loss:0.3481036350936196\n",
      "train loss:0.00234199863171103, test loss:0.3480917560341885\n",
      "train loss:0.002339135279328657, test loss:0.348080184541769\n",
      "train loss:0.0023377075256178718, test loss:0.3480585349128198\n",
      "train loss:0.002336277802041584, test loss:0.34806941724682716\n",
      "train loss:0.002334850638184538, test loss:0.3480549511314009\n",
      "train loss:0.0023334239349432034, test loss:0.34804276555348235\n",
      "train loss:0.0023319991967541667, test loss:0.3480477698910911\n",
      "train loss:0.0023305724685782586, test loss:0.34802598558351144\n",
      "train loss:0.002329150048690284, test loss:0.3480294475972408\n",
      "train loss:0.002327725943180801, test loss:0.34801997159614645\n",
      "train loss:0.0023248829243553404, test loss:0.3480149630244207\n",
      "train loss:0.002323462707552527, test loss:0.3479879047342945\n",
      "train loss:0.002322043357147761, test loss:0.347997891559709\n",
      "train loss:0.002320624494868419, test loss:0.34799005483834006\n",
      "train loss:0.002319206937909297, test loss:0.347975499704836\n",
      "train loss:0.002317790891102018, test loss:0.347983451698977\n",
      "train loss:0.0023163757857484926, test loss:0.34796781108266717\n",
      "train loss:0.002314961075389975, test loss:0.3479665475459456\n",
      "train loss:0.0023135480608137073, test loss:0.34796918615952327\n",
      "train loss:0.002310723472906644, test loss:0.3479620581251655\n",
      "train loss:0.0023093130232179093, test loss:0.34795159737429915\n",
      "train loss:0.002307904077213711, test loss:0.34794749779428547\n",
      "train loss:0.0023064934360794817, test loss:0.34795618571015124\n",
      "train loss:0.002305086977228701, test loss:0.3479369762507668\n",
      "train loss:0.0023036794442668164, test loss:0.3479493914494963\n",
      "train loss:0.0023022731007423993, test loss:0.3479396485270939\n",
      "train loss:0.00230086695913604, test loss:0.34793525108035067\n",
      "train loss:0.0022994631620516194, test loss:0.34793966297868345\n",
      "train loss:0.0022966581424611384, test loss:0.34793547385428736\n",
      "train loss:0.0022952552017779793, test loss:0.3479181716854949\n",
      "train loss:0.0022938563092575917, test loss:0.34792985280778616\n",
      "train loss:0.0022924566216402883, test loss:0.34791455120355386\n",
      "train loss:0.002291056278982406, test loss:0.3479188346344935\n",
      "train loss:0.0022896600925045325, test loss:0.34791609642132576\n",
      "train loss:0.002288260865673203, test loss:0.3479074507926406\n",
      "train loss:0.0022868660895523614, test loss:0.3479168214103955\n",
      "train loss:0.002285470987859169, test loss:0.3478950319248626\n",
      "train loss:0.002282681938749261, test loss:0.347896011256928\n",
      "train loss:0.002281289809534759, test loss:0.3478823306812862\n",
      "train loss:0.0022798985989717535, test loss:0.34789784047294425\n",
      "train loss:0.0022785083790252277, test loss:0.3478749666812691\n",
      "train loss:0.002277117275826656, test loss:0.3478781612856925\n",
      "train loss:0.0022757312369852253, test loss:0.34788297475748126\n",
      "train loss:0.0022743416236699816, test loss:0.34786223004783207\n",
      "train loss:0.002272955344446326, test loss:0.34787823101909043\n",
      "train loss:0.002271569031442613, test loss:0.3478719496385127\n",
      "train loss:0.0022687991357467096, test loss:0.3478773885304191\n",
      "train loss:0.0022674162493638867, test loss:0.3478606539634401\n",
      "train loss:0.002266033145469406, test loss:0.34786861211453785\n",
      "train loss:0.002264651658225457, test loss:0.3478639456005603\n",
      "train loss:0.0022632716269913233, test loss:0.34785592287642164\n",
      "train loss:0.002261890985941592, test loss:0.3478619512374833\n",
      "train loss:0.002260512677734814, test loss:0.3478439950660884\n",
      "train loss:0.0022591350256714023, test loss:0.3478533561501199\n",
      "train loss:0.002257757137195173, test loss:0.3478469222870415\n",
      "train loss:0.0022550048683370534, test loss:0.34785097413312577\n",
      "train loss:0.0022536306413150067, test loss:0.3478317753265559\n",
      "train loss:0.002252257095211067, test loss:0.3478384576131313\n",
      "train loss:0.002250883572634826, test loss:0.34783836536674306\n",
      "train loss:0.002249511114874319, test loss:0.34782861285836025\n",
      "train loss:0.0022481418014500096, test loss:0.34783746763473006\n",
      "train loss:0.0022467693861261246, test loss:0.3478309138921707\n",
      "train loss:0.0022454014792892877, test loss:0.347822592251323\n",
      "train loss:0.002244033422724793, test loss:0.3478315891816317\n",
      "train loss:0.002241296075642849, test loss:0.3478181764302529\n",
      "train loss:0.0022399310256429376, test loss:0.3478103603060563\n",
      "train loss:0.0022385674186489847, test loss:0.34780103153226244\n",
      "train loss:0.0022372032229233097, test loss:0.3478050646410886\n",
      "train loss:0.0022358392896885616, test loss:0.3477904555014145\n",
      "train loss:0.0022344758423116303, test loss:0.34779538349872335\n",
      "train loss:0.002233114865748362, test loss:0.34778691551002083\n",
      "train loss:0.0022317537786415054, test loss:0.34777847364352726\n",
      "train loss:0.002230393402940493, test loss:0.3477816450971716\n",
      "train loss:0.0022276775800941506, test loss:0.3477675963691424\n",
      "train loss:0.0022263198252849893, test loss:0.34775704232702714\n",
      "train loss:0.002224963445807573, test loss:0.3477541507100781\n",
      "train loss:0.002223607977221654, test loss:0.34774953495010513\n",
      "train loss:0.0022222546968173116, test loss:0.34774589948912327\n",
      "train loss:0.0022209010374282744, test loss:0.3477414080838679\n",
      "train loss:0.0022195480594513553, test loss:0.34774014889267435\n",
      "train loss:0.0022181971681229156, test loss:0.3477280647964957\n",
      "train loss:0.0022168465705187184, test loss:0.3477349755433932\n",
      "train loss:0.002214147766661963, test loss:0.34772817494026087\n",
      "train loss:0.002212799985098666, test loss:0.3477129135990827\n",
      "train loss:0.002211451864782425, test loss:0.34771471264250314\n",
      "train loss:0.0022101051150808957, test loss:0.3477222162935714\n",
      "train loss:0.0022087610937689627, test loss:0.34770634455761684\n",
      "train loss:0.00220741659292989, test loss:0.34772423149842413\n",
      "train loss:0.0022060730642012374, test loss:0.34771674669126856\n",
      "train loss:0.00220472974351531, test loss:0.34771102664167525\n",
      "train loss:0.0022033889967685177, test loss:0.3477245917963156\n",
      "train loss:0.0022007048972180953, test loss:0.3477118662436003\n",
      "train loss:0.002199366231081913, test loss:0.34771540688819524\n",
      "train loss:0.0021980270471434046, test loss:0.3477020842220112\n",
      "train loss:0.0021966894593370328, test loss:0.3477203027078838\n",
      "train loss:0.0021953527386478246, test loss:0.3477079142250351\n",
      "train loss:0.0021940163945153132, test loss:0.3477067046254888\n",
      "train loss:0.0021926826247213077, test loss:0.3477235480437256\n",
      "train loss:0.002191347558711001, test loss:0.34770122389722313\n",
      "train loss:0.002190013553454929, test loss:0.3477075043057726\n",
      "train loss:0.0021873499273983928, test loss:0.34769337533753775\n",
      "train loss:0.0021860178653978443, test loss:0.3477011432113703\n",
      "train loss:0.0021846868248668713, test loss:0.3476907297548306\n",
      "train loss:0.0021833593757297335, test loss:0.3476924339998732\n",
      "train loss:0.0021820294343720954, test loss:0.34769027113851064\n",
      "train loss:0.002180702434833093, test loss:0.3476785913780956\n",
      "train loss:0.002179376292050086, test loss:0.3476926327117094\n",
      "train loss:0.002178050227497179, test loss:0.34767552005828634\n",
      "train loss:0.0021767255301462732, test loss:0.34767746038567726\n",
      "train loss:0.002174078310877903, test loss:0.34766602503410987\n",
      "train loss:0.0021727568123613146, test loss:0.34767369054624964\n",
      "train loss:0.002171434161256013, test loss:0.3476694224665826\n",
      "train loss:0.002170114758188404, test loss:0.3476585105096887\n",
      "train loss:0.002168796350710105, test loss:0.34767386595814453\n",
      "train loss:0.0021674760971314563, test loss:0.3476541347992434\n",
      "train loss:0.002166158311455941, test loss:0.3476627287886174\n",
      "train loss:0.002164841667335908, test loss:0.34766280634379454\n",
      "train loss:0.0021635262258283368, test loss:0.3476498304269173\n",
      "train loss:0.0021608972997059464, test loss:0.34765495522641277\n",
      "train loss:0.0021595837688861714, test loss:0.3476541612134363\n",
      "train loss:0.0021582713057228197, test loss:0.34767083027333784\n",
      "train loss:0.0021569599194247867, test loss:0.34765546874058184\n",
      "train loss:0.002155649217618212, test loss:0.3476591330115161\n",
      "train loss:0.0021543402399317835, test loss:0.3476681319518683\n",
      "train loss:0.0021530310163326285, test loss:0.3476518426966978\n",
      "train loss:0.002151722633239819, test loss:0.3476623966206962\n",
      "train loss:0.0021504153152857304, test loss:0.34765956619725213\n",
      "train loss:0.0021478041936263963, test loss:0.34766860241285824\n",
      "train loss:0.0021464985709203307, test loss:0.3476515459005297\n",
      "train loss:0.002145194837567281, test loss:0.3476533893945338\n",
      "train loss:0.0021438922650861584, test loss:0.3476633231032612\n",
      "train loss:0.0021425902610890976, test loss:0.3476471724255728\n",
      "train loss:0.0021412886194102725, test loss:0.34766088968783515\n",
      "train loss:0.002139987987107427, test loss:0.3476525399915675\n",
      "train loss:0.002138688596805205, test loss:0.34764851747917946\n",
      "train loss:0.0021373907571274327, test loss:0.3476538683561794\n",
      "train loss:0.0021347976447465803, test loss:0.34765014229407465\n",
      "train loss:0.002133498578188459, test loss:0.34763938135006034\n",
      "train loss:0.002132204611233546, test loss:0.3476417662758769\n",
      "train loss:0.0021309099896593573, test loss:0.3476344420652662\n",
      "train loss:0.002129616744586229, test loss:0.3476339093208733\n",
      "train loss:0.0021283249636198905, test loss:0.34762603307971185\n",
      "train loss:0.002127033573975559, test loss:0.3476421041832771\n",
      "train loss:0.002125743433612786, test loss:0.34761779708129253\n",
      "train loss:0.002124454535795402, test loss:0.34763897557927653\n",
      "train loss:0.0021218766482493335, test loss:0.34761856324765716\n",
      "train loss:0.002120588738972704, test loss:0.34763105820306167\n",
      "train loss:0.0021193015656700656, test loss:0.3476100407758466\n",
      "train loss:0.0021180159364613937, test loss:0.3476134530213488\n",
      "train loss:0.0021167308293508448, test loss:0.3476180452825547\n",
      "train loss:0.0021154491651026596, test loss:0.3476018938105052\n",
      "train loss:0.0021141631442520422, test loss:0.34761512680085965\n",
      "train loss:0.0021128819942013437, test loss:0.3476043904958857\n",
      "train loss:0.0021116009581469168, test loss:0.34760368711870526\n",
      "train loss:0.0021090401561498894, test loss:0.3475926168200225\n",
      "train loss:0.0021077621798875896, test loss:0.34760629119279546\n",
      "train loss:0.0021064833572339743, test loss:0.34758767093497134\n",
      "train loss:0.0021052059557916582, test loss:0.3475937872909326\n",
      "train loss:0.0021039277383160045, test loss:0.34759267470380606\n",
      "train loss:0.0021026533607791083, test loss:0.34758307159648905\n",
      "train loss:0.002101377644883168, test loss:0.34759159382675964\n",
      "train loss:0.0021001036583280096, test loss:0.3475799365357328\n",
      "train loss:0.00209883151604155, test loss:0.34758153530161845\n",
      "train loss:0.0020962870738073184, test loss:0.34756757600086885\n",
      "train loss:0.002095015181729319, test loss:0.34758191525393356\n",
      "train loss:0.0020937458343136227, test loss:0.34756776368907855\n",
      "train loss:0.0020924759365384287, test loss:0.3475703915238755\n",
      "train loss:0.002091208458073752, test loss:0.34756922218005987\n",
      "train loss:0.002089940585069156, test loss:0.34756239970192243\n",
      "train loss:0.002088674810170275, test loss:0.34756847187073336\n",
      "train loss:0.002087408300475446, test loss:0.3475593717125258\n",
      "train loss:0.0020861425838100914, test loss:0.3475634513753869\n",
      "train loss:0.002083615371563143, test loss:0.34756551163813343\n",
      "train loss:0.0020823524377148613, test loss:0.34755525168352014\n",
      "train loss:0.0020810929503883575, test loss:0.34756495805174015\n",
      "train loss:0.0020798294856964965, test loss:0.34755312353213597\n",
      "train loss:0.0020785712603533083, test loss:0.34755516970821293\n",
      "train loss:0.0020773108125702925, test loss:0.3475572401675197\n",
      "train loss:0.002076051310217035, test loss:0.3475490405449787\n",
      "train loss:0.002074793593093268, test loss:0.34755472854098346\n",
      "train loss:0.002073535827400882, test loss:0.3475465897108433\n",
      "train loss:0.0020710260572953426, test loss:0.34754423129253276\n",
      "train loss:0.0020697725392254214, test loss:0.34755249242098807\n",
      "train loss:0.0020685180482332413, test loss:0.34754226553866696\n",
      "train loss:0.002067265654556488, test loss:0.3475535779291376\n",
      "train loss:0.002066013913310781, test loss:0.34753542126868303\n",
      "train loss:0.002064763713845743, test loss:0.3475541070947319\n",
      "train loss:0.0020635120779129846, test loss:0.3475401376643119\n",
      "train loss:0.002062261867406037, test loss:0.34754669514652303\n",
      "train loss:0.002061014196910317, test loss:0.34754694719519497\n",
      "train loss:0.0020585185042697072, test loss:0.3475505973081705\n",
      "train loss:0.002057272188926843, test loss:0.34754145242849643\n",
      "train loss:0.0020560276032793663, test loss:0.34755048238271946\n",
      "train loss:0.0020547824834962437, test loss:0.3475453374201964\n",
      "train loss:0.0020535391511415195, test loss:0.3475461924246142\n",
      "train loss:0.002052295730705071, test loss:0.34755333936081717\n",
      "train loss:0.0020510531739990905, test loss:0.34754333991229336\n",
      "train loss:0.0020498115531578866, test loss:0.3475623512518483\n",
      "train loss:0.002048570365672125, test loss:0.34754754488611544\n",
      "train loss:0.0020460909743915, test loss:0.34756371884144704\n",
      "train loss:0.002044852510545819, test loss:0.34754927865784935\n",
      "train loss:0.0020436164052241653, test loss:0.34757582929351305\n",
      "train loss:0.0020423786804349463, test loss:0.34756147790159375\n",
      "train loss:0.0020411424833514263, test loss:0.3475567010957205\n",
      "train loss:0.002039910341128976, test loss:0.34758191000046945\n",
      "train loss:0.0020386728548991767, test loss:0.34756078934509793\n",
      "train loss:0.0020374397120212703, test loss:0.34756113569975866\n",
      "train loss:0.002036208289920528, test loss:0.34758048749611353\n",
      "train loss:0.002033746678686576, test loss:0.34756238209230755\n",
      "train loss:0.0020325173104092068, test loss:0.34759743113909974\n",
      "train loss:0.002031287396368799, test loss:0.34757007893995845\n",
      "train loss:0.0020300572657222867, test loss:0.34757976374480387\n",
      "train loss:0.0020288301928651493, test loss:0.34759562793552123\n",
      "train loss:0.002027603128514436, test loss:0.34757933315323725\n",
      "train loss:0.0020263773669471925, test loss:0.34758568289455893\n",
      "train loss:0.0020251537916205486, test loss:0.34759555485527394\n",
      "train loss:0.002023929307171434, test loss:0.34758531910029195\n",
      "train loss:0.002021483580877878, test loss:0.34759555244880896\n",
      "train loss:0.0020202624647602694, test loss:0.34760373877494166\n",
      "train loss:0.002019042038564411, test loss:0.3476021874684887\n",
      "train loss:0.0020178237894626877, test loss:0.347609544267323\n",
      "train loss:0.0020166050492634573, test loss:0.34760027685824213\n",
      "train loss:0.0020153887301391704, test loss:0.34762278656965717\n",
      "train loss:0.0020141728005398643, test loss:0.34759711497023354\n",
      "train loss:0.0020129588661254405, test loss:0.34762330435095884\n",
      "train loss:0.002011745200391676, test loss:0.34761921116601335\n",
      "train loss:0.002009322393582213, test loss:0.34762180164558776\n",
      "train loss:0.0020081129677186364, test loss:0.34762122275868307\n",
      "train loss:0.0020069054230411336, test loss:0.34761297464635815\n",
      "train loss:0.0020057012666682967, test loss:0.3476350466520702\n",
      "train loss:0.0020044996433380155, test loss:0.3476125696451548\n",
      "train loss:0.00200329861572411, test loss:0.34763494685445845\n",
      "train loss:0.0020021019092737163, test loss:0.3476217645305746\n",
      "train loss:0.0020009083552713506, test loss:0.347627828721956\n",
      "train loss:0.001999720171817612, test loss:0.34762517535824883\n",
      "train loss:0.0019973579351399945, test loss:0.3476262870490794\n",
      "train loss:0.001996187567944559, test loss:0.3476321974824932\n",
      "train loss:0.00199502742054446, test loss:0.34761088956305575\n",
      "train loss:0.0019938781923671636, test loss:0.34764985681109944\n",
      "train loss:0.001992744382107211, test loss:0.3476167511860661\n",
      "train loss:0.0019916328896780475, test loss:0.34764455709292735\n",
      "train loss:0.001990546923659094, test loss:0.3476298171594942\n",
      "train loss:0.0019894979968936456, test loss:0.3476451865292969\n",
      "train loss:0.001988501938010697, test loss:0.3476233035378576\n",
      "train loss:0.0019867879043686354, test loss:0.3476112427961392\n",
      "train loss:0.001986186864214626, test loss:0.34768460987381694\n",
      "train loss:0.0019859369794629433, test loss:0.34761564365527753\n",
      "train loss:0.001986368080579888, test loss:0.3477076960054337\n",
      "train loss:0.0019884090170485215, test loss:0.34764365041253037\n",
      "train loss:0.001995264425267914, test loss:0.34780050077623165\n",
      "train loss:0.0020281283119548184, test loss:0.3478048335345113\n",
      "train loss:0.003549700049340929, test loss:0.35137950063467416\n",
      "train loss:4.605848212372976, test loss:4.974568082774525\n",
      "train loss:27.317576430574633, test loss:29.453148038327264\n",
      "train loss:65.98279552776435, test loss:66.64324630416864\n",
      "train loss:111.22844323886926, test loss:111.54167797916068\n",
      "train loss:153.37785884817677, test loss:152.5641529735178\n",
      "train loss:184.54826846987658, test loss:184.13934455523565\n",
      "train loss:207.06411396062765, test loss:208.21269169861444\n",
      "train loss:224.42479539115976, test loss:225.79038359906534\n",
      "train loss:251.96326294567106, test loss:254.0678359674214\n",
      "train loss:258.1005029581814, test loss:260.74279640208476\n",
      "train loss:279.46415245652094, test loss:283.6211727320923\n",
      "train loss:275.02475567640863, test loss:279.28112409826934\n",
      "train loss:273.19575053607963, test loss:278.35100028048464\n",
      "train loss:250.767285503492, test loss:254.93099855893502\n",
      "train loss:236.9952846117842, test loss:240.59839069197565\n",
      "train loss:224.39489302661303, test loss:226.60212834784897\n",
      "train loss:208.2335417661372, test loss:210.2584879968226\n",
      "train loss:186.76391372745638, test loss:188.90844325352532\n",
      "train loss:169.595133075038, test loss:170.56756724271756\n",
      "train loss:143.91039775645126, test loss:144.54126920755965\n",
      "train loss:131.9530432407724, test loss:131.2886556745161\n",
      "train loss:119.5980135425156, test loss:120.37770337521381\n",
      "train loss:110.38017273803499, test loss:111.86894144716608\n",
      "train loss:101.51165156624626, test loss:103.0176208188392\n",
      "train loss:94.81368861515834, test loss:97.50951523102913\n",
      "train loss:85.01283003875531, test loss:87.24021818538618\n",
      "train loss:76.92787420758494, test loss:78.5668727456164\n",
      "train loss:69.97297651362332, test loss:71.27820335454841\n",
      "train loss:57.7564331185011, test loss:60.121459978219285\n",
      "train loss:54.27780208214414, test loss:56.77903576147067\n",
      "train loss:51.53475754853373, test loss:54.205985502622525\n",
      "train loss:48.57668433763442, test loss:51.53120673786997\n",
      "train loss:45.52172501660114, test loss:47.995006359201255\n",
      "train loss:42.601404262336274, test loss:44.42389772041646\n",
      "train loss:39.7172989689169, test loss:41.52416681001423\n",
      "train loss:35.536211378340894, test loss:37.64176222875758\n",
      "train loss:31.427923914821584, test loss:33.82707065121566\n",
      "train loss:27.06426923227278, test loss:28.40832184369216\n",
      "train loss:25.74463669759241, test loss:26.8914856484285\n",
      "train loss:24.000164937330347, test loss:25.16868218677826\n",
      "train loss:22.340755514427833, test loss:23.39401207268256\n",
      "train loss:20.59119133566718, test loss:21.516452201057614\n",
      "train loss:18.7491513181638, test loss:19.626541379435682\n",
      "train loss:17.01945430891638, test loss:18.042893059737644\n",
      "train loss:15.473566313169503, test loss:16.628411990451195\n",
      "train loss:14.094165184438632, test loss:15.42621263911188\n",
      "train loss:11.973268735907215, test loss:13.636462916982397\n",
      "train loss:11.170782531858707, test loss:12.950908892612652\n",
      "train loss:10.645954548263346, test loss:12.40584468041202\n",
      "train loss:10.177572820532793, test loss:11.91852862420158\n",
      "train loss:9.599567257306255, test loss:11.286434390794138\n",
      "train loss:8.973494843103563, test loss:10.589250841560968\n",
      "train loss:8.3753995021189, test loss:10.004393529353486\n",
      "train loss:7.839627582895956, test loss:9.544197348443358\n",
      "train loss:7.368309951627388, test loss:9.117878849509536\n",
      "train loss:6.561774826904312, test loss:8.327860808637002\n",
      "train loss:6.165191438322927, test loss:7.958790359016896\n",
      "train loss:5.720682738094996, test loss:7.531414993619689\n",
      "train loss:5.269895518330477, test loss:7.056256731324187\n",
      "train loss:4.871691898973538, test loss:6.601618401304876\n",
      "train loss:4.583556367045648, test loss:6.224519352226411\n",
      "train loss:4.3757351427486855, test loss:5.940505065413229\n",
      "train loss:4.1983582009922085, test loss:5.698392665461888\n",
      "train loss:4.019195966763864, test loss:5.46524914678599\n",
      "train loss:3.637494870505354, test loss:5.0673876135524\n",
      "train loss:3.516489686148856, test loss:4.937617409614505\n",
      "train loss:3.356952920893085, test loss:4.785798015637355\n",
      "train loss:3.1505722658240103, test loss:4.629555801164284\n",
      "train loss:2.9495372224868146, test loss:4.5045310158389364\n",
      "train loss:2.767774878499834, test loss:4.391113348894032\n",
      "train loss:2.6192506744020694, test loss:4.282514892746725\n",
      "train loss:2.517452972975294, test loss:4.192301087699998\n",
      "train loss:2.440157127692727, test loss:4.108112613749087\n",
      "train loss:2.24495062272848, test loss:3.8983224097980855\n",
      "train loss:2.1455988022155226, test loss:3.7777932875195765\n",
      "train loss:2.066012182478875, test loss:3.6571938970264903\n",
      "train loss:2.0030702220559577, test loss:3.544351390532754\n",
      "train loss:1.9443619684211821, test loss:3.4320798346638224\n",
      "train loss:1.8820159529229805, test loss:3.319816505492831\n",
      "train loss:1.8207330888794397, test loss:3.2162869414626747\n",
      "train loss:1.762802254206936, test loss:3.1246224456946052\n",
      "train loss:1.7120440869458489, test loss:3.0491583597452725\n",
      "train loss:1.6135756452710501, test loss:2.9294099433684986\n",
      "train loss:1.5645796699647598, test loss:2.8779560251576233\n",
      "train loss:1.5223924916555032, test loss:2.8366371432651856\n",
      "train loss:1.486609023010553, test loss:2.799712680160855\n",
      "train loss:1.454515933406748, test loss:2.7611693250620237\n",
      "train loss:1.422020663998375, test loss:2.7171791083330956\n",
      "train loss:1.3846598798315037, test loss:2.667180945075612\n",
      "train loss:1.3441697346111647, test loss:2.61783881160558\n",
      "train loss:1.3095500254702674, test loss:2.580323199296064\n",
      "train loss:1.2540251070413457, test loss:2.536139918103315\n",
      "train loss:1.222771954276039, test loss:2.514020477555695\n",
      "train loss:1.1922082500778641, test loss:2.4911899822326977\n",
      "train loss:1.164447631301674, test loss:2.4694403331214185\n",
      "train loss:1.13858462483884, test loss:2.448111752897981\n",
      "train loss:1.1146002100136359, test loss:2.4260654964898762\n",
      "train loss:1.0913349819170617, test loss:2.40057105112067\n",
      "train loss:1.0672458805709955, test loss:2.3703000715296922\n",
      "train loss:1.0434313760700533, test loss:2.3381843628628687\n",
      "train loss:0.9991718639958036, test loss:2.2765915383227497\n",
      "train loss:0.9769430768349029, test loss:2.246511542693719\n",
      "train loss:0.9552944278769215, test loss:2.2179353126096455\n",
      "train loss:0.9346931249541084, test loss:2.1914759422904666\n",
      "train loss:0.914517174322938, test loss:2.1665391566906567\n",
      "train loss:0.8945904070739077, test loss:2.1425754298377844\n",
      "train loss:0.8754674156767106, test loss:2.1195612571946234\n",
      "train loss:0.8573034638869683, test loss:2.097386731205877\n",
      "train loss:0.8398319140056091, test loss:2.0757704310156164\n",
      "train loss:0.8077784033547726, test loss:2.034439212437864\n",
      "train loss:0.7933530721506246, test loss:2.01509938364095\n",
      "train loss:0.7799159446156808, test loss:1.9971020893865121\n",
      "train loss:0.7676074608214577, test loss:1.9809185857234985\n",
      "train loss:0.7565589115162751, test loss:1.9668510039306404\n",
      "train loss:0.7470544252614475, test loss:1.9551089757267255\n",
      "train loss:0.7393197904582249, test loss:1.9456578564261906\n",
      "train loss:0.732886583650681, test loss:1.9377629669409193\n",
      "train loss:0.726880576105119, test loss:1.9302100210425106\n",
      "train loss:0.7140765201799278, test loss:1.9127211124990047\n",
      "train loss:0.7067312319186808, test loss:1.902019086093287\n",
      "train loss:0.6986691938650604, test loss:1.8902034767969722\n",
      "train loss:0.6902156131024806, test loss:1.8779013444923465\n",
      "train loss:0.6817645678046442, test loss:1.8656378340866804\n",
      "train loss:0.6736101853394578, test loss:1.853677326176856\n",
      "train loss:0.665908179669579, test loss:1.8420570476197895\n",
      "train loss:0.6586633562048962, test loss:1.830775564400705\n",
      "train loss:0.6518203876045305, test loss:1.819836161133629\n",
      "train loss:0.6392246218239604, test loss:1.798976217151664\n",
      "train loss:0.6333218106145468, test loss:1.788958209739376\n",
      "train loss:0.6275847071748644, test loss:1.7791925275062666\n",
      "train loss:0.6220207290541758, test loss:1.7696684539568581\n",
      "train loss:0.6166296660194148, test loss:1.7603228978576015\n",
      "train loss:0.6113712158864202, test loss:1.7510962265446435\n",
      "train loss:0.6062038190344947, test loss:1.7419829403941516\n",
      "train loss:0.6011120923645044, test loss:1.7330097044183699\n",
      "train loss:0.5961138801669028, test loss:1.7242267040958466\n",
      "train loss:0.5865025620531868, test loss:1.7073169177505012\n",
      "train loss:0.5818674057226847, test loss:1.6991553175704375\n",
      "train loss:0.577342914716684, test loss:1.6911591940971245\n",
      "train loss:0.5729443917892743, test loss:1.683315894046043\n",
      "train loss:0.5686687064212254, test loss:1.6755903591583556\n",
      "train loss:0.5645048594087186, test loss:1.6679459469849252\n",
      "train loss:0.5604439315281692, test loss:1.6603590819493133\n",
      "train loss:0.5564752360552485, test loss:1.652803091785004\n",
      "train loss:0.5525877974855937, test loss:1.6452532732225085\n",
      "train loss:0.5450628521874437, test loss:1.6302054271997886\n",
      "train loss:0.5414123175881331, test loss:1.622757471504292\n",
      "train loss:0.5378272786157424, test loss:1.6154043416061414\n",
      "train loss:0.5343054481815138, test loss:1.6081794293248768\n",
      "train loss:0.530848361864384, test loss:1.6011110262564654\n",
      "train loss:0.5274540029476689, test loss:1.5942195106490757\n",
      "train loss:0.5241211674795192, test loss:1.5874822937267972\n",
      "train loss:0.5208497176714804, test loss:1.580888783359511\n",
      "train loss:0.5176390292255495, test loss:1.5744408888283026\n",
      "train loss:0.5113829215157492, test loss:1.562027581862621\n",
      "train loss:0.5083294155920205, test loss:1.556105506268439\n",
      "train loss:0.5053240582443843, test loss:1.5503886837109773\n",
      "train loss:0.5023686215638028, test loss:1.5448806279161427\n",
      "train loss:0.49946075366808435, test loss:1.5395694660163115\n",
      "train loss:0.4965978879337369, test loss:1.5344141483273266\n",
      "train loss:0.4937808402759645, test loss:1.5293736460555496\n",
      "train loss:0.4910078891488459, test loss:1.5244148641921775\n",
      "train loss:0.48827828000588325, test loss:1.5195130680380653\n",
      "train loss:0.482944850709486, test loss:1.5097574023300433\n",
      "train loss:0.48033725014745476, test loss:1.5048701698395877\n",
      "train loss:0.47776610248510626, test loss:1.499970214066166\n",
      "train loss:0.47523171840003525, test loss:1.4950549131955375\n",
      "train loss:0.47273213203264247, test loss:1.4901241330080246\n",
      "train loss:0.4702666364515097, test loss:1.485181514745163\n",
      "train loss:0.4678348192116916, test loss:1.480226888400499\n",
      "train loss:0.46543791979980575, test loss:1.4752672709106855\n",
      "train loss:0.4630733936714868, test loss:1.4703076311562229\n",
      "train loss:0.4584406115553253, test loss:1.460468894495957\n",
      "train loss:0.4561717153267898, test loss:1.4556076596720033\n",
      "train loss:0.45393466158206613, test loss:1.4508120603362993\n",
      "train loss:0.4517278004063407, test loss:1.4460769477421664\n",
      "train loss:0.4495493247038325, test loss:1.441417848062034\n",
      "train loss:0.44740024077517715, test loss:1.4368408014748064\n",
      "train loss:0.44527866864032273, test loss:1.4323373652007543\n",
      "train loss:0.44318349751446173, test loss:1.4279073484491205\n",
      "train loss:0.44111442930569883, test loss:1.4235395669868403\n",
      "train loss:0.437052535817506, test loss:1.4149781209550312\n",
      "train loss:0.4350584952477491, test loss:1.4107766456363724\n",
      "train loss:0.43308901886500384, test loss:1.4066236200053583\n",
      "train loss:0.43114269193311483, test loss:1.4025149807027524\n",
      "train loss:0.4292190052959842, test loss:1.3984480599391829\n",
      "train loss:0.42731706096885835, test loss:1.394435068009041\n",
      "train loss:0.42543687807937614, test loss:1.3904698143026062\n",
      "train loss:0.4235778004931536, test loss:1.3865540692805531\n",
      "train loss:0.42174001705630676, test loss:1.3826838435110211\n",
      "train loss:0.418126985753833, test loss:1.3750583112282575\n",
      "train loss:0.4163504193125658, test loss:1.371295399288823\n",
      "train loss:0.41459378918051204, test loss:1.3675614651653165\n",
      "train loss:0.41285628315092393, test loss:1.3638609800528174\n",
      "train loss:0.4111363535872508, test loss:1.3602003979659822\n",
      "train loss:0.40943456999789746, test loss:1.3565783364044615\n",
      "train loss:0.40775077862131837, test loss:1.3529918719817566\n",
      "train loss:0.4060848387892855, test loss:1.3494504032258712\n",
      "train loss:0.40443676435297665, test loss:1.3459420537072015\n",
      "train loss:0.40119265521875336, test loss:1.339012781914445\n",
      "train loss:0.39959498157149387, test loss:1.3355819193390375\n",
      "train loss:0.39801351733563706, test loss:1.3321804970627362\n",
      "train loss:0.39644799887857135, test loss:1.3288055808977584\n",
      "train loss:0.39489865819108905, test loss:1.3254574822920204\n",
      "train loss:0.3933646853268514, test loss:1.3221420258487155\n",
      "train loss:0.39184614243480237, test loss:1.318855376887188\n",
      "train loss:0.39034252040262174, test loss:1.3155966269050403\n",
      "train loss:0.3888533489408385, test loss:1.3123775101118687\n",
      "train loss:0.3859183290268436, test loss:1.3060233462552167\n",
      "train loss:0.3844719752754278, test loss:1.3028824851605052\n",
      "train loss:0.38303894704644825, test loss:1.2997707018171147\n",
      "train loss:0.38161918857354316, test loss:1.2966776201885637\n",
      "train loss:0.38021221275984124, test loss:1.293613286170929\n",
      "train loss:0.37881914100084424, test loss:1.290576161678084\n",
      "train loss:0.3774387783295788, test loss:1.2875685886218613\n",
      "train loss:0.37607062357494275, test loss:1.2845916733988474\n",
      "train loss:0.3747147233744286, test loss:1.2816438798088594\n",
      "train loss:0.37203886661814284, test loss:1.2758345439801684\n",
      "train loss:0.3707187243641628, test loss:1.272980431705388\n",
      "train loss:0.36941005657483966, test loss:1.270160858191099\n",
      "train loss:0.3681133679657829, test loss:1.2673697218640887\n",
      "train loss:0.3668274642764748, test loss:1.2646038771206292\n",
      "train loss:0.36555210509159136, test loss:1.2618645354886366\n",
      "train loss:0.36428738621269746, test loss:1.2591479451599876\n",
      "train loss:0.3630335292175489, test loss:1.2564572602408883\n",
      "train loss:0.36179024033007, test loss:1.2537874089713466\n",
      "train loss:0.35933564488452885, test loss:1.2485101837779482\n",
      "train loss:0.3581236231333228, test loss:1.2458945958014502\n",
      "train loss:0.3569215846670843, test loss:1.2432944148594047\n",
      "train loss:0.35572933703418824, test loss:1.2407051024352538\n",
      "train loss:0.3545468308813821, test loss:1.2381240052144076\n",
      "train loss:0.35337424314410293, test loss:1.2355567064518513\n",
      "train loss:0.3522106348868848, test loss:1.2330027908573784\n",
      "train loss:0.3510570927831384, test loss:1.2304625189456933\n",
      "train loss:0.34991256058509673, test loss:1.2279341719910504\n",
      "train loss:0.34765073861562606, test loss:1.2229265896915973\n",
      "train loss:0.3465332811868668, test loss:1.2204532622989073\n",
      "train loss:0.3454245004843938, test loss:1.2180053270655216\n",
      "train loss:0.3443246976370156, test loss:1.215584290591528\n",
      "train loss:0.34323351417434184, test loss:1.2131846907147432\n",
      "train loss:0.342151227438919, test loss:1.2108052752736036\n",
      "train loss:0.3410765020567576, test loss:1.2084387549237605\n",
      "train loss:0.34000950412455266, test loss:1.2060906590413887\n",
      "train loss:0.338950478449101, test loss:1.2037572816751112\n",
      "train loss:0.3368554650886949, test loss:1.1991363344830226\n",
      "train loss:0.335819412063088, test loss:1.1968519871617282\n",
      "train loss:0.3347903885371452, test loss:1.194579070313455\n",
      "train loss:0.33376846482999545, test loss:1.1923271715029213\n",
      "train loss:0.33275400759275114, test loss:1.1900894463057576\n",
      "train loss:0.3317465899464391, test loss:1.1878648603745123\n",
      "train loss:0.33074615246108496, test loss:1.1856533600077126\n",
      "train loss:0.32975276669110676, test loss:1.1834584239460637\n",
      "train loss:0.3287664142027625, test loss:1.1812785904593524\n",
      "train loss:0.32681469903537286, test loss:1.1769513303673695\n",
      "train loss:0.32584927295844585, test loss:1.1747987391809789\n",
      "train loss:0.32489067524481524, test loss:1.1726529523970912\n",
      "train loss:0.3239392084022526, test loss:1.1705108793237475\n",
      "train loss:0.3229944533476335, test loss:1.16838134366762\n",
      "train loss:0.32205643988185495, test loss:1.1662623930829754\n",
      "train loss:0.3211252280897938, test loss:1.1641496109246166\n",
      "train loss:0.3202003939934409, test loss:1.162043189165596\n",
      "train loss:0.3192821068979127, test loss:1.1599420065854535\n",
      "train loss:0.317463770476191, test loss:1.155750824845048\n",
      "train loss:0.3165635831118027, test loss:1.1536652295524294\n",
      "train loss:0.31566940404455507, test loss:1.1515868529271687\n",
      "train loss:0.3147809028237663, test loss:1.149519101508992\n",
      "train loss:0.31389818541101194, test loss:1.1474629364079079\n",
      "train loss:0.31302125351215704, test loss:1.1454202549777484\n",
      "train loss:0.3121497087997965, test loss:1.1433931781880504\n",
      "train loss:0.3112836527291205, test loss:1.1413802640202746\n",
      "train loss:0.31042300612968887, test loss:1.1393852260919386\n",
      "train loss:0.3087176013192579, test loss:1.1354250080633592\n",
      "train loss:0.3078729855101765, test loss:1.1334650891002205\n",
      "train loss:0.30703333783920955, test loss:1.1315131390420905\n",
      "train loss:0.3061990529401704, test loss:1.1295701124469306\n",
      "train loss:0.30537005358891883, test loss:1.1276322088860338\n",
      "train loss:0.30454594432873533, test loss:1.1257052226956081\n",
      "train loss:0.3037268456924985, test loss:1.1237841302915776\n",
      "train loss:0.3029128755565939, test loss:1.1218805713435889\n",
      "train loss:0.30210372270810537, test loss:1.119988614593152\n",
      "train loss:0.30050056156356547, test loss:1.116255931127152\n",
      "train loss:0.2997061450140959, test loss:1.114407465807059\n",
      "train loss:0.2989168596005604, test loss:1.1125732553323657\n",
      "train loss:0.2981320122051079, test loss:1.110753177789527\n",
      "train loss:0.29735246238406743, test loss:1.1089389908228806\n",
      "train loss:0.2965773843730122, test loss:1.1071302957045257\n",
      "train loss:0.2958069616674373, test loss:1.1053242640820669\n",
      "train loss:0.29504134833228185, test loss:1.103521587635112\n",
      "train loss:0.29428021229519485, test loss:1.1017248901302898\n",
      "train loss:0.29277224409261177, test loss:1.0981374320243475\n",
      "train loss:0.2920251636614466, test loss:1.0963498267748548\n",
      "train loss:0.29128251854636067, test loss:1.0945735505476677\n",
      "train loss:0.2905446881315775, test loss:1.0928089482160623\n",
      "train loss:0.28981152093091894, test loss:1.0910567666826225\n",
      "train loss:0.2890822457713957, test loss:1.0893165260565518\n",
      "train loss:0.2883569543828325, test loss:1.0875898228827339\n",
      "train loss:0.2876356541417561, test loss:1.0858779632110556\n",
      "train loss:0.28691846557915757, test loss:1.0841737154077864\n",
      "train loss:0.28549725123519243, test loss:1.0807916135863977\n",
      "train loss:0.28479298995475877, test loss:1.0791054700340217\n",
      "train loss:0.28409272270868374, test loss:1.0774287553708097\n",
      "train loss:0.28339641809374483, test loss:1.075756820442419\n",
      "train loss:0.28270419029773924, test loss:1.0740885565618181\n",
      "train loss:0.2820157772719995, test loss:1.072422179555507\n",
      "train loss:0.2813310369106903, test loss:1.070757672044385\n",
      "train loss:0.28065025496430074, test loss:1.0690916076546177\n",
      "train loss:0.2799731435322548, test loss:1.0674226160749587\n",
      "train loss:0.27863049282869146, test loss:1.064104345159083\n",
      "train loss:0.27796451076959416, test loss:1.062453003773126\n",
      "train loss:0.27730231333236294, test loss:1.0608095179867545\n",
      "train loss:0.2766435332827295, test loss:1.0591724041060135\n",
      "train loss:0.27598859111163415, test loss:1.0575469190092177\n",
      "train loss:0.27533727521836926, test loss:1.0559279144148492\n",
      "train loss:0.2746894547297696, test loss:1.0543180155739804\n",
      "train loss:0.27404523742020853, test loss:1.0527185449331047\n",
      "train loss:0.2734047667086123, test loss:1.0511247849138297\n",
      "train loss:0.2721344061390121, test loss:1.0479687149303516\n",
      "train loss:0.2715048080437854, test loss:1.0464062657123636\n",
      "train loss:0.270878021007812, test loss:1.044852642638568\n",
      "train loss:0.27025534413860797, test loss:1.0433059514755978\n",
      "train loss:0.2696357722996459, test loss:1.0417658503696845\n",
      "train loss:0.2690193472379417, test loss:1.0402299742608248\n",
      "train loss:0.2684064714912651, test loss:1.038705024703031\n",
      "train loss:0.2677967745681713, test loss:1.0371829630598448\n",
      "train loss:0.2671900286106254, test loss:1.0356727390763607\n",
      "train loss:0.2659855984895292, test loss:1.032676802309165\n",
      "train loss:0.2653881963406654, test loss:1.0311906400149493\n",
      "train loss:0.26479425095716913, test loss:1.0297113336109305\n",
      "train loss:0.26420377010644414, test loss:1.0282385868848916\n",
      "train loss:0.26361624673024053, test loss:1.0267680954288396\n",
      "train loss:0.26303170085936795, test loss:1.0253029478965845\n",
      "train loss:0.2624504887213617, test loss:1.0238382775471602\n",
      "train loss:0.26187240835504955, test loss:1.0223748852785894\n",
      "train loss:0.26129754934654587, test loss:1.0209131241064708\n",
      "train loss:0.26015614609191373, test loss:1.018001924931942\n",
      "train loss:0.25958976006059026, test loss:1.0165536155731392\n",
      "train loss:0.2590262771397556, test loss:1.0151071196490835\n",
      "train loss:0.25846564600341426, test loss:1.013663490135147\n",
      "train loss:0.2579074136719595, test loss:1.0122229666476752\n",
      "train loss:0.25735171728725653, test loss:1.010788104123833\n",
      "train loss:0.2567987379167721, test loss:1.0093531913789686\n",
      "train loss:0.25624841993114855, test loss:1.0079227938379736\n",
      "train loss:0.2557008220712745, test loss:1.0064958137294118\n",
      "train loss:0.2546140254665017, test loss:1.003670371086037\n",
      "train loss:0.2540747571694751, test loss:1.0022754741334152\n",
      "train loss:0.25353808712596637, test loss:1.0008885024578404\n",
      "train loss:0.2530040582352226, test loss:0.9995096428804094\n",
      "train loss:0.2524725853059788, test loss:0.9981397116200331\n",
      "train loss:0.25194344317274475, test loss:0.9967766001215332\n",
      "train loss:0.25141696748529113, test loss:0.9954209228873411\n",
      "train loss:0.2508926502528561, test loss:0.9940757091094115\n",
      "train loss:0.2503709042453867, test loss:0.992736384823612\n",
      "train loss:0.24933530014144195, test loss:0.9900861183808295\n",
      "train loss:0.2488211482180372, test loss:0.988773352608534\n",
      "train loss:0.24830953841585396, test loss:0.9874679614002958\n",
      "train loss:0.24779994351243637, test loss:0.9861654575235338\n",
      "train loss:0.24729313364689426, test loss:0.9848651359075483\n",
      "train loss:0.24678838534748107, test loss:0.983569246429039\n",
      "train loss:0.2462859265107684, test loss:0.9822716099771402\n",
      "train loss:0.24578593107133306, test loss:0.9809777844248349\n",
      "train loss:0.24528788523128853, test loss:0.9796862858571722\n",
      "train loss:0.2442986060679836, test loss:0.977108712115625\n",
      "train loss:0.2438072772067373, test loss:0.9758248711842397\n",
      "train loss:0.24331840923017148, test loss:0.9745404253369906\n",
      "train loss:0.2428317683704504, test loss:0.9732591916269011\n",
      "train loss:0.24234737004317325, test loss:0.9719828701391733\n",
      "train loss:0.24186503399228304, test loss:0.970708459893134\n",
      "train loss:0.2413847026505774, test loss:0.9694420788992879\n",
      "train loss:0.24090649241022766, test loss:0.968184521606394\n",
      "train loss:0.24043019618475173, test loss:0.9669342324491942\n",
      "train loss:0.23948445286774836, test loss:0.9644534156569233\n",
      "train loss:0.2390147283308066, test loss:0.9632183589664712\n",
      "train loss:0.23854724988660803, test loss:0.9619883093265325\n",
      "train loss:0.23808161231750685, test loss:0.9607555146502985\n",
      "train loss:0.237618371061066, test loss:0.9595300558830163\n",
      "train loss:0.23715684944856646, test loss:0.9583079745326021\n",
      "train loss:0.2366969692234956, test loss:0.957089183590398\n",
      "train loss:0.2362393904172967, test loss:0.9558749605582756\n",
      "train loss:0.23578374307634842, test loss:0.9546652879152333\n",
      "train loss:0.23487844475721587, test loss:0.9522581942007681\n",
      "train loss:0.2344286879583543, test loss:0.9510585911379303\n",
      "train loss:0.23398098669469505, test loss:0.9498675478035905\n",
      "train loss:0.2335350816250182, test loss:0.9486806172075516\n",
      "train loss:0.23309107930925532, test loss:0.9474995689190804\n",
      "train loss:0.23264895437800376, test loss:0.9463209706271477\n",
      "train loss:0.23220850137043636, test loss:0.9451471761861528\n",
      "train loss:0.23176997922614934, test loss:0.9439789593135084\n",
      "train loss:0.23133319430790455, test loss:0.942820249017341\n",
      "train loss:0.23046489428695371, test loss:0.9405179135922238\n",
      "train loss:0.23003343470176985, test loss:0.9393688737901914\n",
      "train loss:0.22960352328342593, test loss:0.9382240796256796\n",
      "train loss:0.22917551007124373, test loss:0.9370831921124667\n",
      "train loss:0.22874942537047474, test loss:0.9359496673492599\n",
      "train loss:0.2283249289059598, test loss:0.9348228274405082\n",
      "train loss:0.2279024526987055, test loss:0.93369889927924\n",
      "train loss:0.22748144914090976, test loss:0.9325844234729298\n",
      "train loss:0.22706229170898412, test loss:0.9314766379440015\n",
      "train loss:0.2262290274341778, test loss:0.9292726095583324\n",
      "train loss:0.2258149819608494, test loss:0.9281794642117319\n",
      "train loss:0.2254023526313705, test loss:0.927086932530286\n",
      "train loss:0.22499161401085116, test loss:0.9259996161607685\n",
      "train loss:0.22458238373989353, test loss:0.9249114896468288\n",
      "train loss:0.22417471725413884, test loss:0.9238262314226507\n",
      "train loss:0.22376850323136252, test loss:0.9227395562516482\n",
      "train loss:0.2233638451980099, test loss:0.9216555940311963\n",
      "train loss:0.22296055526763017, test loss:0.9205731094445828\n",
      "train loss:0.22215914164830036, test loss:0.9184262369060854\n",
      "train loss:0.221760923903315, test loss:0.9173623972105641\n",
      "train loss:0.2213639472786988, test loss:0.9163007376257651\n",
      "train loss:0.22096861131219825, test loss:0.9152450178021858\n",
      "train loss:0.22057469529840776, test loss:0.9141897995078431\n",
      "train loss:0.2201825015970719, test loss:0.9131383792823915\n",
      "train loss:0.21979199689380768, test loss:0.9120891121555842\n",
      "train loss:0.21940271732647393, test loss:0.9110432792588322\n",
      "train loss:0.21901497996640176, test loss:0.9099968595537735\n",
      "train loss:0.21824400409624556, test loss:0.907900626053138\n",
      "train loss:0.21786087533085843, test loss:0.9068485158167164\n",
      "train loss:0.217479379727412, test loss:0.9057944133076647\n",
      "train loss:0.21709959858210298, test loss:0.9047361023560871\n",
      "train loss:0.21672103819873045, test loss:0.9036763935697093\n",
      "train loss:0.21634402450449725, test loss:0.9026137461069355\n",
      "train loss:0.2159684479464134, test loss:0.9015527768244525\n",
      "train loss:0.21559410688405403, test loss:0.900494059251366\n",
      "train loss:0.215221390779807, test loss:0.899438059720697\n",
      "train loss:0.21447952187793518, test loss:0.8973316774574798\n",
      "train loss:0.2141108211842749, test loss:0.8962850341124303\n",
      "train loss:0.21374349431354905, test loss:0.8952436931538563\n",
      "train loss:0.2133772470807869, test loss:0.8942110183474735\n",
      "train loss:0.2130125781669207, test loss:0.8931827109298575\n",
      "train loss:0.21264931917861846, test loss:0.8921609800468718\n",
      "train loss:0.2122874549970256, test loss:0.8911442644615253\n",
      "train loss:0.21192707407001385, test loss:0.8901348891206052\n",
      "train loss:0.21156785809770293, test loss:0.8891332397816631\n",
      "train loss:0.2108535009817107, test loss:0.8871443368280076\n",
      "train loss:0.21049825893044477, test loss:0.8861552793526987\n",
      "train loss:0.21014436019810484, test loss:0.8851737364657051\n",
      "train loss:0.20979163698280023, test loss:0.8841913556670487\n",
      "train loss:0.2094401343537049, test loss:0.883218370877622\n",
      "train loss:0.20909020169989928, test loss:0.8822474095866132\n",
      "train loss:0.20874146773136634, test loss:0.8812822351470593\n",
      "train loss:0.2083940120435175, test loss:0.8803232013191855\n",
      "train loss:0.20804783940731392, test loss:0.879367526981697\n",
      "train loss:0.20735874410750132, test loss:0.8774714751420095\n",
      "train loss:0.2070161890528034, test loss:0.8765309866302057\n",
      "train loss:0.20667483749427118, test loss:0.8755973509428404\n",
      "train loss:0.20633487745717133, test loss:0.8746661718826131\n",
      "train loss:0.20599622710622317, test loss:0.8737407150812259\n",
      "train loss:0.20565856764281956, test loss:0.8728151156081788\n",
      "train loss:0.2053223185476969, test loss:0.871890121346998\n",
      "train loss:0.20498702210554548, test loss:0.8709610727461963\n",
      "train loss:0.2046528851039143, test loss:0.8700277477424908\n",
      "train loss:0.20398829491270604, test loss:0.8681478306707947\n",
      "train loss:0.20365767374257868, test loss:0.8672001216024515\n",
      "train loss:0.2033280939952703, test loss:0.866252346495591\n",
      "train loss:0.20299985214593055, test loss:0.8652992913414396\n",
      "train loss:0.20267271027195569, test loss:0.8643428111814488\n",
      "train loss:0.20234665335573795, test loss:0.863389862093136\n",
      "train loss:0.20202147783673918, test loss:0.862436175195586\n",
      "train loss:0.20169750960856392, test loss:0.861486309165462\n",
      "train loss:0.20137457496955585, test loss:0.8605419624149478\n",
      "train loss:0.20073219006211632, test loss:0.858665611250065\n",
      "train loss:0.200412906298218, test loss:0.8577323116213703\n",
      "train loss:0.20009442377228054, test loss:0.8568085190531597\n",
      "train loss:0.19977712037586765, test loss:0.8558895504173581\n",
      "train loss:0.19946058647425516, test loss:0.8549746887033389\n",
      "train loss:0.19914513792815125, test loss:0.8540658967002794\n",
      "train loss:0.19883068923525662, test loss:0.8531609578781817\n",
      "train loss:0.19851686115033626, test loss:0.8522646325245923\n",
      "train loss:0.19820459804288149, test loss:0.8513783310865829\n",
      "train loss:0.19758285473695528, test loss:0.8496169834330582\n",
      "train loss:0.19727332358600497, test loss:0.8487471808125895\n",
      "train loss:0.19696509636809303, test loss:0.84788109933811\n",
      "train loss:0.19665771369553725, test loss:0.8470200269642792\n",
      "train loss:0.1963515180643816, test loss:0.8461605256230921\n",
      "train loss:0.19604616430384483, test loss:0.8453018953109167\n",
      "train loss:0.1957418260960579, test loss:0.8444475208620751\n",
      "train loss:0.1954385205631589, test loss:0.8435948753317769\n",
      "train loss:0.1951361727092122, test loss:0.8427432947278903\n",
      "train loss:0.194534659609405, test loss:0.8410562079086099\n",
      "train loss:0.19423533874620483, test loss:0.840222735815411\n",
      "train loss:0.19393704237414222, test loss:0.8393938603188984\n",
      "train loss:0.19363976977908343, test loss:0.8385734148969534\n",
      "train loss:0.19334345201540654, test loss:0.8377595276914164\n",
      "train loss:0.19304804970252049, test loss:0.8369538253575138\n",
      "train loss:0.19275369485360766, test loss:0.8361528695533654\n",
      "train loss:0.19246019560975502, test loss:0.8353564692651815\n",
      "train loss:0.19216773603786244, test loss:0.8345649064706732\n",
      "train loss:0.19158545798969112, test loss:0.8329905298097683\n",
      "train loss:0.19129571616120483, test loss:0.8322053037401806\n",
      "train loss:0.1910068337519447, test loss:0.8314235495151312\n",
      "train loss:0.19071868080969206, test loss:0.8306413158165274\n",
      "train loss:0.19043155169684017, test loss:0.8298617024413789\n",
      "train loss:0.1901451448682544, test loss:0.8290825269940842\n",
      "train loss:0.1898596814523202, test loss:0.8283047816979754\n",
      "train loss:0.18957513194227665, test loss:0.8275297231542994\n",
      "train loss:0.18929145445465498, test loss:0.8267559077808817\n",
      "train loss:0.18872655334822688, test loss:0.8252090841523416\n",
      "train loss:0.1884454913938588, test loss:0.8244387679323721\n",
      "train loss:0.18816546027389466, test loss:0.8236675854113215\n",
      "train loss:0.18788616527951854, test loss:0.8228969099062734\n",
      "train loss:0.18760777991774452, test loss:0.8221273208155736\n",
      "train loss:0.18733000817132722, test loss:0.8213611289148077\n",
      "train loss:0.1870529481342445, test loss:0.8205960887950754\n",
      "train loss:0.18677661512039057, test loss:0.8198342360573048\n",
      "train loss:0.18650116855357424, test loss:0.8190743687828819\n",
      "train loss:0.18595201051118693, test loss:0.8175598249436614\n",
      "train loss:0.18567869082508645, test loss:0.8168054252201549\n",
      "train loss:0.18540607457493932, test loss:0.8160557189023991\n",
      "train loss:0.18513437339185718, test loss:0.8153092578180902\n",
      "train loss:0.18486359373369277, test loss:0.8145673856500512\n",
      "train loss:0.18459352687996225, test loss:0.8138266050127198\n",
      "train loss:0.18432455625666086, test loss:0.8130887461839601\n",
      "train loss:0.18405625726990393, test loss:0.8123529541227044\n",
      "train loss:0.1837886351632366, test loss:0.8116175781628977\n",
      "train loss:0.18325570396976934, test loss:0.8101497256895979\n",
      "train loss:0.18299052468773377, test loss:0.8094168855403394\n",
      "train loss:0.18272584064052239, test loss:0.8086852432703415\n",
      "train loss:0.18246197442560375, test loss:0.8079558744943698\n",
      "train loss:0.1821989393215136, test loss:0.8072304734709888\n",
      "train loss:0.18193645152223858, test loss:0.806508939933044\n",
      "train loss:0.1816747601301511, test loss:0.8057881633734769\n",
      "train loss:0.18141367713658385, test loss:0.8050693990254899\n",
      "train loss:0.1811533400902359, test loss:0.8043519461819756\n",
      "train loss:0.18063493432141717, test loss:0.802920784764189\n",
      "train loss:0.18037679881831775, test loss:0.802208513117534\n",
      "train loss:0.18011934508916472, test loss:0.8014976192894675\n",
      "train loss:0.17986281347198066, test loss:0.8007852145522292\n",
      "train loss:0.1796069757380329, test loss:0.8000767730881642\n",
      "train loss:0.17935176041583942, test loss:0.7993677753681867\n",
      "train loss:0.17909735843115923, test loss:0.7986577909507055\n",
      "train loss:0.17884348128314576, test loss:0.7979524324931069\n",
      "train loss:0.17859056682071828, test loss:0.7972502801619995\n",
      "train loss:0.17808668452833662, test loss:0.7958475296077612\n",
      "train loss:0.17783573871737932, test loss:0.7951482210285593\n",
      "train loss:0.17758539904282888, test loss:0.7944489277899266\n",
      "train loss:0.17733581479368488, test loss:0.7937504871020516\n",
      "train loss:0.17708672936616157, test loss:0.7930523800345546\n",
      "train loss:0.17683834336694965, test loss:0.7923560083937061\n",
      "train loss:0.1765907928246146, test loss:0.7916617040700089\n",
      "train loss:0.17634400037875622, test loss:0.7909663552039481\n",
      "train loss:0.1760976876809964, test loss:0.790266738566588\n",
      "train loss:0.17560702449176732, test loss:0.7888657793239288\n",
      "train loss:0.17536266631581238, test loss:0.7881637005075125\n",
      "train loss:0.1751191885762026, test loss:0.7874616012691996\n",
      "train loss:0.17487613735072766, test loss:0.78675883430833\n",
      "train loss:0.17463375202376985, test loss:0.7860586982901095\n",
      "train loss:0.1743923967517448, test loss:0.7853605960686681\n",
      "train loss:0.1741515677884672, test loss:0.7846643764416322\n",
      "train loss:0.17391125356433365, test loss:0.7839719572975377\n",
      "train loss:0.17367166968246267, test loss:0.7832799004346005\n",
      "train loss:0.17319450175859635, test loss:0.781909562052818\n",
      "train loss:0.17295689450157936, test loss:0.7812299966966711\n",
      "train loss:0.1727198252650768, test loss:0.7805548878861428\n",
      "train loss:0.17248353305378558, test loss:0.7798778968346605\n",
      "train loss:0.1722478741557558, test loss:0.7792060345552965\n",
      "train loss:0.17201287328537657, test loss:0.7785368372200645\n",
      "train loss:0.17177835353057444, test loss:0.7778734351555876\n",
      "train loss:0.17154468622882332, test loss:0.7772125253805379\n",
      "train loss:0.17131157018607865, test loss:0.7765571701440563\n",
      "train loss:0.17084720214535926, test loss:0.7752507073549745\n",
      "train loss:0.17061606962616882, test loss:0.7746004939082248\n",
      "train loss:0.17038552493142672, test loss:0.773953749406405\n",
      "train loss:0.17015580582656595, test loss:0.7733060053114198\n",
      "train loss:0.16992679538029723, test loss:0.772660865870424\n",
      "train loss:0.16969821136257823, test loss:0.7720167070380174\n",
      "train loss:0.16947028351414442, test loss:0.7713751599841356\n",
      "train loss:0.16924285321876412, test loss:0.770732529055955\n",
      "train loss:0.16901617892146367, test loss:0.7700949603661702\n",
      "train loss:0.1685646290668906, test loss:0.7688192117878344\n",
      "train loss:0.16833957704992508, test loss:0.768182280466671\n",
      "train loss:0.1681152653714843, test loss:0.7675449142605246\n",
      "train loss:0.16789145891340507, test loss:0.7669044298244344\n",
      "train loss:0.16766814900612195, test loss:0.7662649542458869\n",
      "train loss:0.16744540799463697, test loss:0.7656226589874756\n",
      "train loss:0.16722309085378462, test loss:0.764976659643334\n",
      "train loss:0.16700160384149457, test loss:0.76432518483849\n",
      "train loss:0.1667806006904931, test loss:0.76367279039952\n",
      "train loss:0.16634053015490813, test loss:0.7623613403558726\n",
      "train loss:0.1661211666256056, test loss:0.7617052494096908\n",
      "train loss:0.16590248219422568, test loss:0.7610471067648432\n",
      "train loss:0.1656843854105745, test loss:0.7603919157107729\n",
      "train loss:0.16546692875813476, test loss:0.7597380369972149\n",
      "train loss:0.16524986598163208, test loss:0.7590839353862763\n",
      "train loss:0.1650337502981432, test loss:0.7584336647545022\n",
      "train loss:0.16481802261123574, test loss:0.7577809890973508\n",
      "train loss:0.16460292175282445, test loss:0.7571264698703966\n",
      "train loss:0.1641745710070226, test loss:0.755817863009606\n",
      "train loss:0.16396102234215035, test loss:0.7551625989246593\n",
      "train loss:0.1637481159894908, test loss:0.7545086058190711\n",
      "train loss:0.16353568180814498, test loss:0.7538564834635574\n",
      "train loss:0.16332401897174237, test loss:0.7532036400128318\n",
      "train loss:0.16311250624146006, test loss:0.7525550503047527\n",
      "train loss:0.16290169282334957, test loss:0.7519083256597873\n",
      "train loss:0.16269145586372563, test loss:0.7512634632265667\n",
      "train loss:0.16248169122873515, test loss:0.7506228716192405\n",
      "train loss:0.1620640646712836, test loss:0.749356030802754\n",
      "train loss:0.16185612630654433, test loss:0.748729736943254\n",
      "train loss:0.1616485022384514, test loss:0.7481097654056139\n",
      "train loss:0.1614415268133043, test loss:0.7474946020506229\n",
      "train loss:0.16123498905302397, test loss:0.746885115106854\n",
      "train loss:0.16102902676813796, test loss:0.746280126852162\n",
      "train loss:0.16082346552983956, test loss:0.7456798137467598\n",
      "train loss:0.16061861464869726, test loss:0.7450784264631819\n",
      "train loss:0.16041424232027737, test loss:0.744481841276353\n",
      "train loss:0.16000717168496226, test loss:0.7432834952404918\n",
      "train loss:0.15980442321179497, test loss:0.7426873873628307\n",
      "train loss:0.15960211078809847, test loss:0.7420928353949227\n",
      "train loss:0.1594004397407846, test loss:0.7415001689374415\n",
      "train loss:0.15919921077933222, test loss:0.7409082516607826\n",
      "train loss:0.15899859316843595, test loss:0.7403138716771939\n",
      "train loss:0.15879841742924625, test loss:0.7397211767858747\n",
      "train loss:0.1585986857296807, test loss:0.7391301809808191\n",
      "train loss:0.15839958328970422, test loss:0.7385408171141589\n",
      "train loss:0.15800289682565746, test loss:0.7373629626466098\n",
      "train loss:0.15780537971180755, test loss:0.7367772505644514\n",
      "train loss:0.15760829603413903, test loss:0.7361919724594895\n",
      "train loss:0.15741160417747121, test loss:0.7356113007667852\n",
      "train loss:0.157215462960726, test loss:0.735032886731933\n",
      "train loss:0.15701969141457095, test loss:0.7344563109695501\n",
      "train loss:0.15682446787861476, test loss:0.7338816188602922\n",
      "train loss:0.1566293808222164, test loss:0.7333121614997091\n",
      "train loss:0.1564349168988511, test loss:0.7327465782872336\n",
      "train loss:0.1560470555147182, test loss:0.7316191470974214\n",
      "train loss:0.1558538912795611, test loss:0.7310627644894678\n",
      "train loss:0.15566107528450995, test loss:0.7305057875828401\n",
      "train loss:0.15546869239331676, test loss:0.7299505517822279\n",
      "train loss:0.15527667978371285, test loss:0.729397095036787\n",
      "train loss:0.15508511176568981, test loss:0.7288440047526434\n",
      "train loss:0.15489395468712971, test loss:0.7282896622600897\n",
      "train loss:0.15470327709093304, test loss:0.7277366871778032\n",
      "train loss:0.15451307736161232, test loss:0.727180004342819\n",
      "train loss:0.1541342519541897, test loss:0.7260648941632734\n",
      "train loss:0.153945362006853, test loss:0.7255047396475839\n",
      "train loss:0.15375705504068468, test loss:0.7249453524244339\n",
      "train loss:0.15356913750861315, test loss:0.7243867913509252\n",
      "train loss:0.1533815720167353, test loss:0.7238281170589592\n",
      "train loss:0.1531946482659283, test loss:0.7232687410023769\n",
      "train loss:0.15300813109830919, test loss:0.7227139671416479\n",
      "train loss:0.1528220644618604, test loss:0.7221595933709409\n",
      "train loss:0.15263643281188322, test loss:0.7216094206426901\n",
      "train loss:0.15226643000245138, test loss:0.7205119470298151\n",
      "train loss:0.15208205522881485, test loss:0.7199639123473014\n",
      "train loss:0.15189812708964925, test loss:0.7194193780747161\n",
      "train loss:0.15171465431965586, test loss:0.7188744506325198\n",
      "train loss:0.1515316419674455, test loss:0.7183314179408962\n",
      "train loss:0.15134904539554922, test loss:0.7177890530890839\n",
      "train loss:0.1511670254800341, test loss:0.7172469577314096\n",
      "train loss:0.15098514790938547, test loss:0.7167043907911865\n",
      "train loss:0.15080385897955828, test loss:0.7161608431900283\n",
      "train loss:0.15044262097008076, test loss:0.7150742490016643\n",
      "train loss:0.1502627852346775, test loss:0.7145329608081437\n",
      "train loss:0.15008336586742096, test loss:0.7139926736891863\n",
      "train loss:0.1499043106524389, test loss:0.7134503294416564\n",
      "train loss:0.14972575151832812, test loss:0.7129114761685545\n",
      "train loss:0.149547537759217, test loss:0.7123685015202719\n",
      "train loss:0.1493699233002146, test loss:0.7118257504937343\n",
      "train loss:0.14919249722848404, test loss:0.7112833064299972\n",
      "train loss:0.149015765025213, test loss:0.710739878982909\n",
      "train loss:0.14866362496279367, test loss:0.7096510190531415\n",
      "train loss:0.1484882843349565, test loss:0.7091063366664921\n",
      "train loss:0.14831322501448033, test loss:0.7085612861837433\n",
      "train loss:0.14813879173117073, test loss:0.7080167483509008\n",
      "train loss:0.1479646860725839, test loss:0.7074722642305271\n",
      "train loss:0.14779105735036624, test loss:0.7069316971323548\n",
      "train loss:0.14761780598971674, test loss:0.7063935024385042\n",
      "train loss:0.14744503919394125, test loss:0.7058551861935959\n",
      "train loss:0.14727274035935983, test loss:0.7053182808634402\n",
      "train loss:0.14692944336490216, test loss:0.704244646213876\n",
      "train loss:0.14675825717052277, test loss:0.7037087817453176\n",
      "train loss:0.14658747794801838, test loss:0.7031699000253725\n",
      "train loss:0.14641711416845524, test loss:0.7026283867773557\n",
      "train loss:0.14624699061198412, test loss:0.7020907554568161\n",
      "train loss:0.14607730343774553, test loss:0.7015503922781957\n",
      "train loss:0.1459079592534935, test loss:0.7010109757588223\n",
      "train loss:0.14573909277095173, test loss:0.7004723342722603\n",
      "train loss:0.14557042060172992, test loss:0.699935162903732\n",
      "train loss:0.14523438056567206, test loss:0.6988640299025759\n",
      "train loss:0.14506695407507386, test loss:0.6983289928495882\n",
      "train loss:0.14489982387394093, test loss:0.6977992677075194\n",
      "train loss:0.14473299136796336, test loss:0.6972716109649465\n",
      "train loss:0.14456662699792627, test loss:0.6967491587722094\n",
      "train loss:0.14440060011241845, test loss:0.6962279375920855\n",
      "train loss:0.14423490375986134, test loss:0.6957112455450825\n",
      "train loss:0.1440696506394339, test loss:0.6951964833750524\n",
      "train loss:0.14390478128847747, test loss:0.6946823953396639\n",
      "train loss:0.1435759324462928, test loss:0.6936619376915782\n",
      "train loss:0.14341206052474245, test loss:0.6931536903918072\n",
      "train loss:0.1432484946582123, test loss:0.6926451795431791\n",
      "train loss:0.14308540532796835, test loss:0.6921401321491547\n",
      "train loss:0.14292261845340748, test loss:0.6916340716452021\n",
      "train loss:0.1427602217347266, test loss:0.6911324629496638\n",
      "train loss:0.1425981827685202, test loss:0.6906290245319163\n",
      "train loss:0.14243651085903253, test loss:0.690127416212873\n",
      "train loss:0.1422752094216156, test loss:0.6896244849003278\n",
      "train loss:0.14195382692442532, test loss:0.6886273454930742\n",
      "train loss:0.14179351038940546, test loss:0.6881291142102512\n",
      "train loss:0.14163351614752218, test loss:0.6876285255499933\n",
      "train loss:0.1414738067761646, test loss:0.6871309846475514\n",
      "train loss:0.14131439824122236, test loss:0.6866346879061416\n",
      "train loss:0.14115545653102068, test loss:0.6861409477436811\n",
      "train loss:0.14099665663059743, test loss:0.6856489406482635\n",
      "train loss:0.14083830746308526, test loss:0.6851582934365197\n",
      "train loss:0.14068026212508522, test loss:0.6846720480558814\n",
      "train loss:0.14036523729108785, test loss:0.6836978395589459\n",
      "train loss:0.14020816191496666, test loss:0.6832095707664824\n",
      "train loss:0.14005167496647586, test loss:0.6827264670453297\n",
      "train loss:0.13989537215172343, test loss:0.6822394084970209\n",
      "train loss:0.13973943067207292, test loss:0.6817520129388538\n",
      "train loss:0.1395838719997155, test loss:0.6812618434102996\n",
      "train loss:0.13942860482977387, test loss:0.6807711844454718\n",
      "train loss:0.13927377280786857, test loss:0.6802779452563391\n",
      "train loss:0.13911914187080593, test loss:0.6797853315250189\n",
      "train loss:0.1388109723871142, test loss:0.6788018927496529\n",
      "train loss:0.13865743235446143, test loss:0.6783120130055401\n",
      "train loss:0.13850406669717677, test loss:0.6778230824733261\n",
      "train loss:0.13835127185393994, test loss:0.6773353953995335\n",
      "train loss:0.13819860355133562, test loss:0.6768525555102549\n",
      "train loss:0.1380462905599988, test loss:0.6763694955697513\n",
      "train loss:0.13789446034748604, test loss:0.6758882125838898\n",
      "train loss:0.13774294862548084, test loss:0.6754076210531553\n",
      "train loss:0.13759166999908984, test loss:0.6749312624835093\n",
      "train loss:0.13729010709101785, test loss:0.6739845372120414\n",
      "train loss:0.1371398389655744, test loss:0.6735131478527436\n",
      "train loss:0.13698997443878386, test loss:0.673043941383338\n",
      "train loss:0.1368402758766631, test loss:0.6725760106043547\n",
      "train loss:0.13669097123616614, test loss:0.6721080420393803\n",
      "train loss:0.13654198757071626, test loss:0.6716415582718637\n",
      "train loss:0.13639312295759842, test loss:0.6711771772754986\n",
      "train loss:0.13624470237670613, test loss:0.6707117273080571\n",
      "train loss:0.1360965597766188, test loss:0.670248068699449\n",
      "train loss:0.1358011261948722, test loss:0.6693273407796131\n",
      "train loss:0.13565394816793655, test loss:0.6688705222925587\n",
      "train loss:0.1355070784445425, test loss:0.6684138628340145\n",
      "train loss:0.1353604133654358, test loss:0.667959641561842\n",
      "train loss:0.13521424717715655, test loss:0.6675097955628955\n",
      "train loss:0.13506825719846746, test loss:0.6670581294721991\n",
      "train loss:0.13492270773613807, test loss:0.6666106504066117\n",
      "train loss:0.1347775455835625, test loss:0.6661621560211805\n",
      "train loss:0.13463251653297192, test loss:0.6657147261978398\n",
      "train loss:0.13434362569722913, test loss:0.6648208687366223\n",
      "train loss:0.13419965348557966, test loss:0.664371784802837\n",
      "train loss:0.13405579058271483, test loss:0.6639245002262378\n",
      "train loss:0.13391236123615477, test loss:0.6634789463646956\n",
      "train loss:0.13376921154857838, test loss:0.6630372956526929\n",
      "train loss:0.1336262672218982, test loss:0.6625943716228971\n",
      "train loss:0.13348359673510768, test loss:0.6621515324138281\n",
      "train loss:0.13334126215059416, test loss:0.6617132341952519\n",
      "train loss:0.1331990837668822, test loss:0.6612743274903307\n",
      "train loss:0.13291599303523313, test loss:0.6604022169952724\n",
      "train loss:0.1327747332947169, test loss:0.6599691242341377\n",
      "train loss:0.13263375579020886, test loss:0.6595375703933294\n",
      "train loss:0.13249310774414547, test loss:0.6591059218391264\n",
      "train loss:0.13235268484670165, test loss:0.6586761560872497\n",
      "train loss:0.1322125023559323, test loss:0.6582443662971379\n",
      "train loss:0.13207244659015394, test loss:0.6578141248613331\n",
      "train loss:0.1319328732941991, test loss:0.6573865474033301\n",
      "train loss:0.13179355896685155, test loss:0.6569590919882213\n",
      "train loss:0.13151579100387356, test loss:0.656109999591299\n",
      "train loss:0.13137728320405143, test loss:0.6556894083456493\n",
      "train loss:0.13123895525518428, test loss:0.6552693606290109\n",
      "train loss:0.13110099851779655, test loss:0.6548542535433478\n",
      "train loss:0.13096317576308947, test loss:0.6544403915248845\n",
      "train loss:0.13082565037607993, test loss:0.6540294854880186\n",
      "train loss:0.13068843038031064, test loss:0.6536171124561805\n",
      "train loss:0.13055141933854425, test loss:0.6532078170028406\n",
      "train loss:0.13041482809604382, test loss:0.6527982026805919\n",
      "train loss:0.13014230815230293, test loss:0.6519766483513795\n",
      "train loss:0.13000658874676724, test loss:0.6515662967828372\n",
      "train loss:0.12987100166061757, test loss:0.6511571714848913\n",
      "train loss:0.12973576522655828, test loss:0.6507469570776929\n",
      "train loss:0.12960069926412138, test loss:0.6503354984243502\n",
      "train loss:0.12946585338176156, test loss:0.649922959526638\n",
      "train loss:0.12933134962679324, test loss:0.6495112682786967\n",
      "train loss:0.12919703977869873, test loss:0.6490971705236556\n",
      "train loss:0.12906299689321235, test loss:0.6486821135814668\n",
      "train loss:0.1287956785134619, test loss:0.6478463317561669\n",
      "train loss:0.1286624216608992, test loss:0.6474247093963277\n",
      "train loss:0.12852948236866008, test loss:0.6470017336756526\n",
      "train loss:0.1283965750875075, test loss:0.6465757281906894\n",
      "train loss:0.1282641224272252, test loss:0.6461508828300057\n",
      "train loss:0.1281318123049678, test loss:0.6457239977652727\n",
      "train loss:0.1279997792868897, test loss:0.6452952467756078\n",
      "train loss:0.1278680642230326, test loss:0.6448657351781016\n",
      "train loss:0.12773642260040668, test loss:0.6444357821444157\n",
      "train loss:0.12747424492806772, test loss:0.6435754108376526\n",
      "train loss:0.1273435043455825, test loss:0.6431471463599641\n",
      "train loss:0.1272129980309855, test loss:0.6427185337712074\n",
      "train loss:0.12708283491401115, test loss:0.6422911126170124\n",
      "train loss:0.12695286937750047, test loss:0.6418662563167488\n",
      "train loss:0.1268231022383418, test loss:0.641443020888302\n",
      "train loss:0.1266935833798055, test loss:0.6410216057786355\n",
      "train loss:0.12656429459499607, test loss:0.6406037117712717\n",
      "train loss:0.12643532054558715, test loss:0.6401868779422263\n",
      "train loss:0.12617790241772137, test loss:0.639354009687905\n",
      "train loss:0.12604948698480845, test loss:0.6389398499749077\n",
      "train loss:0.125921304182428, test loss:0.6385237141177793\n",
      "train loss:0.12579339923689217, test loss:0.6381091582816004\n",
      "train loss:0.12566576995817774, test loss:0.6376969587233519\n",
      "train loss:0.1255383768307485, test loss:0.637284755335574\n",
      "train loss:0.1254111640186857, test loss:0.6368737828248903\n",
      "train loss:0.12528432341437717, test loss:0.6364649373327794\n",
      "train loss:0.12515767120839752, test loss:0.6360582927009016\n",
      "train loss:0.12490499400224016, test loss:0.6352451286107055\n",
      "train loss:0.12477912112935234, test loss:0.634840795867179\n",
      "train loss:0.12465355882000578, test loss:0.634434697075129\n",
      "train loss:0.12452803999651701, test loss:0.6340307461160712\n",
      "train loss:0.12440304001472019, test loss:0.6336273845673395\n",
      "train loss:0.12427811812260947, test loss:0.6332269031187427\n",
      "train loss:0.12415347687594638, test loss:0.6328274493941071\n",
      "train loss:0.12402913277525987, test loss:0.6324269862134368\n",
      "train loss:0.12390495036557099, test loss:0.6320274056052525\n",
      "train loss:0.12365755133937019, test loss:0.6312303382233482\n",
      "train loss:0.12353416898126965, test loss:0.6308314496340072\n",
      "train loss:0.12341099802089928, test loss:0.630432097129139\n",
      "train loss:0.12328813219025224, test loss:0.6300325495343875\n",
      "train loss:0.1231654015369297, test loss:0.6296321027567398\n",
      "train loss:0.12304294440517208, test loss:0.6292336801329419\n",
      "train loss:0.12292070401692987, test loss:0.6288354535188175\n",
      "train loss:0.12279857162778923, test loss:0.6284347371028257\n",
      "train loss:0.12267684875684245, test loss:0.6280356745945738\n",
      "train loss:0.12243371838699675, test loss:0.62724009085779\n",
      "train loss:0.12231252604549468, test loss:0.6268450509358982\n",
      "train loss:0.12219154107784994, test loss:0.6264500886693865\n",
      "train loss:0.12207071704615466, test loss:0.6260557108275472\n",
      "train loss:0.12195010292095049, test loss:0.6256607692586652\n",
      "train loss:0.12182971894596056, test loss:0.6252656136159437\n",
      "train loss:0.12170953851139095, test loss:0.6248691201433068\n",
      "train loss:0.12158963011943613, test loss:0.62447004420967\n",
      "train loss:0.12146995251586644, test loss:0.6240717786889725\n",
      "train loss:0.12123124334543978, test loss:0.623270838728603\n",
      "train loss:0.12111221192888459, test loss:0.6228709759144432\n",
      "train loss:0.12099345498881865, test loss:0.6224692311456571\n",
      "train loss:0.12087488598536043, test loss:0.6220672209647643\n",
      "train loss:0.12075644990266528, test loss:0.6216667963494366\n",
      "train loss:0.12063819627756486, test loss:0.6212670005184582\n",
      "train loss:0.12052019261328614, test loss:0.6208690484439476\n",
      "train loss:0.12040226707353917, test loss:0.6204722044964351\n",
      "train loss:0.12028469485000999, test loss:0.6200770000652971\n",
      "train loss:0.12004992032352937, test loss:0.6192921837380457\n",
      "train loss:0.11993285946388478, test loss:0.6189008709634118\n",
      "train loss:0.1198160615951463, test loss:0.6185112085588302\n",
      "train loss:0.11969937061209734, test loss:0.6181220937252644\n",
      "train loss:0.11958293974978723, test loss:0.6177319555846283\n",
      "train loss:0.11946669171618048, test loss:0.6173414161163686\n",
      "train loss:0.11935069560091151, test loss:0.6169533394637122\n",
      "train loss:0.11923493028511462, test loss:0.6165639544497165\n",
      "train loss:0.11911924198955522, test loss:0.6161759348110023\n",
      "train loss:0.11888876356251286, test loss:0.6154013787939274\n",
      "train loss:0.11877377253996611, test loss:0.6150147020412922\n",
      "train loss:0.11865905625339386, test loss:0.6146293764988711\n",
      "train loss:0.11854454008925916, test loss:0.6142429696505645\n",
      "train loss:0.1184302139927802, test loss:0.6138583898552163\n",
      "train loss:0.11831616857146142, test loss:0.6134723951759343\n",
      "train loss:0.11820239801220414, test loss:0.6130859206653961\n",
      "train loss:0.11808880819644348, test loss:0.6126989860384214\n",
      "train loss:0.11797545200726292, test loss:0.612308056855189\n",
      "train loss:0.11774929560036258, test loss:0.6115242076449808\n",
      "train loss:0.1176364933119304, test loss:0.6111326329952801\n",
      "train loss:0.11752394432048832, test loss:0.6107409376113092\n",
      "train loss:0.11741156684026739, test loss:0.610350902641485\n",
      "train loss:0.11729927461048617, test loss:0.6099613984586011\n",
      "train loss:0.11718718283633824, test loss:0.6095751388233623\n",
      "train loss:0.11707521597165323, test loss:0.6091895161198699\n",
      "train loss:0.11696348849596436, test loss:0.6088038947349389\n",
      "train loss:0.11685197114538413, test loss:0.608420212904785\n",
      "train loss:0.11662941601121908, test loss:0.6076616452168819\n",
      "train loss:0.11651836647471853, test loss:0.6072847408263503\n",
      "train loss:0.11640753366644212, test loss:0.6069102894827018\n",
      "train loss:0.11629682600189349, test loss:0.6065376270745155\n",
      "train loss:0.11618635395960687, test loss:0.6061663000620435\n",
      "train loss:0.11607595942122576, test loss:0.6057959256206383\n",
      "train loss:0.11596587304787888, test loss:0.6054266694160665\n",
      "train loss:0.11585590664299096, test loss:0.6050588051082796\n",
      "train loss:0.11574614506386333, test loss:0.6046924045628984\n",
      "train loss:0.1155270962379393, test loss:0.6039610931628161\n",
      "train loss:0.11541788709314162, test loss:0.6035975726180722\n",
      "train loss:0.11530892122220328, test loss:0.6032372482349844\n",
      "train loss:0.11520001164583049, test loss:0.6028784243009694\n",
      "train loss:0.11509138168469002, test loss:0.6025204694092858\n",
      "train loss:0.11498287488913914, test loss:0.6021641319253795\n",
      "train loss:0.11487461797540229, test loss:0.6018069540025569\n",
      "train loss:0.11476653343013357, test loss:0.6014506324471803\n",
      "train loss:0.11465859049703746, test loss:0.6010952981071624\n",
      "train loss:0.11444333935821531, test loss:0.6003854371463119\n",
      "train loss:0.11433597678150229, test loss:0.6000336268601378\n",
      "train loss:0.11422891963214792, test loss:0.5996824226235064\n",
      "train loss:0.11412200013842794, test loss:0.5993315910105796\n",
      "train loss:0.1140152423820209, test loss:0.5989822563699428\n",
      "train loss:0.11390851986385146, test loss:0.5986319721744804\n",
      "train loss:0.11380212230355147, test loss:0.5982860234680079\n",
      "train loss:0.11369593730081146, test loss:0.59793665310598\n",
      "train loss:0.11358996232521837, test loss:0.5975896367231496\n",
      "train loss:0.11337836280639285, test loss:0.5968955272014369\n",
      "train loss:0.1132728499188157, test loss:0.5965498719986424\n",
      "train loss:0.1131675119754737, test loss:0.5962061882610692\n",
      "train loss:0.11306236977912322, test loss:0.5958608206758084\n",
      "train loss:0.11295734946500517, test loss:0.595516521333609\n",
      "train loss:0.11285253377544407, test loss:0.5951738702236531\n",
      "train loss:0.11274791044107621, test loss:0.594830748098907\n",
      "train loss:0.11264350274227863, test loss:0.5944884549946571\n",
      "train loss:0.11253935454142461, test loss:0.594145650519867\n",
      "train loss:0.11233141668489903, test loss:0.5934629158844691\n",
      "train loss:0.11222782467637651, test loss:0.5931199852407827\n",
      "train loss:0.11212431250545482, test loss:0.5927793693065292\n",
      "train loss:0.11202110396440511, test loss:0.5924376881747061\n",
      "train loss:0.11191801041222948, test loss:0.5920948991933518\n",
      "train loss:0.11181517393880556, test loss:0.5917555019248503\n",
      "train loss:0.11171243039156441, test loss:0.5914173023460593\n",
      "train loss:0.11160989188498416, test loss:0.591081753928055\n",
      "train loss:0.11150755003606207, test loss:0.590749724305526\n",
      "train loss:0.11130330987855169, test loss:0.5900908301793022\n",
      "train loss:0.11120156384947903, test loss:0.5897620974741953\n",
      "train loss:0.11109986251591868, test loss:0.5894367842147364\n",
      "train loss:0.11099845689874763, test loss:0.589114297829829\n",
      "train loss:0.110897184997733, test loss:0.588794531372935\n",
      "train loss:0.11079608232542025, test loss:0.5884718619379703\n",
      "train loss:0.11069518085988575, test loss:0.5881536447423695\n",
      "train loss:0.11059449107207707, test loss:0.5878352157084493\n",
      "train loss:0.11049388067365655, test loss:0.5875174275240589\n",
      "train loss:0.1102933045789725, test loss:0.5868835102230653\n",
      "train loss:0.11019322679279034, test loss:0.5865674144615619\n",
      "train loss:0.11009335163092057, test loss:0.586249599640707\n",
      "train loss:0.10999363433905451, test loss:0.5859301738058877\n",
      "train loss:0.1098941066367407, test loss:0.5856130231330635\n",
      "train loss:0.10979468319115604, test loss:0.5852960003420804\n",
      "train loss:0.10969552101983282, test loss:0.584978666224498\n",
      "train loss:0.10959648710998693, test loss:0.5846605480829459\n",
      "train loss:0.10949750566569291, test loss:0.5843423290203542\n",
      "train loss:0.10930020826672414, test loss:0.5837053016394578\n",
      "train loss:0.10920172921509913, test loss:0.5833860042286043\n",
      "train loss:0.10910358499485334, test loss:0.583067520501749\n",
      "train loss:0.10900551474175668, test loss:0.5827479867700908\n",
      "train loss:0.10890752720093054, test loss:0.582429190988903\n",
      "train loss:0.10880995613739733, test loss:0.5821078603616918\n",
      "train loss:0.10871229711489688, test loss:0.5817886632357577\n",
      "train loss:0.10861494059387926, test loss:0.5814714946389178\n",
      "train loss:0.10851762744768045, test loss:0.5811532527307657\n",
      "train loss:0.10832365158334709, test loss:0.5805210833455317\n",
      "train loss:0.10822688135404768, test loss:0.5802071856624527\n",
      "train loss:0.10813033075819209, test loss:0.5798927769966398\n",
      "train loss:0.10803385011221188, test loss:0.5795799870752628\n",
      "train loss:0.10793767501547653, test loss:0.5792662349409273\n",
      "train loss:0.10784150568766991, test loss:0.5789527485861631\n",
      "train loss:0.10774555781559098, test loss:0.5786388678522861\n",
      "train loss:0.10764976009135735, test loss:0.5783289829170681\n",
      "train loss:0.10755418934606432, test loss:0.5780186270379609\n",
      "train loss:0.10736357794031305, test loss:0.5774007173384966\n",
      "train loss:0.10726853555503638, test loss:0.577095086782905\n",
      "train loss:0.10717355932601207, test loss:0.5767912104348237\n",
      "train loss:0.10707869251420474, test loss:0.5764910219003208\n",
      "train loss:0.10698398836083095, test loss:0.5761890416583857\n",
      "train loss:0.10688951719530077, test loss:0.5758906355297033\n",
      "train loss:0.10679526528736992, test loss:0.5755914168614944\n",
      "train loss:0.10670104469105343, test loss:0.5752917095775935\n",
      "train loss:0.10660709344706697, test loss:0.5749922294751963\n",
      "train loss:0.10641952682780777, test loss:0.5743961405117641\n",
      "train loss:0.10632583917609105, test loss:0.5741005648147849\n",
      "train loss:0.10623243548691429, test loss:0.5738060896009111\n",
      "train loss:0.10613908505056349, test loss:0.5735128344373168\n",
      "train loss:0.10604588771428987, test loss:0.5732228997827572\n",
      "train loss:0.10595279913229032, test loss:0.5729350967962691\n",
      "train loss:0.10585992433881673, test loss:0.5726495467925627\n",
      "train loss:0.10576710943083799, test loss:0.5723650471229883\n",
      "train loss:0.10567452955697683, test loss:0.5720804547927789\n",
      "train loss:0.10548960833982626, test loss:0.5715129613083832\n",
      "train loss:0.10539734377452296, test loss:0.5712299716049445\n",
      "train loss:0.10530529482194355, test loss:0.5709464265636139\n",
      "train loss:0.10521323730296843, test loss:0.570661078090194\n",
      "train loss:0.10512153022643687, test loss:0.5703769808345129\n",
      "train loss:0.10502981634933203, test loss:0.5700926241637261\n",
      "train loss:0.1049382121676591, test loss:0.5698071367376881\n",
      "train loss:0.1048467640687452, test loss:0.5695199998096665\n",
      "train loss:0.1047555185532071, test loss:0.5692319533991109\n",
      "train loss:0.10457331942135115, test loss:0.5686465701834097\n",
      "train loss:0.10448249789568738, test loss:0.5683531600941406\n",
      "train loss:0.10439176073012582, test loss:0.5680557172750379\n",
      "train loss:0.10430115988909103, test loss:0.5677548343213784\n",
      "train loss:0.10421058196362262, test loss:0.5674535490949887\n",
      "train loss:0.10412021041241104, test loss:0.5671519409170909\n",
      "train loss:0.10403008349331828, test loss:0.5668499446392814\n",
      "train loss:0.10393995130666275, test loss:0.5665476771551635\n",
      "train loss:0.10385003905315052, test loss:0.5662465757912275\n",
      "train loss:0.10367072207519987, test loss:0.5656503374481442\n",
      "train loss:0.10358119218290937, test loss:0.565356128545607\n",
      "train loss:0.10349193756123773, test loss:0.5650630834590445\n",
      "train loss:0.10340277656782837, test loss:0.5647716854082767\n",
      "train loss:0.10331381150570669, test loss:0.56448248829791\n",
      "train loss:0.10322493813999685, test loss:0.5641943593126626\n",
      "train loss:0.10313617150124477, test loss:0.5639076552266954\n",
      "train loss:0.10304755380758124, test loss:0.563620142272835\n",
      "train loss:0.10295900712024192, test loss:0.5633316815442532\n",
      "train loss:0.10278253569056967, test loss:0.5627537541712323\n",
      "train loss:0.10269438219129232, test loss:0.562465047026818\n",
      "train loss:0.10260645852794709, test loss:0.5621734544603215\n",
      "train loss:0.1025185761084428, test loss:0.5618840731653504\n",
      "train loss:0.10243089711022857, test loss:0.5615919928470603\n",
      "train loss:0.10234328420112077, test loss:0.5612992953813742\n",
      "train loss:0.10225589467617471, test loss:0.5610080865447253\n",
      "train loss:0.10216858733497901, test loss:0.5607182091244144\n",
      "train loss:0.10208139789309575, test loss:0.5604286314734492\n",
      "train loss:0.10190742127326825, test loss:0.5598535304466389\n",
      "train loss:0.10182049535745473, test loss:0.5595678016024268\n",
      "train loss:0.10173378282311693, test loss:0.5592832805693306\n",
      "train loss:0.10164719946713636, test loss:0.5590016303551361\n",
      "train loss:0.10156075315521927, test loss:0.5587196299324557\n",
      "train loss:0.10147442093014747, test loss:0.5584405235320139\n",
      "train loss:0.10138809229992127, test loss:0.5581636990636615\n",
      "train loss:0.10130206527067885, test loss:0.5578889845760673\n",
      "train loss:0.10121611322005704, test loss:0.5576133947320586\n",
      "train loss:0.10104460650384213, test loss:0.5570642039642696\n",
      "train loss:0.10095902406687077, test loss:0.5567892824119536\n",
      "train loss:0.10087347219588194, test loss:0.5565148270363162\n",
      "train loss:0.10078813461433728, test loss:0.5562392402487658\n",
      "train loss:0.10070292195068285, test loss:0.5559657905007969\n",
      "train loss:0.10061781959719794, test loss:0.5556925515216761\n",
      "train loss:0.10053279795172043, test loss:0.5554213845066971\n",
      "train loss:0.10044787893911529, test loss:0.5551523882337989\n",
      "train loss:0.10036307168126955, test loss:0.5548809193345314\n",
      "train loss:0.1001939302846708, test loss:0.5543400104026075\n",
      "train loss:0.10010941667104607, test loss:0.5540674951105587\n",
      "train loss:0.10002521429567614, test loss:0.5537955595511743\n",
      "train loss:0.09994105600788107, test loss:0.5535260774959799\n",
      "train loss:0.09985701799272896, test loss:0.5532555768466999\n",
      "train loss:0.09977306344341044, test loss:0.552986599058968\n",
      "train loss:0.09968920989415966, test loss:0.5527169504074816\n",
      "train loss:0.09960556489782173, test loss:0.5524481333683868\n",
      "train loss:0.09952194898736014, test loss:0.5521770285221926\n",
      "train loss:0.0993551915998912, test loss:0.5516355150535818\n",
      "train loss:0.09927187004724483, test loss:0.551364968028381\n",
      "train loss:0.09918869550790639, test loss:0.5510928426054923\n",
      "train loss:0.0991055845804598, test loss:0.550820831735812\n",
      "train loss:0.09902265568766622, test loss:0.5505500929099743\n",
      "train loss:0.0989397759699417, test loss:0.5502803128559519\n",
      "train loss:0.09885704282810522, test loss:0.5500133919655024\n",
      "train loss:0.09877431488297535, test loss:0.5497436888176257\n",
      "train loss:0.09869180516393744, test loss:0.5494772752272684\n",
      "train loss:0.09852706470695864, test loss:0.5489420222246252\n",
      "train loss:0.09844478223653466, test loss:0.5486742856676609\n",
      "train loss:0.09836276455264965, test loss:0.548408600407167\n",
      "train loss:0.0982808617306752, test loss:0.548143818012422\n",
      "train loss:0.0981989890239938, test loss:0.5478807980666177\n",
      "train loss:0.09811716317504687, test loss:0.5476190112391953\n",
      "train loss:0.0980357356976876, test loss:0.5473585471221993\n",
      "train loss:0.09795426155095538, test loss:0.5470974394304587\n",
      "train loss:0.0978729087921565, test loss:0.5468418328860801\n",
      "train loss:0.0977106416065429, test loss:0.5463320952370414\n",
      "train loss:0.09762964192522468, test loss:0.5460802480279673\n",
      "train loss:0.09754878461247764, test loss:0.5458299909806492\n",
      "train loss:0.09746797343644242, test loss:0.5455817452164108\n",
      "train loss:0.09738733970578538, test loss:0.5453346636986488\n",
      "train loss:0.09730670201022852, test loss:0.5450894307180831\n",
      "train loss:0.09722615820551836, test loss:0.5448455095777535\n",
      "train loss:0.09714585141850714, test loss:0.5445994961320321\n",
      "train loss:0.09706551543701891, test loss:0.5443548216012896\n",
      "train loss:0.09690517386818083, test loss:0.5438656578287806\n",
      "train loss:0.09682513369476148, test loss:0.543622751351303\n",
      "train loss:0.09674521084400138, test loss:0.543379805310405\n",
      "train loss:0.09666533058423661, test loss:0.5431400515957822\n",
      "train loss:0.09658559822645031, test loss:0.5428997284753034\n",
      "train loss:0.09650585804087079, test loss:0.5426611265175569\n",
      "train loss:0.0964263818888717, test loss:0.5424248677217152\n",
      "train loss:0.09634695347487618, test loss:0.5421868327043126\n",
      "train loss:0.09626758646981254, test loss:0.5419495129955674\n",
      "train loss:0.09610928762600446, test loss:0.5414777218359227\n",
      "train loss:0.0960302655378088, test loss:0.5412407419419859\n",
      "train loss:0.0959514044003524, test loss:0.5410025281985105\n",
      "train loss:0.09587256728530728, test loss:0.5407632636969276\n",
      "train loss:0.09579391557610982, test loss:0.5405219598252723\n",
      "train loss:0.09571526751388225, test loss:0.5402822118671854\n",
      "train loss:0.09563678175100093, test loss:0.540039009019315\n",
      "train loss:0.09555838509431397, test loss:0.5397958950733455\n",
      "train loss:0.0954800830081496, test loss:0.5395518692068569\n",
      "train loss:0.09532378202503493, test loss:0.5390621069775761\n",
      "train loss:0.09524580061240939, test loss:0.5388161935243605\n",
      "train loss:0.09516792464149453, test loss:0.5385708803660426\n",
      "train loss:0.09509015954370406, test loss:0.5383257373354368\n",
      "train loss:0.09501251478836442, test loss:0.5380793208285735\n",
      "train loss:0.09493499140129152, test loss:0.5378328285908021\n",
      "train loss:0.09485753482305524, test loss:0.5375846669742823\n",
      "train loss:0.0947801888101224, test loss:0.5373361219212452\n",
      "train loss:0.09470291885117431, test loss:0.5370848226716951\n",
      "train loss:0.09454872494250834, test loss:0.5365828946472732\n",
      "train loss:0.09447173420423778, test loss:0.5363295787970439\n",
      "train loss:0.09439487841040323, test loss:0.5360780204889751\n",
      "train loss:0.09431819596370349, test loss:0.535826897335262\n",
      "train loss:0.09424151135062978, test loss:0.5355753654188535\n",
      "train loss:0.09416507430397283, test loss:0.5353250336830206\n",
      "train loss:0.09408864772798885, test loss:0.5350767837856205\n",
      "train loss:0.0940123827086366, test loss:0.5348292612120048\n",
      "train loss:0.09393629138781805, test loss:0.5345818272332628\n",
      "train loss:0.09378436216737046, test loss:0.5340853591206268\n",
      "train loss:0.0937085324536623, test loss:0.5338352625801336\n",
      "train loss:0.09363286366002326, test loss:0.5335823559051392\n",
      "train loss:0.09355729989688982, test loss:0.5333309745799246\n",
      "train loss:0.09348183446845425, test loss:0.5330783422174991\n",
      "train loss:0.09340640048911891, test loss:0.5328255034956757\n",
      "train loss:0.09333117754666378, test loss:0.5325736399141544\n",
      "train loss:0.09325594029326192, test loss:0.5323244207065244\n",
      "train loss:0.09318081801077177, test loss:0.5320769391192266\n",
      "train loss:0.09303095159607343, test loss:0.5315874790057907\n",
      "train loss:0.0929561727028562, test loss:0.5313456012011157\n",
      "train loss:0.09288158595503361, test loss:0.5311033255046868\n",
      "train loss:0.09280706604708966, test loss:0.5308624941063927\n",
      "train loss:0.09273261580032931, test loss:0.5306210433514015\n",
      "train loss:0.09265821691164221, test loss:0.5303815687263207\n",
      "train loss:0.09258401252533591, test loss:0.5301410203863243\n",
      "train loss:0.09250988540720956, test loss:0.5299003031625126\n",
      "train loss:0.09243586120109244, test loss:0.5296613249998952\n",
      "train loss:0.09228808468649917, test loss:0.5291823338801589\n",
      "train loss:0.09221430188652131, test loss:0.5289443560043712\n",
      "train loss:0.09214067292237114, test loss:0.5287058772887682\n",
      "train loss:0.09206711992619684, test loss:0.5284675648482843\n",
      "train loss:0.09199357347702683, test loss:0.5282272553005949\n",
      "train loss:0.09192028006937805, test loss:0.5279878739789943\n",
      "train loss:0.09184693014382246, test loss:0.5277485609702123\n",
      "train loss:0.09177373483643063, test loss:0.5275087108313995\n",
      "train loss:0.0917006829180914, test loss:0.5272679525503425\n",
      "train loss:0.09155483195051523, test loss:0.5267846237068146\n",
      "train loss:0.09148191514120495, test loss:0.5265436945314563\n",
      "train loss:0.0914091686283999, test loss:0.5263024389592842\n",
      "train loss:0.09133648539000899, test loss:0.5260626337647943\n",
      "train loss:0.09126399076479912, test loss:0.5258186522250677\n",
      "train loss:0.09119153200752868, test loss:0.5255769650947529\n",
      "train loss:0.09111908841051423, test loss:0.5253342148387761\n",
      "train loss:0.09104679199240037, test loss:0.5250909746062941\n",
      "train loss:0.09097462161450813, test loss:0.5248500903637032\n",
      "train loss:0.09083051647134001, test loss:0.5243637239791238\n",
      "train loss:0.09075865383281019, test loss:0.5241208822525515\n",
      "train loss:0.09068678045304188, test loss:0.5238761759560726\n",
      "train loss:0.09061500808339891, test loss:0.5236288059953446\n",
      "train loss:0.09054343886087227, test loss:0.5233791560530293\n",
      "train loss:0.09047190205626375, test loss:0.5231272280402457\n",
      "train loss:0.0904005331898218, test loss:0.5228754456245067\n",
      "train loss:0.09032914940443697, test loss:0.5226220512501029\n",
      "train loss:0.09025787549598255, test loss:0.5223669706993492\n",
      "train loss:0.09011567505968116, test loss:0.5218580990729648\n",
      "train loss:0.09004480270972574, test loss:0.5216000226869999\n",
      "train loss:0.08997381225056357, test loss:0.5213430770437495\n",
      "train loss:0.08990317883553835, test loss:0.5210848192548201\n",
      "train loss:0.08983254560372536, test loss:0.5208244748502152\n",
      "train loss:0.08976191317429898, test loss:0.5205659889036515\n",
      "train loss:0.08969150350859095, test loss:0.5203051868546482\n",
      "train loss:0.08962110797545675, test loss:0.5200461611445862\n",
      "train loss:0.0895508164418977, test loss:0.5197869577648704\n",
      "train loss:0.08941038835010084, test loss:0.5192689188747368\n",
      "train loss:0.08934045253838717, test loss:0.5190108777419039\n",
      "train loss:0.08927039439888287, test loss:0.5187546290751023\n",
      "train loss:0.08920050651280116, test loss:0.5184987943651278\n",
      "train loss:0.08913076540597561, test loss:0.5182453434367621\n",
      "train loss:0.08906105682506826, test loss:0.5179942153620188\n",
      "train loss:0.08899147682190545, test loss:0.5177453791706749\n",
      "train loss:0.08892192412645841, test loss:0.5174980057309388\n",
      "train loss:0.08885249936255235, test loss:0.51725312435505\n",
      "train loss:0.0887139678281311, test loss:0.5167683407712599\n",
      "train loss:0.08864478129460397, test loss:0.5165288639657003\n",
      "train loss:0.08857575435941194, test loss:0.5162911014012834\n",
      "train loss:0.08850686228537404, test loss:0.5160555484340854\n",
      "train loss:0.0884379833578985, test loss:0.5158202317971212\n",
      "train loss:0.08836914831593132, test loss:0.5155882048439411\n",
      "train loss:0.08830051527826052, test loss:0.5153555628126223\n",
      "train loss:0.08823190812077286, test loss:0.515125198071192\n",
      "train loss:0.08816340412509366, test loss:0.5148961441124342\n",
      "train loss:0.088026543278484, test loss:0.5144396799441004\n",
      "train loss:0.08795823124819399, test loss:0.5142107991557104\n",
      "train loss:0.08789015927458264, test loss:0.5139828230521856\n",
      "train loss:0.0878219743893395, test loss:0.5137551351599717\n",
      "train loss:0.0877540420401887, test loss:0.51352657316817\n",
      "train loss:0.08768605218090472, test loss:0.5132985729701686\n",
      "train loss:0.08761821200455676, test loss:0.5130706409362688\n",
      "train loss:0.08755051049056008, test loss:0.5128428351533934\n",
      "train loss:0.08748278572058049, test loss:0.5126139025072602\n",
      "train loss:0.08734769001376576, test loss:0.5121590660615659\n",
      "train loss:0.08728025337028929, test loss:0.51193200558437\n",
      "train loss:0.08721286415656984, test loss:0.5117037966065741\n",
      "train loss:0.08714556824740564, test loss:0.5114766883357309\n",
      "train loss:0.08707835148018549, test loss:0.5112506979249986\n",
      "train loss:0.0870112300546871, test loss:0.5110243946953307\n",
      "train loss:0.08694412224546866, test loss:0.5107977063490778\n",
      "train loss:0.08687716545869345, test loss:0.51057044513331\n",
      "train loss:0.08681024731593755, test loss:0.5103426054088782\n",
      "train loss:0.08667667561123314, test loss:0.509883276432199\n",
      "train loss:0.08661007279070743, test loss:0.5096529590774357\n",
      "train loss:0.08654349921565939, test loss:0.5094213849485519\n",
      "train loss:0.08647699359543783, test loss:0.5091897171907749\n",
      "train loss:0.08641064999032762, test loss:0.508958619606148\n",
      "train loss:0.08634438838165485, test loss:0.5087292993260385\n",
      "train loss:0.08627828162549982, test loss:0.5085001682065545\n",
      "train loss:0.08621206712201027, test loss:0.5082720900544853\n",
      "train loss:0.08614610291754185, test loss:0.5080449732496815\n",
      "train loss:0.08601435240470105, test loss:0.5075883431743309\n",
      "train loss:0.08594862494244011, test loss:0.5073620045257065\n",
      "train loss:0.08588305057541189, test loss:0.5071361896376866\n",
      "train loss:0.08581738813082951, test loss:0.5069091396154332\n",
      "train loss:0.08575194524039813, test loss:0.5066837502970789\n",
      "train loss:0.08568655681161622, test loss:0.5064564679628237\n",
      "train loss:0.08562121940233527, test loss:0.5062308253356022\n",
      "train loss:0.08555596379582682, test loss:0.5060033488936846\n",
      "train loss:0.08549082689490965, test loss:0.5057756082577018\n",
      "train loss:0.08536081509666042, test loss:0.5053202567232516\n",
      "train loss:0.0852958672239932, test loss:0.505090594634835\n",
      "train loss:0.08523102246193073, test loss:0.5048607970433995\n",
      "train loss:0.08516635648244095, test loss:0.504629277681166\n",
      "train loss:0.08510170482780263, test loss:0.5043961817203698\n",
      "train loss:0.08503702712023847, test loss:0.5041637329716934\n",
      "train loss:0.0849725409586731, test loss:0.5039323801141689\n",
      "train loss:0.08490812644274622, test loss:0.5037005504833236\n",
      "train loss:0.08484381135684119, test loss:0.5034690131924027\n",
      "train loss:0.0847153734309549, test loss:0.5030069872467927\n",
      "train loss:0.0846512653937383, test loss:0.5027769150368985\n",
      "train loss:0.08458724485819, test loss:0.5025476788255095\n",
      "train loss:0.08452331945691813, test loss:0.502318941936077\n",
      "train loss:0.084459510468234, test loss:0.502091328739757\n",
      "train loss:0.0843957416125111, test loss:0.5018637620158027\n",
      "train loss:0.08433209734016113, test loss:0.5016369855997294\n",
      "train loss:0.08426844762329395, test loss:0.5014093203814091\n",
      "train loss:0.08420494359363684, test loss:0.5011843183485392\n",
      "train loss:0.0840781419196935, test loss:0.5007322924590406\n",
      "train loss:0.0840148905353769, test loss:0.500509644068524\n",
      "train loss:0.08395167316849148, test loss:0.5002864980634635\n",
      "train loss:0.08388864144187014, test loss:0.5000637700419416\n",
      "train loss:0.08382555750822669, test loss:0.4998441701807765\n",
      "train loss:0.08376259396366967, test loss:0.49962476957306823\n",
      "train loss:0.08369973235634923, test loss:0.4994082112929093\n",
      "train loss:0.08363698299589814, test loss:0.49919292614833843\n",
      "train loss:0.08357424802244945, test loss:0.49897644880453323\n",
      "train loss:0.08344908197730343, test loss:0.4985493917974394\n",
      "train loss:0.08338662349742702, test loss:0.4983343810035794\n",
      "train loss:0.08332422896912578, test loss:0.49812133560869504\n",
      "train loss:0.08326189509933772, test loss:0.49790701764159634\n",
      "train loss:0.08319961127434614, test loss:0.49769297310769456\n",
      "train loss:0.08313753885340817, test loss:0.49747757104524026\n",
      "train loss:0.08307537580619023, test loss:0.49726206013383817\n",
      "train loss:0.08301332780027151, test loss:0.4970468730463065\n",
      "train loss:0.08295143490066956, test loss:0.49683052477540357\n",
      "train loss:0.08282788502563666, test loss:0.4963962052822758\n",
      "train loss:0.08276621797672809, test loss:0.4961785263331114\n",
      "train loss:0.08270464114416808, test loss:0.49596051614356673\n",
      "train loss:0.0826431656035869, test loss:0.4957419737392099\n",
      "train loss:0.08258166769174256, test loss:0.49552435729255906\n",
      "train loss:0.0825203604231264, test loss:0.49530578964985256\n",
      "train loss:0.08245910713495105, test loss:0.49508460591322156\n",
      "train loss:0.08239789010744032, test loss:0.49486471269528076\n",
      "train loss:0.08233677074216264, test loss:0.494643363931797\n",
      "train loss:0.0822147992064931, test loss:0.494202370650513\n",
      "train loss:0.0821538540591848, test loss:0.4939813282512859\n",
      "train loss:0.0820929489642394, test loss:0.4937613265688157\n",
      "train loss:0.0820321984826345, test loss:0.493540464036455\n",
      "train loss:0.08197152561743891, test loss:0.4933211014012596\n",
      "train loss:0.0819108151112734, test loss:0.493102145312986\n",
      "train loss:0.0818502501419458, test loss:0.49288326153873246\n",
      "train loss:0.08178978247791889, test loss:0.49266466935946374\n",
      "train loss:0.08172935536967352, test loss:0.492447490241809\n",
      "train loss:0.08160882609235583, test loss:0.49201289170248363\n",
      "train loss:0.0815486245579141, test loss:0.4917966236502593\n",
      "train loss:0.08148855843463856, test loss:0.49158166421815225\n",
      "train loss:0.08142846952986911, test loss:0.491367938120244\n",
      "train loss:0.08136851436511792, test loss:0.4911538504778171\n",
      "train loss:0.08130859687892844, test loss:0.49094228992306566\n",
      "train loss:0.0812488303034516, test loss:0.4907297730707558\n",
      "train loss:0.08118908276080923, test loss:0.49051971111830794\n",
      "train loss:0.08112941323341219, test loss:0.49030788755343846\n",
      "train loss:0.08101024930396215, test loss:0.48988773629805643\n",
      "train loss:0.08095074799148937, test loss:0.48967693659701345\n",
      "train loss:0.08089137292799435, test loss:0.489467511352612\n",
      "train loss:0.08083204354533995, test loss:0.48925806412211875\n",
      "train loss:0.08077274893184128, test loss:0.48904769277542975\n",
      "train loss:0.08071360375707909, test loss:0.48883770108602986\n",
      "train loss:0.08065446387796091, test loss:0.48862724446273553\n",
      "train loss:0.08059540055787544, test loss:0.48841737581075173\n",
      "train loss:0.08053642893026766, test loss:0.48820803320058004\n",
      "train loss:0.08041860960739503, test loss:0.48778561737213927\n",
      "train loss:0.0803598263603662, test loss:0.487574470502386\n",
      "train loss:0.08030113478535472, test loss:0.48736361541048046\n",
      "train loss:0.0802424868776332, test loss:0.48715196569608554\n",
      "train loss:0.08018392650897425, test loss:0.4869389007136648\n",
      "train loss:0.0801254554075795, test loss:0.48672906102280694\n",
      "train loss:0.08006708151215475, test loss:0.4865167745372642\n",
      "train loss:0.08000874113378595, test loss:0.486305877668136\n",
      "train loss:0.07995049502483842, test loss:0.48609377004098464\n",
      "train loss:0.07983423580773567, test loss:0.48567385135596636\n",
      "train loss:0.07977616923054359, test loss:0.48546530866366344\n",
      "train loss:0.07971821682049578, test loss:0.48525660276931615\n",
      "train loss:0.079660340812332, test loss:0.4850507139507869\n",
      "train loss:0.07960248161699972, test loss:0.4848441442423237\n",
      "train loss:0.07954472038296276, test loss:0.4846383602873352\n",
      "train loss:0.07948706442938967, test loss:0.4844328781293977\n",
      "train loss:0.079429489777991, test loss:0.48423042339521893\n",
      "train loss:0.07937196851098108, test loss:0.48402675449356924\n",
      "train loss:0.07925720073093287, test loss:0.4836211054385733\n",
      "train loss:0.07919990059611345, test loss:0.48342015074196554\n",
      "train loss:0.07914272634103021, test loss:0.4832186896027193\n",
      "train loss:0.07908555021539908, test loss:0.48301765547351\n",
      "train loss:0.07902845602245923, test loss:0.48281801131890595\n",
      "train loss:0.07897133504539074, test loss:0.4826174207035224\n",
      "train loss:0.0789143805815775, test loss:0.4824178126224155\n",
      "train loss:0.07885742486935342, test loss:0.48221848000722517\n",
      "train loss:0.07880055589951689, test loss:0.4820188182026257\n",
      "train loss:0.07868701271446542, test loss:0.48162012875470317\n",
      "train loss:0.07863034902240378, test loss:0.48142089084883716\n",
      "train loss:0.07857375035598789, test loss:0.4812228256395649\n",
      "train loss:0.07851722184746192, test loss:0.48102817622346233\n",
      "train loss:0.07846075412293775, test loss:0.4808337458696148\n",
      "train loss:0.0784043666478622, test loss:0.4806413973119545\n",
      "train loss:0.07834805267853984, test loss:0.4804500107535321\n",
      "train loss:0.07829178502250134, test loss:0.48026050871428344\n",
      "train loss:0.07823568192055375, test loss:0.48007214978128004\n",
      "train loss:0.07812350955109659, test loss:0.4796938202119041\n",
      "train loss:0.07806752371546398, test loss:0.47950647829926585\n",
      "train loss:0.07801165344908673, test loss:0.4793196155499454\n",
      "train loss:0.07795587497966022, test loss:0.4791311518527788\n",
      "train loss:0.07790013212939927, test loss:0.47894365457107635\n",
      "train loss:0.07784446203680614, test loss:0.4787581732323549\n",
      "train loss:0.07778881473658093, test loss:0.47857164680675723\n",
      "train loss:0.07773318305198072, test loss:0.47838389477479787\n",
      "train loss:0.07767775395318252, test loss:0.47819990339818047\n",
      "train loss:0.0775669633468798, test loss:0.4778267110710823\n",
      "train loss:0.077511681634838, test loss:0.4776415298936846\n",
      "train loss:0.07745640045497681, test loss:0.47745505328719484\n",
      "train loss:0.07740124705547513, test loss:0.4772698228480473\n",
      "train loss:0.07734617190796149, test loss:0.477083394538662\n",
      "train loss:0.0772910791969374, test loss:0.47689793197978675\n",
      "train loss:0.07723607978002556, test loss:0.4767132111114878\n",
      "train loss:0.07718122711523946, test loss:0.47652921741381266\n",
      "train loss:0.07712633460816819, test loss:0.4763438684567834\n",
      "train loss:0.07701676112591214, test loss:0.475974432595438\n",
      "train loss:0.07696201734927409, test loss:0.4757886354176395\n",
      "train loss:0.07690748168812502, test loss:0.4756041716636202\n",
      "train loss:0.0768528325930553, test loss:0.47541918970514807\n",
      "train loss:0.0767983259960734, test loss:0.4752347261705653\n",
      "train loss:0.07674399136537893, test loss:0.4750522012001244\n",
      "train loss:0.07668953287064502, test loss:0.4748691838740186\n",
      "train loss:0.07663522004960104, test loss:0.4746870585056\n",
      "train loss:0.07658097770574972, test loss:0.4745059914690169\n",
      "train loss:0.07647256513578858, test loss:0.4741481093019384\n",
      "train loss:0.07641842645141975, test loss:0.47396947206432555\n",
      "train loss:0.07636444302842571, test loss:0.47379327673007415\n",
      "train loss:0.07631041881551805, test loss:0.47361552794787454\n",
      "train loss:0.07625653875641301, test loss:0.47343980345402986\n",
      "train loss:0.07620263774354133, test loss:0.4732642937602046\n",
      "train loss:0.07614894398749512, test loss:0.47308945262146074\n",
      "train loss:0.07609515966712384, test loss:0.47291494928760824\n",
      "train loss:0.07604152019572262, test loss:0.4727391532063604\n",
      "train loss:0.07593436774453796, test loss:0.4723875618984677\n",
      "train loss:0.07588087687302948, test loss:0.4722098157391297\n",
      "train loss:0.07582750306147286, test loss:0.4720316031655934\n",
      "train loss:0.07577408931982568, test loss:0.47185400038245195\n",
      "train loss:0.07572084441452986, test loss:0.4716755771537302\n",
      "train loss:0.07566755492850621, test loss:0.4714986032111094\n",
      "train loss:0.07561431937668035, test loss:0.4713207076942288\n",
      "train loss:0.07556125765673476, test loss:0.4711423829731955\n",
      "train loss:0.07550814073979645, test loss:0.47096386668492995\n",
      "train loss:0.07540214614205157, test loss:0.47060421714925577\n",
      "train loss:0.07534918635646254, test loss:0.4704223417617794\n",
      "train loss:0.07529630307500504, test loss:0.4702424687146702\n",
      "train loss:0.07524347611144235, test loss:0.47006224027992805\n",
      "train loss:0.07519078615981631, test loss:0.4698827834511515\n",
      "train loss:0.07513807047782788, test loss:0.46970238536327447\n",
      "train loss:0.07508540900104513, test loss:0.46952250438981324\n",
      "train loss:0.07503289044533394, test loss:0.46934144590433374\n",
      "train loss:0.07498036691114572, test loss:0.46916121239173725\n",
      "train loss:0.07487554213484435, test loss:0.4687994312874768\n",
      "train loss:0.07482317233321716, test loss:0.4686197013418089\n",
      "train loss:0.07477087833174045, test loss:0.4684394014154607\n",
      "train loss:0.0747186357378064, test loss:0.46825955280673387\n",
      "train loss:0.07466643531979604, test loss:0.4680806221161899\n",
      "train loss:0.07461433425374762, test loss:0.4679025845263391\n",
      "train loss:0.07456228602015572, test loss:0.46772460355646733\n",
      "train loss:0.07451024719145669, test loss:0.46754628422304867\n",
      "train loss:0.0744582748928973, test loss:0.467369244472213\n",
      "train loss:0.07435458124607393, test loss:0.4670127587664502\n",
      "train loss:0.07430283626962919, test loss:0.46683400275949855\n",
      "train loss:0.07425116759988763, test loss:0.4666557400134567\n",
      "train loss:0.07419947301828812, test loss:0.4664765758205476\n",
      "train loss:0.07414791715068658, test loss:0.4662969625020832\n",
      "train loss:0.0740963896997833, test loss:0.46611749643067174\n",
      "train loss:0.07404496432247709, test loss:0.4659382198601566\n",
      "train loss:0.07399350335478252, test loss:0.46575841030425263\n",
      "train loss:0.07394220800441112, test loss:0.46557771594584285\n",
      "train loss:0.07383964147949526, test loss:0.46521972999393735\n",
      "train loss:0.07378849867749332, test loss:0.4650403260004436\n",
      "train loss:0.0737374232636112, test loss:0.4648618770312122\n",
      "train loss:0.07368631583593259, test loss:0.4646817709261199\n",
      "train loss:0.07363529587470172, test loss:0.46450345668949344\n",
      "train loss:0.07358433044029952, test loss:0.4643240972749425\n",
      "train loss:0.07353349310016492, test loss:0.4641453630086228\n",
      "train loss:0.07348263813034218, test loss:0.46396727367428997\n",
      "train loss:0.07343191806657372, test loss:0.46378984089325126\n",
      "train loss:0.07333049746644738, test loss:0.46343668781873615\n",
      "train loss:0.07327990719054278, test loss:0.46326170759617846\n",
      "train loss:0.07322932218479183, test loss:0.463085084967527\n",
      "train loss:0.07317890994005044, test loss:0.4629112236691581\n",
      "train loss:0.07312847645473695, test loss:0.46273880232915704\n",
      "train loss:0.0730780888373039, test loss:0.4625665106809273\n",
      "train loss:0.07302780532407717, test loss:0.46239525692981637\n",
      "train loss:0.07297744452965169, test loss:0.4622263390057385\n",
      "train loss:0.07292729283092811, test loss:0.4620581793599168\n",
      "train loss:0.07282697217945029, test loss:0.46172324898199035\n",
      "train loss:0.07277691931420455, test loss:0.4615550130386761\n",
      "train loss:0.07272688766364234, test loss:0.4613879966443992\n",
      "train loss:0.07267690066007734, test loss:0.46122016928386583\n",
      "train loss:0.07262703257292993, test loss:0.4610521296559902\n",
      "train loss:0.07257717213065323, test loss:0.4608865809826056\n",
      "train loss:0.07252736844691084, test loss:0.4607191553781056\n",
      "train loss:0.0724776716011214, test loss:0.4605541691058938\n",
      "train loss:0.07242787068723965, test loss:0.4603872512420659\n",
      "train loss:0.07232869313115586, test loss:0.4600544577638019\n",
      "train loss:0.07227919950119244, test loss:0.45988813254739447\n",
      "train loss:0.07222976232795739, test loss:0.4597202898910345\n",
      "train loss:0.07218036015795519, test loss:0.4595547654208006\n",
      "train loss:0.07213101642208625, test loss:0.45938959941333113\n",
      "train loss:0.07208165015151395, test loss:0.45922594553323504\n",
      "train loss:0.07203243845065793, test loss:0.4590619939984203\n",
      "train loss:0.07198325018715605, test loss:0.4588983363546283\n",
      "train loss:0.07193407863756011, test loss:0.45873580128130953\n",
      "train loss:0.07183594951867431, test loss:0.45840632773704626\n",
      "train loss:0.07178693877024013, test loss:0.4582422648317417\n",
      "train loss:0.0717380300493667, test loss:0.4580758982974115\n",
      "train loss:0.07168911577572606, test loss:0.45790930789298895\n",
      "train loss:0.07164030879861272, test loss:0.4577423821477955\n",
      "train loss:0.07159151669639845, test loss:0.457574747545851\n",
      "train loss:0.07154275697832983, test loss:0.4574066364940233\n",
      "train loss:0.07149405288638641, test loss:0.4572383551805497\n",
      "train loss:0.07144540061359937, test loss:0.4570706086333415\n",
      "train loss:0.07134822982058589, test loss:0.45673781611975417\n",
      "train loss:0.07129974005174299, test loss:0.45657350764317617\n",
      "train loss:0.0712512683830758, test loss:0.45641005561635506\n",
      "train loss:0.0712028794265633, test loss:0.4562480270127457\n",
      "train loss:0.0711545556785572, test loss:0.4560856532720592\n",
      "train loss:0.07110630484010123, test loss:0.45592329845222956\n",
      "train loss:0.07105803374482839, test loss:0.4557623458761465\n",
      "train loss:0.07100985603235914, test loss:0.4555999652817294\n",
      "train loss:0.07096165100712615, test loss:0.4554375846879159\n",
      "train loss:0.07086557762931216, test loss:0.45511607853049685\n",
      "train loss:0.0708176109734787, test loss:0.45495561920065775\n",
      "train loss:0.07076962329580225, test loss:0.4547966461793156\n",
      "train loss:0.07072171860503366, test loss:0.45463620521156395\n",
      "train loss:0.07067393860837717, test loss:0.4544776728498444\n",
      "train loss:0.07062619443232832, test loss:0.45431813350754036\n",
      "train loss:0.07057848526501012, test loss:0.4541595855479822\n",
      "train loss:0.07053078135287122, test loss:0.45400114527091345\n",
      "train loss:0.07048313124290201, test loss:0.4538427416535158\n",
      "train loss:0.0703880279643115, test loss:0.4535261923998344\n",
      "train loss:0.07034058032348837, test loss:0.45336812221924994\n",
      "train loss:0.0702931190073204, test loss:0.4532082453121966\n",
      "train loss:0.07024574764621257, test loss:0.4530498962387129\n",
      "train loss:0.07019849423660765, test loss:0.45289175940286724\n",
      "train loss:0.07015117475509772, test loss:0.452731292390791\n",
      "train loss:0.0701039505932082, test loss:0.4525716216490158\n",
      "train loss:0.07005675672393526, test loss:0.452412878075211\n",
      "train loss:0.07000968501636902, test loss:0.4522537623000106\n",
      "train loss:0.06991562315652625, test loss:0.45193668220637806\n",
      "train loss:0.0698687042908747, test loss:0.45177868765184115\n",
      "train loss:0.0698217817880067, test loss:0.45162131420955864\n",
      "train loss:0.06977495204015463, test loss:0.45146248861576954\n",
      "train loss:0.06972815766116326, test loss:0.451304211171077\n",
      "train loss:0.06968138064807754, test loss:0.45114602967348455\n",
      "train loss:0.06963464305224898, test loss:0.45098714946291707\n",
      "train loss:0.06958797622380337, test loss:0.45082856556958556\n",
      "train loss:0.06954128887895376, test loss:0.4506682034688157\n",
      "train loss:0.0694482728317349, test loss:0.45034984423076685\n",
      "train loss:0.069401823368114, test loss:0.45019155363316077\n",
      "train loss:0.06935539279748673, test loss:0.4500329281825006\n",
      "train loss:0.06930896701079076, test loss:0.4498754212973049\n",
      "train loss:0.06926259779033708, test loss:0.44971760711858033\n",
      "train loss:0.06921627810469813, test loss:0.44956146182144224\n",
      "train loss:0.06917006682371556, test loss:0.4494039996568416\n",
      "train loss:0.06912381841689194, test loss:0.44924566568775814\n",
      "train loss:0.06907760126375843, test loss:0.44908858110162775\n",
      "train loss:0.06898537561453331, test loss:0.44877138139613304\n",
      "train loss:0.06893930141949361, test loss:0.44861218705701117\n",
      "train loss:0.0688932833802436, test loss:0.44845205313130104\n",
      "train loss:0.06884731661205559, test loss:0.4482903043328637\n",
      "train loss:0.06880141987218588, test loss:0.44812820229833017\n",
      "train loss:0.06875546108256435, test loss:0.4479669066115889\n",
      "train loss:0.06870963906990088, test loss:0.44780583897658993\n",
      "train loss:0.06866388662623238, test loss:0.4476446506409629\n",
      "train loss:0.06861808120874285, test loss:0.44748430744775086\n",
      "train loss:0.06852670542513985, test loss:0.4471632920028399\n",
      "train loss:0.06848107796084958, test loss:0.44700471737573433\n",
      "train loss:0.06843554272518952, test loss:0.4468458329911897\n",
      "train loss:0.0683900027272182, test loss:0.44668933963038754\n",
      "train loss:0.0683445362120882, test loss:0.4465332082361669\n",
      "train loss:0.06829908988531425, test loss:0.4463768161210817\n",
      "train loss:0.06825369006753595, test loss:0.44621985626330085\n",
      "train loss:0.06820837454382281, test loss:0.44606322609219673\n",
      "train loss:0.06816308663899197, test loss:0.4459066288892306\n",
      "train loss:0.0680726409902773, test loss:0.4455947880512673\n",
      "train loss:0.06802747330210443, test loss:0.4454387565108893\n",
      "train loss:0.06798236975254976, test loss:0.4452825073986013\n",
      "train loss:0.06793729411210338, test loss:0.4451288444176684\n",
      "train loss:0.06789229499070308, test loss:0.4449722444009346\n",
      "train loss:0.06784727404675117, test loss:0.4448172414124199\n",
      "train loss:0.06780231630253593, test loss:0.44466280351655335\n",
      "train loss:0.0677574812711163, test loss:0.44450979814759906\n",
      "train loss:0.06771265962101436, test loss:0.44435559421722054\n",
      "train loss:0.06762298396661762, test loss:0.4440495651846738\n",
      "train loss:0.0675782280554537, test loss:0.4438982624681005\n",
      "train loss:0.067533549956237, test loss:0.4437458299388316\n",
      "train loss:0.0674888644724762, test loss:0.4435936854650416\n",
      "train loss:0.06744428068187353, test loss:0.4434418768119238\n",
      "train loss:0.06739967193881699, test loss:0.44328934878141796\n",
      "train loss:0.06735514966616374, test loss:0.44313721082481317\n",
      "train loss:0.0673106549818846, test loss:0.4429858729211087\n",
      "train loss:0.06726622206568612, test loss:0.44283407687395404\n",
      "train loss:0.06717735596366986, test loss:0.4425315317477997\n",
      "train loss:0.0671330285107016, test loss:0.4423813049679297\n",
      "train loss:0.06708870618440255, test loss:0.4422318493669989\n",
      "train loss:0.06704442580415532, test loss:0.4420834297180781\n",
      "train loss:0.06700024483479719, test loss:0.4419371599669531\n",
      "train loss:0.06695599834300778, test loss:0.4417899493002556\n",
      "train loss:0.06691184381164346, test loss:0.4416447394657963\n",
      "train loss:0.06686772975391188, test loss:0.4414986036563264\n",
      "train loss:0.066823714002543, test loss:0.4413548447601985\n",
      "train loss:0.06673562882426114, test loss:0.4410689059707916\n",
      "train loss:0.06669164129744103, test loss:0.44092584378886657\n",
      "train loss:0.06664769958897586, test loss:0.44078261061285984\n",
      "train loss:0.06660387347863223, test loss:0.44064111478935375\n",
      "train loss:0.06656002982242706, test loss:0.44049853360156\n",
      "train loss:0.06651620824361758, test loss:0.44035576920481334\n",
      "train loss:0.06647247170341107, test loss:0.44021360473918075\n",
      "train loss:0.06642873305374725, test loss:0.4400699911526699\n",
      "train loss:0.06638506059467088, test loss:0.43992713231864367\n",
      "train loss:0.06629791698995124, test loss:0.4396382574832736\n",
      "train loss:0.06625432450877855, test loss:0.4394945395747413\n",
      "train loss:0.06621078874393584, test loss:0.4393507493372455\n",
      "train loss:0.06616734739444341, test loss:0.43920871036936765\n",
      "train loss:0.0661239015150279, test loss:0.43906415686543376\n",
      "train loss:0.06608049660947232, test loss:0.4389198470928485\n",
      "train loss:0.0660371550060665, test loss:0.4387778154472959\n",
      "train loss:0.06599377939728425, test loss:0.43863568219204196\n",
      "train loss:0.06595046093117392, test loss:0.43849450797549694\n",
      "train loss:0.0658639858815267, test loss:0.4382112303126197\n",
      "train loss:0.06582082371001302, test loss:0.43807115402535807\n",
      "train loss:0.06577766605277284, test loss:0.4379317216693523\n",
      "train loss:0.06573455241498008, test loss:0.43779151502923674\n",
      "train loss:0.0656915172839811, test loss:0.43765084978602065\n",
      "train loss:0.06564847776773582, test loss:0.4375101445880605\n",
      "train loss:0.06560553258990752, test loss:0.43736889071591506\n",
      "train loss:0.06556258526817536, test loss:0.43722996428018807\n",
      "train loss:0.06551975398346398, test loss:0.4370899210627404\n",
      "train loss:0.06543408433798005, test loss:0.4368105010491211\n",
      "train loss:0.06539131754738424, test loss:0.4366706702472755\n",
      "train loss:0.06534863261520582, test loss:0.4365311029042455\n",
      "train loss:0.06530598829521393, test loss:0.4363919392817032\n",
      "train loss:0.06526339121468888, test loss:0.4362521105789477\n",
      "train loss:0.06522074353965124, test loss:0.4361112091790147\n",
      "train loss:0.06517825417400733, test loss:0.4359704766234567\n",
      "train loss:0.06513573210212173, test loss:0.4358297770231364\n",
      "train loss:0.06509328489434527, test loss:0.43568861430005695\n",
      "train loss:0.0650085382923138, test loss:0.4354063931927183\n",
      "train loss:0.06496617016339781, test loss:0.43526481140080336\n",
      "train loss:0.0649239286403476, test loss:0.4351239351470414\n",
      "train loss:0.06488171170484049, test loss:0.434980079142113\n",
      "train loss:0.06483947839933754, test loss:0.43483725586972777\n",
      "train loss:0.06479736494017073, test loss:0.4346942997433696\n",
      "train loss:0.06475520636760275, test loss:0.4345504665548437\n",
      "train loss:0.06471308765291234, test loss:0.43440768663251383\n",
      "train loss:0.0646711147208401, test loss:0.43426547484273187\n",
      "train loss:0.06458717755659417, test loss:0.4339798749181936\n",
      "train loss:0.06454523738237555, test loss:0.4338380442801915\n",
      "train loss:0.06450335756012673, test loss:0.43369586307502195\n",
      "train loss:0.06446150028678019, test loss:0.433554607279863\n",
      "train loss:0.06441970473753156, test loss:0.433413424378926\n",
      "train loss:0.06437793651007091, test loss:0.43327269446322475\n",
      "train loss:0.06433620791498795, test loss:0.43313346217237475\n",
      "train loss:0.06429448951171898, test loss:0.4329937143778246\n",
      "train loss:0.06425288380169705, test loss:0.43285635258320154\n",
      "train loss:0.06416972681116542, test loss:0.43258142444120423\n",
      "train loss:0.06412816869176376, test loss:0.4324432510109791\n",
      "train loss:0.0640866785671529, test loss:0.4323055212255011\n",
      "train loss:0.06404519531925988, test loss:0.4321656261187721\n",
      "train loss:0.06400374578515922, test loss:0.43202517667000406\n",
      "train loss:0.06396240392099058, test loss:0.43188326043402736\n",
      "train loss:0.0639210615977925, test loss:0.4317420298753842\n",
      "train loss:0.06387973225994699, test loss:0.43159975322066685\n",
      "train loss:0.06383843241227034, test loss:0.4314576501864714\n",
      "train loss:0.06375604751694561, test loss:0.43117400521630556\n",
      "train loss:0.06371493788430943, test loss:0.4310320171873312\n",
      "train loss:0.0636738454757627, test loss:0.43089130026239003\n",
      "train loss:0.0636328331648505, test loss:0.43075046062285827\n",
      "train loss:0.06359180836525602, test loss:0.4306075965130817\n",
      "train loss:0.0635508311910296, test loss:0.4304650897742523\n",
      "train loss:0.06350993054134763, test loss:0.43032376774286063\n",
      "train loss:0.06346904612643778, test loss:0.43018167650673966\n",
      "train loss:0.0634282157781886, test loss:0.43004095403998105\n",
      "train loss:0.06334663077828447, test loss:0.4297576057388304\n",
      "train loss:0.06330590308370365, test loss:0.4296160274873638\n",
      "train loss:0.06326520138407785, test loss:0.4294719129635944\n",
      "train loss:0.06322457158265458, test loss:0.42932790373112456\n",
      "train loss:0.06318400444351763, test loss:0.4291824895973457\n",
      "train loss:0.06314335745563972, test loss:0.42903865996829743\n",
      "train loss:0.06310286710699042, test loss:0.42889334608719193\n",
      "train loss:0.06306236181537248, test loss:0.4287494936573881\n",
      "train loss:0.06302190425136077, test loss:0.42860543999234824\n",
      "train loss:0.06294116740981859, test loss:0.4283194962126965\n",
      "train loss:0.06290079966822669, test loss:0.4281781172532514\n",
      "train loss:0.06286045447569702, test loss:0.42803680106707676\n",
      "train loss:0.06282025040801001, test loss:0.42789687056690656\n",
      "train loss:0.06278006152444279, test loss:0.4277567634511008\n",
      "train loss:0.06273986878020142, test loss:0.42761623190796044\n",
      "train loss:0.06269981948055409, test loss:0.42747779053297724\n",
      "train loss:0.06265970353075172, test loss:0.4273375957394947\n",
      "train loss:0.06261963593774544, test loss:0.4271975990998066\n",
      "train loss:0.0625396117042677, test loss:0.4269168450856884\n",
      "train loss:0.06249966904461035, test loss:0.4267753510430516\n",
      "train loss:0.06245973408454037, test loss:0.42663339526849886\n",
      "train loss:0.06241988454596128, test loss:0.42649166027982194\n",
      "train loss:0.062380043137717965, test loss:0.4263509685183772\n",
      "train loss:0.06234022853540877, test loss:0.42621014198828805\n",
      "train loss:0.06230044507299858, test loss:0.42607189788211347\n",
      "train loss:0.06226070136838009, test loss:0.42593373436672455\n",
      "train loss:0.062220999607491397, test loss:0.42579637266248055\n",
      "train loss:0.06214176686114931, test loss:0.4255222544593641\n",
      "train loss:0.06210216030001464, test loss:0.4253848790292551\n",
      "train loss:0.062062612299786206, test loss:0.4252496335799206\n",
      "train loss:0.06202308025148146, test loss:0.42511417820854375\n",
      "train loss:0.06198359968184283, test loss:0.42497967027913436\n",
      "train loss:0.06194416016312221, test loss:0.42484550918104375\n",
      "train loss:0.0619047175728199, test loss:0.4247110899983572\n",
      "train loss:0.06186536651991345, test loss:0.42457848615365895\n",
      "train loss:0.06182602966856827, test loss:0.42444662067246247\n",
      "train loss:0.06174746757244559, test loss:0.4241818640072874\n",
      "train loss:0.0617082930636212, test loss:0.4240504850394884\n",
      "train loss:0.06166912988785508, test loss:0.4239190661094948\n",
      "train loss:0.061629975122720254, test loss:0.4237870544263041\n",
      "train loss:0.06159092848486095, test loss:0.4236571961454843\n",
      "train loss:0.06155185916967549, test loss:0.42352646586107073\n",
      "train loss:0.06151289854872736, test loss:0.42339512745409613\n",
      "train loss:0.061473956542493866, test loss:0.4232631386368897\n",
      "train loss:0.06143504953581308, test loss:0.42313051011304276\n",
      "train loss:0.06135733006302186, test loss:0.4228659716380233\n",
      "train loss:0.06131855985983193, test loss:0.42273501972409977\n",
      "train loss:0.061279798366338206, test loss:0.4226040014607452\n",
      "train loss:0.061241089650629285, test loss:0.42247422260034834\n",
      "train loss:0.06120242901797874, test loss:0.42234551786985003\n",
      "train loss:0.061163741779052073, test loss:0.4222144938118465\n",
      "train loss:0.06112514932602957, test loss:0.4220837486620775\n",
      "train loss:0.061086607824579135, test loss:0.42195234941139176\n",
      "train loss:0.06104809260414894, test loss:0.4218204543718394\n",
      "train loss:0.06097112769308941, test loss:0.42155671689396174\n",
      "train loss:0.06093265363135192, test loss:0.42142637099183944\n",
      "train loss:0.06089426326647591, test loss:0.42129657935262926\n",
      "train loss:0.06085594604973971, test loss:0.42116727671756626\n",
      "train loss:0.06081760459529232, test loss:0.42103781023896003\n",
      "train loss:0.06077928421925421, test loss:0.4209087750259842\n",
      "train loss:0.06074107213394861, test loss:0.42078089565997534\n",
      "train loss:0.06070287785223142, test loss:0.4206520262874844\n",
      "train loss:0.06066472162603757, test loss:0.4205243533075232\n",
      "train loss:0.060588463964588904, test loss:0.4202684400258962\n",
      "train loss:0.06055037144130692, test loss:0.42014057313168557\n",
      "train loss:0.06051239030953843, test loss:0.4200124664219148\n",
      "train loss:0.060474365414123425, test loss:0.41988487355260784\n",
      "train loss:0.060436384277284365, test loss:0.4197567956410758\n",
      "train loss:0.06039847598499152, test loss:0.41962774282587956\n",
      "train loss:0.06036060786740059, test loss:0.4194994452849889\n",
      "train loss:0.06032274720476954, test loss:0.41937061521309466\n",
      "train loss:0.060284915350707624, test loss:0.4192426137785733\n",
      "train loss:0.06020942836889573, test loss:0.4189864282173267\n",
      "train loss:0.060171714614369994, test loss:0.4188596200442381\n",
      "train loss:0.06013400154542448, test loss:0.4187319495107266\n",
      "train loss:0.0600963383555546, test loss:0.41860564238121345\n",
      "train loss:0.060058758192576965, test loss:0.4184786321572254\n",
      "train loss:0.060021173512689166, test loss:0.41835240721522216\n",
      "train loss:0.05998358642702355, test loss:0.4182246921660809\n",
      "train loss:0.0599460684364812, test loss:0.4180975261615396\n",
      "train loss:0.05990853098192447, test loss:0.41797053703062886\n",
      "train loss:0.059833687548966875, test loss:0.4177165917377098\n",
      "train loss:0.059796357204794176, test loss:0.4175881156979615\n",
      "train loss:0.059758965483645975, test loss:0.41745989739809053\n",
      "train loss:0.05972161984152349, test loss:0.4173305979573149\n",
      "train loss:0.059684323520405716, test loss:0.4172020161656045\n",
      "train loss:0.05964709881495415, test loss:0.4170727891194531\n",
      "train loss:0.05960991709585378, test loss:0.41694461474667266\n",
      "train loss:0.059572698472898686, test loss:0.41681568444478106\n",
      "train loss:0.05953553786928659, test loss:0.4166875160058327\n",
      "train loss:0.05946133660157294, test loss:0.4164318259100199\n",
      "train loss:0.059424265299726685, test loss:0.41630359168492087\n",
      "train loss:0.05938721010468334, test loss:0.4161769706226083\n",
      "train loss:0.05935020787410984, test loss:0.41604989744274945\n",
      "train loss:0.05931326796577473, test loss:0.4159257802382586\n",
      "train loss:0.05927630360188658, test loss:0.4158033275194199\n",
      "train loss:0.05923939664891648, test loss:0.4156814185529147\n",
      "train loss:0.05920253816586391, test loss:0.4155603234775306\n",
      "train loss:0.05916571124239921, test loss:0.4154387621678718\n",
      "train loss:0.059092089339335754, test loss:0.4151972976357643\n",
      "train loss:0.0590553359016029, test loss:0.4150758944877888\n",
      "train loss:0.05901856791529982, test loss:0.41495502630807485\n",
      "train loss:0.05898185349475772, test loss:0.41483437799184236\n",
      "train loss:0.05894517468760996, test loss:0.4147147391286994\n",
      "train loss:0.05890850366360302, test loss:0.41459494706755456\n",
      "train loss:0.05887186320134687, test loss:0.41447572525910403\n",
      "train loss:0.05883531527697944, test loss:0.4143560616493289\n",
      "train loss:0.05879869680559221, test loss:0.4142371216695912\n",
      "train loss:0.05872565148380808, test loss:0.4140003146915047\n",
      "train loss:0.05868917199217956, test loss:0.4138832407498184\n",
      "train loss:0.05865272425075629, test loss:0.41376677532830636\n",
      "train loss:0.0586163501525634, test loss:0.41365013051425925\n",
      "train loss:0.05857991591422707, test loss:0.4135347560198224\n",
      "train loss:0.058543534258498724, test loss:0.4134182987703177\n",
      "train loss:0.05850718151664672, test loss:0.4133019988497681\n",
      "train loss:0.05847089571798483, test loss:0.4131858596178641\n",
      "train loss:0.058434630891548635, test loss:0.41306962982413253\n",
      "train loss:0.058362117915379826, test loss:0.41283736519202624\n",
      "train loss:0.058325891773166404, test loss:0.41272037789575594\n",
      "train loss:0.05828979793784965, test loss:0.41260364861671883\n",
      "train loss:0.058253637726114076, test loss:0.4124855263531652\n",
      "train loss:0.058217528108062834, test loss:0.41236643785381044\n",
      "train loss:0.058181457694229284, test loss:0.41224629995779316\n",
      "train loss:0.058145423814194505, test loss:0.41212645027314254\n",
      "train loss:0.05810942114099774, test loss:0.41200617691386554\n",
      "train loss:0.05807343401885962, test loss:0.411887141192063\n",
      "train loss:0.058001572102156326, test loss:0.41164820267298435\n",
      "train loss:0.057965647134782104, test loss:0.41152884080238333\n",
      "train loss:0.057929808031597356, test loss:0.4114101128589477\n",
      "train loss:0.05789398019386771, test loss:0.41129081307816495\n",
      "train loss:0.05785819085438546, test loss:0.4111711441154124\n",
      "train loss:0.057822431668754955, test loss:0.41105266740998364\n",
      "train loss:0.05778670001446797, test loss:0.4109342945913189\n",
      "train loss:0.0577509746838723, test loss:0.41081468487778533\n",
      "train loss:0.05771535275500466, test loss:0.4106973221968848\n",
      "train loss:0.057644109618890355, test loss:0.4104640746140863\n",
      "train loss:0.057608572717423166, test loss:0.41034804333751584\n",
      "train loss:0.057573069843174285, test loss:0.4102322509817735\n",
      "train loss:0.05753755560235164, test loss:0.4101161060733461\n",
      "train loss:0.057502089143280276, test loss:0.410000454803547\n",
      "train loss:0.057466675245814926, test loss:0.40988475774687655\n",
      "train loss:0.05743127342648897, test loss:0.4097696962863637\n",
      "train loss:0.0573959320505998, test loss:0.4096548866526632\n",
      "train loss:0.05736060133554442, test loss:0.40953983844873615\n",
      "train loss:0.05729004393385375, test loss:0.4093095161381869\n",
      "train loss:0.05725481868716782, test loss:0.4091953496933993\n",
      "train loss:0.05721954210406048, test loss:0.4090821238216077\n",
      "train loss:0.05718440838933805, test loss:0.40897014702257817\n",
      "train loss:0.05714925778639982, test loss:0.40885721415498066\n",
      "train loss:0.05711411349235957, test loss:0.4087426323086316\n",
      "train loss:0.057079006998399344, test loss:0.4086324006834359\n",
      "train loss:0.05704396778695825, test loss:0.40851829090427627\n",
      "train loss:0.05700889302866351, test loss:0.40840433837786716\n",
      "train loss:0.05693898074884281, test loss:0.4081789168906627\n",
      "train loss:0.05690402197304767, test loss:0.4080652267510303\n",
      "train loss:0.056869131408613244, test loss:0.40795314908660646\n",
      "train loss:0.05683429303281172, test loss:0.407840498447886\n",
      "train loss:0.05679947704223069, test loss:0.40772795142484913\n",
      "train loss:0.05676469723702942, test loss:0.4076174849295931\n",
      "train loss:0.056729912115883796, test loss:0.4075064931939148\n",
      "train loss:0.05669514232603385, test loss:0.4073958406769995\n",
      "train loss:0.056660475204287436, test loss:0.4072858276945217\n",
      "train loss:0.05659116138693739, test loss:0.40706583563072263\n",
      "train loss:0.056556505503694975, test loss:0.40695622630924694\n",
      "train loss:0.056521939422461545, test loss:0.4068469326291208\n",
      "train loss:0.056487395005943034, test loss:0.4067387211377252\n",
      "train loss:0.05645290374704395, test loss:0.40662984362337584\n",
      "train loss:0.05641838488403829, test loss:0.40652161230318956\n",
      "train loss:0.05638386716187004, test loss:0.4064126427894803\n",
      "train loss:0.0563495159365657, test loss:0.40630458493043986\n",
      "train loss:0.056315129530226674, test loss:0.4061956998407279\n",
      "train loss:0.05624641049545445, test loss:0.40597703947184155\n",
      "train loss:0.056212127488246694, test loss:0.4058672031997309\n",
      "train loss:0.05617781699513899, test loss:0.40575836508405233\n",
      "train loss:0.05614359421033822, test loss:0.4056472176728725\n",
      "train loss:0.056109461655053455, test loss:0.405538127467591\n",
      "train loss:0.05607526917487316, test loss:0.4054267794107791\n",
      "train loss:0.05604113916234856, test loss:0.4053166322142888\n",
      "train loss:0.05600702284520635, test loss:0.4052054666948205\n",
      "train loss:0.05597293472791727, test loss:0.4050948583964609\n",
      "train loss:0.05590493934709864, test loss:0.4048724435267778\n",
      "train loss:0.05587094088571489, test loss:0.4047613858376087\n",
      "train loss:0.05583703167770378, test loss:0.40465142059486575\n",
      "train loss:0.055803147749930004, test loss:0.4045406848077814\n",
      "train loss:0.055769250388170655, test loss:0.4044317721058847\n",
      "train loss:0.0557353953542265, test loss:0.4043232941067839\n",
      "train loss:0.05570158771867465, test loss:0.4042144178666673\n",
      "train loss:0.05566777811561576, test loss:0.4041060174123871\n",
      "train loss:0.05563403597334008, test loss:0.4040004120810088\n",
      "train loss:0.05556663252134914, test loss:0.40378741266717244\n",
      "train loss:0.05553292750523655, test loss:0.4036836093774334\n",
      "train loss:0.055499283743424555, test loss:0.40357892459485917\n",
      "train loss:0.055465658847150406, test loss:0.403475453625522\n",
      "train loss:0.05543209108632212, test loss:0.40337239605872977\n",
      "train loss:0.05539859508752466, test loss:0.4032699255579005\n",
      "train loss:0.05536508159469344, test loss:0.40316755846307484\n",
      "train loss:0.05533153813321799, test loss:0.4030639663934784\n",
      "train loss:0.05529806255705425, test loss:0.40296104524523224\n",
      "train loss:0.055231272098048054, test loss:0.40275558280870244\n",
      "train loss:0.05519790277032261, test loss:0.4026518276495808\n",
      "train loss:0.05516458875827822, test loss:0.4025478944342168\n",
      "train loss:0.055131295102785624, test loss:0.40244377045254004\n",
      "train loss:0.055098005913385914, test loss:0.40233856908977467\n",
      "train loss:0.05506479813050111, test loss:0.40223379873485177\n",
      "train loss:0.05503159753046514, test loss:0.40212881534501616\n",
      "train loss:0.054998469792917316, test loss:0.4020248652182263\n",
      "train loss:0.054965267014300816, test loss:0.4019193349845296\n",
      "train loss:0.05489907776317502, test loss:0.4017117091716728\n",
      "train loss:0.05486602084928015, test loss:0.4016085543914697\n",
      "train loss:0.05483297968361718, test loss:0.4015045927517175\n",
      "train loss:0.05479999165804701, test loss:0.40140246377767475\n",
      "train loss:0.0547669492264026, test loss:0.4013009137185544\n",
      "train loss:0.0547340131164562, test loss:0.40119856910232293\n",
      "train loss:0.054701062009630486, test loss:0.40109608419757403\n",
      "train loss:0.05466815897043176, test loss:0.40099424655759647\n",
      "train loss:0.054635266576766504, test loss:0.4008915018382295\n",
      "train loss:0.05456960768069158, test loss:0.4006870339703567\n",
      "train loss:0.054536760097797926, test loss:0.400583364148142\n",
      "train loss:0.054504020857802794, test loss:0.400480886024545\n",
      "train loss:0.05447124918335447, test loss:0.40037782236679437\n",
      "train loss:0.05443854316039273, test loss:0.400274212434816\n",
      "train loss:0.05440583074662745, test loss:0.4001713479277265\n",
      "train loss:0.054373186535271, test loss:0.4000671891979313\n",
      "train loss:0.05434052804947935, test loss:0.3999635628004013\n",
      "train loss:0.054307864798717444, test loss:0.3998589365585725\n",
      "train loss:0.05424273185591093, test loss:0.3996480483277703\n",
      "train loss:0.05421017604903292, test loss:0.39954119169749863\n",
      "train loss:0.05417764455315864, test loss:0.39943522572770046\n",
      "train loss:0.05414506994096178, test loss:0.3993268882403054\n",
      "train loss:0.054112608390820806, test loss:0.39921755026142847\n",
      "train loss:0.054080139973993135, test loss:0.39910724652241036\n",
      "train loss:0.05404765528579652, test loss:0.3989962960789424\n",
      "train loss:0.05401527251849619, test loss:0.39888446166709723\n",
      "train loss:0.053982852040969725, test loss:0.3987731008360084\n",
      "train loss:0.053918119151401496, test loss:0.39854356982746575\n",
      "train loss:0.05388581984474798, test loss:0.39842836455865227\n",
      "train loss:0.053853581544805476, test loss:0.3983133617343775\n",
      "train loss:0.053821271952742944, test loss:0.3981975339054085\n",
      "train loss:0.05378908044142206, test loss:0.398082601938842\n",
      "train loss:0.053756885459919156, test loss:0.39796768290570655\n",
      "train loss:0.05372468389152582, test loss:0.3978533835643096\n",
      "train loss:0.0536925245935744, test loss:0.39773944287304414\n",
      "train loss:0.053660414092201146, test loss:0.3976268787613626\n",
      "train loss:0.05359621858976609, test loss:0.3974047068756581\n",
      "train loss:0.05356411316381529, test loss:0.3972956193264224\n",
      "train loss:0.05353212700313087, test loss:0.39718606227570985\n",
      "train loss:0.053500131259948916, test loss:0.3970777883851511\n",
      "train loss:0.053468171740431183, test loss:0.396969428177219\n",
      "train loss:0.05343622491246311, test loss:0.3968626186539419\n",
      "train loss:0.05340428799097867, test loss:0.3967568283401711\n",
      "train loss:0.05337241444694161, test loss:0.3966511156521028\n",
      "train loss:0.053340507032974244, test loss:0.3965463994350228\n",
      "train loss:0.05327684527974172, test loss:0.39634077887055913\n",
      "train loss:0.05324504217933362, test loss:0.39623858336680595\n",
      "train loss:0.05321326250629601, test loss:0.3961374512010813\n",
      "train loss:0.0531815412169631, test loss:0.3960372240907672\n",
      "train loss:0.05314979836186862, test loss:0.39593747652039557\n",
      "train loss:0.05311810941333185, test loss:0.3958384226856649\n",
      "train loss:0.053086491903137814, test loss:0.39573874231524825\n",
      "train loss:0.0530548256109017, test loss:0.3956393531321572\n",
      "train loss:0.05302322648745612, test loss:0.3955415395395234\n",
      "train loss:0.05296010604201048, test loss:0.39534224677481317\n",
      "train loss:0.05292856420409296, test loss:0.3952428923032945\n",
      "train loss:0.052897051732711343, test loss:0.3951423661788466\n",
      "train loss:0.052865609354545956, test loss:0.39504131666317555\n",
      "train loss:0.05283416907614412, test loss:0.39493844491110675\n",
      "train loss:0.05280271991067754, test loss:0.3948365925961662\n",
      "train loss:0.05277131809249247, test loss:0.3947331535374129\n",
      "train loss:0.05273999114457201, test loss:0.39462987054789517\n",
      "train loss:0.052708632680020406, test loss:0.39452683072391925\n",
      "train loss:0.052646050506491626, test loss:0.39431899910026225\n",
      "train loss:0.05261477822782749, test loss:0.39421419389565293\n",
      "train loss:0.05258356427582212, test loss:0.394108406759057\n",
      "train loss:0.052552378760375974, test loss:0.3940017557070633\n",
      "train loss:0.052521168729704616, test loss:0.3938942623997032\n",
      "train loss:0.05249002881351184, test loss:0.3937856436537759\n",
      "train loss:0.05245893479446817, test loss:0.3936767069597498\n",
      "train loss:0.05242783651039292, test loss:0.3935674290081782\n",
      "train loss:0.052396838091487936, test loss:0.39345744367364766\n",
      "train loss:0.05233478244904703, test loss:0.3932385719035262\n",
      "train loss:0.052303860181135996, test loss:0.39312916463745995\n",
      "train loss:0.052272932226822874, test loss:0.39302070570482367\n",
      "train loss:0.05224201176451075, test loss:0.39291228027517533\n",
      "train loss:0.05221115312781228, test loss:0.39280479557132586\n",
      "train loss:0.052180328258358104, test loss:0.3926983826330312\n",
      "train loss:0.052149516500939706, test loss:0.3925926445114237\n",
      "train loss:0.05211871525927553, test loss:0.3924871308419413\n",
      "train loss:0.052087955703477046, test loss:0.39238298099902685\n",
      "train loss:0.05202648973946667, test loss:0.39217741580061194\n",
      "train loss:0.051995855006087985, test loss:0.39207574255164157\n",
      "train loss:0.051965163566716155, test loss:0.3919748581050366\n",
      "train loss:0.0519345762539028, test loss:0.39187165854915806\n",
      "train loss:0.051903998284806484, test loss:0.39176836093279105\n",
      "train loss:0.05187337417648334, test loss:0.3916648269498062\n",
      "train loss:0.051842837348832126, test loss:0.3915603610448058\n",
      "train loss:0.051812346611217815, test loss:0.3914547684417742\n",
      "train loss:0.05178184666724296, test loss:0.3913484195830731\n",
      "train loss:0.051720917614184146, test loss:0.39113514212383027\n",
      "train loss:0.051690508825708935, test loss:0.39102571681036646\n",
      "train loss:0.05166007707905188, test loss:0.39091780295590545\n",
      "train loss:0.05162968732217639, test loss:0.3908083242712561\n",
      "train loss:0.051599372975362906, test loss:0.3906984151103093\n",
      "train loss:0.05156903054787161, test loss:0.3905880671432824\n",
      "train loss:0.05153872629328338, test loss:0.39047900454666673\n",
      "train loss:0.05150844580160825, test loss:0.39037032959434187\n",
      "train loss:0.05147820167295476, test loss:0.3902603018908593\n",
      "train loss:0.051417757268025596, test loss:0.3900421259762519\n",
      "train loss:0.051387575585197784, test loss:0.3899308801950908\n",
      "train loss:0.05135741828367846, test loss:0.3898196016831638\n",
      "train loss:0.05132728664301321, test loss:0.38970905730248157\n",
      "train loss:0.05129720463020111, test loss:0.38959871562938103\n",
      "train loss:0.0512670949049632, test loss:0.3894890745595665\n",
      "train loss:0.05123705604589278, test loss:0.38937829153306724\n",
      "train loss:0.051207008856705226, test loss:0.3892702610378199\n",
      "train loss:0.051176971942559546, test loss:0.389162059758342\n",
      "train loss:0.05111701568909539, test loss:0.38894680383849284\n",
      "train loss:0.051087130192285155, test loss:0.388840207776918\n",
      "train loss:0.051057170857785765, test loss:0.3887352795982167\n",
      "train loss:0.05102730543576198, test loss:0.3886314007356655\n",
      "train loss:0.05099743018006803, test loss:0.3885264625290731\n",
      "train loss:0.05096758269876691, test loss:0.3884230214159014\n",
      "train loss:0.05093780467528468, test loss:0.38831855884052374\n",
      "train loss:0.05090797063252932, test loss:0.38821437714264106\n",
      "train loss:0.050878175412348035, test loss:0.388109160836851\n",
      "train loss:0.05081864764181629, test loss:0.3878991964614697\n",
      "train loss:0.050788934008088954, test loss:0.3877946399924081\n",
      "train loss:0.050759192470176705, test loss:0.38768956373684776\n",
      "train loss:0.05072954292409892, test loss:0.3875845791626235\n",
      "train loss:0.05069984405671652, test loss:0.3874805597318821\n",
      "train loss:0.05067017953847268, test loss:0.3873733331897056\n",
      "train loss:0.05064058686329721, test loss:0.38726696690625184\n",
      "train loss:0.050610927075596425, test loss:0.38716154424628446\n",
      "train loss:0.05058130670548581, test loss:0.3870555253152531\n",
      "train loss:0.05052228807105359, test loss:0.38684509920018545\n",
      "train loss:0.05049277658550991, test loss:0.38673996471749417\n",
      "train loss:0.05046320256838498, test loss:0.38663490025974284\n",
      "train loss:0.050433746120122254, test loss:0.3865304348316336\n",
      "train loss:0.050404267578335944, test loss:0.3864267704188044\n",
      "train loss:0.050374844996353085, test loss:0.3863211986294337\n",
      "train loss:0.05034544675388488, test loss:0.38621635548706823\n",
      "train loss:0.05031608654487045, test loss:0.3861114777890977\n",
      "train loss:0.050286701001353844, test loss:0.3860066962419791\n",
      "train loss:0.050228065014407706, test loss:0.38579473904420813\n",
      "train loss:0.050198729026055385, test loss:0.38568889436548676\n",
      "train loss:0.05016951504105739, test loss:0.3855818400785147\n",
      "train loss:0.050140225483520796, test loss:0.38547513842313336\n",
      "train loss:0.05011097862884402, test loss:0.3853706107624162\n",
      "train loss:0.050081796946792545, test loss:0.3852647262900961\n",
      "train loss:0.0500525803202141, test loss:0.3851593428880782\n",
      "train loss:0.05002341713467221, test loss:0.3850558765343889\n",
      "train loss:0.049994279836075954, test loss:0.38495283200436387\n",
      "train loss:0.04993606625129531, test loss:0.38474902800826805\n",
      "train loss:0.04990698618917789, test loss:0.384648011888436\n",
      "train loss:0.049878024710261426, test loss:0.3845473760276287\n",
      "train loss:0.04984897895233721, test loss:0.38444690246280605\n",
      "train loss:0.049820018439488634, test loss:0.3843470223763944\n",
      "train loss:0.049791091723195235, test loss:0.3842463762257551\n",
      "train loss:0.04976212892075337, test loss:0.384147301547881\n",
      "train loss:0.04973325767479194, test loss:0.3840472232940427\n",
      "train loss:0.04970438013470281, test loss:0.38394581162242536\n",
      "train loss:0.049646729257295796, test loss:0.3837418020287471\n",
      "train loss:0.04961791835040817, test loss:0.3836376625533989\n",
      "train loss:0.04958912888495844, test loss:0.383532496679051\n",
      "train loss:0.04956036804434599, test loss:0.38342635898461197\n",
      "train loss:0.049531644349720766, test loss:0.38331921635111976\n",
      "train loss:0.04950291823877312, test loss:0.3832107170762715\n",
      "train loss:0.049474245551446784, test loss:0.38310233671844496\n",
      "train loss:0.049445606831650245, test loss:0.38299408751588343\n",
      "train loss:0.04941695464776699, test loss:0.3828867318089856\n",
      "train loss:0.04935980820344068, test loss:0.38267061204369895\n",
      "train loss:0.04933128395604156, test loss:0.3825648865748333\n",
      "train loss:0.04930273090321679, test loss:0.38245841557851556\n",
      "train loss:0.049274219469335176, test loss:0.3823530859638328\n",
      "train loss:0.0492457378249822, test loss:0.3822485640699886\n",
      "train loss:0.049217321855450136, test loss:0.3821428707697048\n",
      "train loss:0.049188857259554825, test loss:0.38203984219195253\n",
      "train loss:0.049160474373239595, test loss:0.3819364428388704\n",
      "train loss:0.049132051627477934, test loss:0.3818335328804146\n",
      "train loss:0.049075358487036626, test loss:0.38162817693778556\n",
      "train loss:0.04904707890931382, test loss:0.3815264226753703\n",
      "train loss:0.0490188027719834, test loss:0.3814247736149081\n",
      "train loss:0.048990514796626375, test loss:0.3813221624061259\n",
      "train loss:0.048962297675196796, test loss:0.38122023668797556\n",
      "train loss:0.04893409940661596, test loss:0.38111846457476295\n",
      "train loss:0.048905885571693675, test loss:0.3810177238525718\n",
      "train loss:0.048877736915961624, test loss:0.3809169685736705\n",
      "train loss:0.048849582333737866, test loss:0.3808162937115988\n",
      "train loss:0.04879336665619628, test loss:0.3806138385610958\n",
      "train loss:0.04876529355959964, test loss:0.38051256567530917\n",
      "train loss:0.04873725787730646, test loss:0.3804112750763305\n",
      "train loss:0.04870927109089322, test loss:0.38030982626225485\n",
      "train loss:0.04868128408983669, test loss:0.3802077442443306\n",
      "train loss:0.048653317321561744, test loss:0.38010631089484637\n",
      "train loss:0.048625380742100266, test loss:0.38000426898541617\n",
      "train loss:0.04859744559985153, test loss:0.37990052563693766\n",
      "train loss:0.048569578856092234, test loss:0.3797988514173846\n",
      "train loss:0.048513850692627924, test loss:0.37959232571662294\n",
      "train loss:0.048486050999038396, test loss:0.3794874584846736\n",
      "train loss:0.048458245438582945, test loss:0.37938383381661733\n",
      "train loss:0.04843047980919003, test loss:0.3792801281703563\n",
      "train loss:0.048402722575549346, test loss:0.3791759306455944\n",
      "train loss:0.04837497469353459, test loss:0.379071422630197\n",
      "train loss:0.0483472725153283, test loss:0.37896684633191374\n",
      "train loss:0.0483195942380288, test loss:0.37886153204921263\n",
      "train loss:0.048291942280339284, test loss:0.37875673804312643\n",
      "train loss:0.04823673303697011, test loss:0.37854341756341564\n",
      "train loss:0.04820911126237081, test loss:0.3784352919105104\n",
      "train loss:0.04818155795664361, test loss:0.3783277157555233\n",
      "train loss:0.048154058525679805, test loss:0.3782184024644194\n",
      "train loss:0.04812654526692937, test loss:0.37811168572556647\n",
      "train loss:0.048099045853417025, test loss:0.37800450120627865\n",
      "train loss:0.04807156530274407, test loss:0.3778957971162627\n",
      "train loss:0.04804412171325578, test loss:0.37778931967481955\n",
      "train loss:0.04801672503788683, test loss:0.3776803840807969\n",
      "train loss:0.04796196911132897, test loss:0.3774670665533852\n",
      "train loss:0.0479345796920235, test loss:0.3773590181645099\n",
      "train loss:0.04790726638563281, test loss:0.3772508711729583\n",
      "train loss:0.04787994893053267, test loss:0.3771435929546205\n",
      "train loss:0.047852648100220586, test loss:0.3770351425714558\n",
      "train loss:0.04782538821806018, test loss:0.37692731109361016\n",
      "train loss:0.047798122311123646, test loss:0.3768194863623371\n",
      "train loss:0.047770927263946036, test loss:0.3767127808713413\n",
      "train loss:0.04774372145611435, test loss:0.3766060504868401\n",
      "train loss:0.04768934235929254, test loss:0.3763939285414764\n",
      "train loss:0.04766224869712703, test loss:0.37628949756536056\n",
      "train loss:0.04763509491159946, test loss:0.37618469843680763\n",
      "train loss:0.04760805928379924, test loss:0.37608145116707165\n",
      "train loss:0.04758097366828643, test loss:0.37597919019532433\n",
      "train loss:0.047553857724103525, test loss:0.37587702320267535\n",
      "train loss:0.04752686037182883, test loss:0.3757762458724587\n",
      "train loss:0.04749987471453897, test loss:0.37567416663705205\n",
      "train loss:0.04747287602242915, test loss:0.37557306829153553\n",
      "train loss:0.04741899579034223, test loss:0.37537094336941534\n",
      "train loss:0.04739204998524009, test loss:0.3752696354150145\n",
      "train loss:0.04736516423224934, test loss:0.37516847203232423\n",
      "train loss:0.04733831644435104, test loss:0.3750663422109744\n",
      "train loss:0.04731145111825162, test loss:0.3749650501046308\n",
      "train loss:0.04728465071512752, test loss:0.37486377809275323\n",
      "train loss:0.04725781464299411, test loss:0.3747624407341675\n",
      "train loss:0.047231032237769255, test loss:0.3746627773388764\n",
      "train loss:0.047204216094677294, test loss:0.3745618507331748\n",
      "train loss:0.04715069886853442, test loss:0.3743626586809595\n",
      "train loss:0.04712396021574839, test loss:0.3742623169227451\n",
      "train loss:0.04709728808499194, test loss:0.3741613612547558\n",
      "train loss:0.04707058537127303, test loss:0.3740602543831837\n",
      "train loss:0.047043962460643726, test loss:0.37395734242399226\n",
      "train loss:0.04701726111658033, test loss:0.3738542176957606\n",
      "train loss:0.04699064458163142, test loss:0.3737512610392005\n",
      "train loss:0.046964058330857913, test loss:0.37364709334881463\n",
      "train loss:0.04693750137381649, test loss:0.3735429412552063\n",
      "train loss:0.04688434760722611, test loss:0.3733314776929371\n",
      "train loss:0.046857837119228585, test loss:0.373225740908368\n",
      "train loss:0.046831342151428285, test loss:0.3731206673441418\n",
      "train loss:0.046804858549065735, test loss:0.37301525296147814\n",
      "train loss:0.04677840949932159, test loss:0.3729105842090538\n",
      "train loss:0.04675198144457957, test loss:0.37280658222335034\n",
      "train loss:0.046725538541251666, test loss:0.3727018992972236\n",
      "train loss:0.04669914161273155, test loss:0.37259841836226903\n",
      "train loss:0.04667279555062921, test loss:0.37249563017135673\n",
      "train loss:0.04662008131680032, test loss:0.3722887855829461\n",
      "train loss:0.046593749020291814, test loss:0.37218596160794415\n",
      "train loss:0.04656742893889442, test loss:0.372081949138781\n",
      "train loss:0.04654118046252729, test loss:0.37197830256174474\n",
      "train loss:0.0465149208531523, test loss:0.3718730877890025\n",
      "train loss:0.04648867639208541, test loss:0.37176901504506576\n",
      "train loss:0.04646250019648893, test loss:0.3716636892299563\n",
      "train loss:0.046436285848827974, test loss:0.37155893687794284\n",
      "train loss:0.04641010850213693, test loss:0.37145337942945966\n",
      "train loss:0.04635787412632871, test loss:0.37124178880981057\n",
      "train loss:0.046331752338018645, test loss:0.37113724077338556\n",
      "train loss:0.0463057319842128, test loss:0.3710318479424997\n",
      "train loss:0.04627962030563913, test loss:0.37092610507149365\n",
      "train loss:0.04625362264624868, test loss:0.37082084867038645\n",
      "train loss:0.046227600591664766, test loss:0.37071607515537397\n",
      "train loss:0.04620160715717168, test loss:0.3706116885603074\n",
      "train loss:0.046175641399540085, test loss:0.3705085083161302\n",
      "train loss:0.04614972699336385, test loss:0.3704061507967149\n",
      "train loss:0.04609785111109747, test loss:0.37020216870103806\n",
      "train loss:0.046071955928053625, test loss:0.37010193853263024\n",
      "train loss:0.046046119860344036, test loss:0.3700036621077025\n",
      "train loss:0.04602024339016174, test loss:0.3699040679297675\n",
      "train loss:0.04599440398836073, test loss:0.3698055481077367\n",
      "train loss:0.045968616637958364, test loss:0.36970720820536573\n",
      "train loss:0.04594277489550892, test loss:0.36960976882229335\n",
      "train loss:0.04591697725842131, test loss:0.3695115369491067\n",
      "train loss:0.04589122312108861, test loss:0.3694134679303315\n",
      "train loss:0.045839725334636774, test loss:0.36921600624517054\n",
      "train loss:0.045814031091252524, test loss:0.3691161751016138\n",
      "train loss:0.04578832571349708, test loss:0.3690161073052125\n",
      "train loss:0.045762634081149375, test loss:0.36891672216455695\n",
      "train loss:0.04573699471000513, test loss:0.36881647091678643\n",
      "train loss:0.04571139667973132, test loss:0.3687178000270791\n",
      "train loss:0.04568575104814874, test loss:0.3686172727937796\n",
      "train loss:0.04566018315107175, test loss:0.36851849911925805\n",
      "train loss:0.04563465203857905, test loss:0.36841786454954334\n",
      "train loss:0.04558355120391107, test loss:0.3682195317141303\n",
      "train loss:0.045558039623620976, test loss:0.36812138963383195\n",
      "train loss:0.045532602488191826, test loss:0.36802383330470334\n",
      "train loss:0.04550711173155073, test loss:0.36792525748664145\n",
      "train loss:0.045481671971304274, test loss:0.3678273732301659\n",
      "train loss:0.045456252439652796, test loss:0.36772856346188687\n",
      "train loss:0.04543078940979312, test loss:0.3676314241107851\n",
      "train loss:0.04540544088201525, test loss:0.36753302628270573\n",
      "train loss:0.04538004194218675, test loss:0.3674355762369511\n",
      "train loss:0.04532932270905467, test loss:0.3672383949513779\n",
      "train loss:0.04530398261393463, test loss:0.3671396308113216\n",
      "train loss:0.045278688472926265, test loss:0.36704050609122085\n",
      "train loss:0.045253403339701424, test loss:0.3669415288809968\n",
      "train loss:0.045228088573842244, test loss:0.3668417792882673\n",
      "train loss:0.0452028654890232, test loss:0.3667405891949664\n",
      "train loss:0.04517761453372194, test loss:0.36664153527637594\n",
      "train loss:0.045152417441530826, test loss:0.36654072228888623\n",
      "train loss:0.04512719462247589, test loss:0.3664408047936174\n",
      "train loss:0.045076834358998334, test loss:0.3662400647475976\n",
      "train loss:0.04505169745741539, test loss:0.3661399456831113\n",
      "train loss:0.045026559513096684, test loss:0.3660400854033022\n",
      "train loss:0.045001453897643394, test loss:0.36594134358359315\n",
      "train loss:0.04497634746646885, test loss:0.3658428897695066\n",
      "train loss:0.04495129449746035, test loss:0.36574463240679583\n",
      "train loss:0.044926229718281356, test loss:0.36564568758400895\n",
      "train loss:0.044901167346770424, test loss:0.3655479625305532\n",
      "train loss:0.04487617106529526, test loss:0.3654513207710464\n",
      "train loss:0.0448261798264127, test loss:0.36525757164765327\n",
      "train loss:0.044801237405547374, test loss:0.3651619681356005\n",
      "train loss:0.04477626864430392, test loss:0.3650679422664655\n",
      "train loss:0.04475138273127914, test loss:0.36497357636972894\n",
      "train loss:0.04472647974556532, test loss:0.3648785200815142\n",
      "train loss:0.04470162908474539, test loss:0.36478510030889133\n",
      "train loss:0.044676711520833, test loss:0.3646921044398923\n",
      "train loss:0.04465187500514511, test loss:0.364599929646144\n",
      "train loss:0.044627054545105696, test loss:0.36450827629622234\n",
      "train loss:0.0445774605499548, test loss:0.3643283505725432\n",
      "train loss:0.04455268750054588, test loss:0.36423846954491507\n",
      "train loss:0.04452795391793453, test loss:0.36414906161143357\n",
      "train loss:0.04450324788657569, test loss:0.3640591084042422\n",
      "train loss:0.04447854247168591, test loss:0.36396799489911075\n",
      "train loss:0.044453864101889506, test loss:0.36387836688550745\n",
      "train loss:0.04442922014925984, test loss:0.3637873486077491\n",
      "train loss:0.044404565425142685, test loss:0.3636962528544133\n",
      "train loss:0.044379955369116625, test loss:0.3636053125747467\n",
      "train loss:0.04433078620531494, test loss:0.36342359374859506\n",
      "train loss:0.044306223680881114, test loss:0.363333606669993\n",
      "train loss:0.0442816509295739, test loss:0.36324389948672925\n",
      "train loss:0.044257100323540514, test loss:0.363154229552037\n",
      "train loss:0.04423266015931498, test loss:0.3630651922961368\n",
      "train loss:0.04420809928179933, test loss:0.3629757755353484\n",
      "train loss:0.04418363667846405, test loss:0.36288557063985166\n",
      "train loss:0.044159143176075916, test loss:0.3627954147697564\n",
      "train loss:0.044134733958243674, test loss:0.3627058034065324\n",
      "train loss:0.044085873751698375, test loss:0.36252416288450184\n",
      "train loss:0.044061515668674035, test loss:0.3624339429575057\n",
      "train loss:0.0440371233899602, test loss:0.36234327557316043\n",
      "train loss:0.04401278525558223, test loss:0.36225153415302613\n",
      "train loss:0.04398848359882625, test loss:0.3621609269628211\n",
      "train loss:0.043964173324479036, test loss:0.36206984256454844\n",
      "train loss:0.0439398763198151, test loss:0.3619772891775382\n",
      "train loss:0.04391557107734265, test loss:0.3618845046745963\n",
      "train loss:0.04389134942956232, test loss:0.36179230780437305\n",
      "train loss:0.04384284805785279, test loss:0.361609595054635\n",
      "train loss:0.0438186416573634, test loss:0.3615184664522403\n",
      "train loss:0.04379443751568306, test loss:0.36142683523206093\n",
      "train loss:0.04377024725335314, test loss:0.3613356261883171\n",
      "train loss:0.04374607028398085, test loss:0.3612445003780931\n",
      "train loss:0.04372196714846587, test loss:0.36115277310376126\n",
      "train loss:0.04369782038305294, test loss:0.3610610564000252\n",
      "train loss:0.04367370332001207, test loss:0.36096888783633096\n",
      "train loss:0.04364959993663391, test loss:0.36087548201365566\n",
      "train loss:0.04360143932547561, test loss:0.3606902964522026\n",
      "train loss:0.04357738600658506, test loss:0.36059778204087456\n",
      "train loss:0.0435533565654682, test loss:0.36050632341970773\n",
      "train loss:0.04352934078722688, test loss:0.36041447696192747\n",
      "train loss:0.04350536397563528, test loss:0.3603231797210575\n",
      "train loss:0.04348139200370195, test loss:0.36023360041114444\n",
      "train loss:0.04345740132425669, test loss:0.36014457175577486\n",
      "train loss:0.043433477254714505, test loss:0.3600552254483334\n",
      "train loss:0.04340952320444518, test loss:0.3599660688897998\n",
      "train loss:0.04336174159046843, test loss:0.3597905466365975\n",
      "train loss:0.04333785758678949, test loss:0.359702894811834\n",
      "train loss:0.043313984938685456, test loss:0.35961754865717727\n",
      "train loss:0.043290134015052946, test loss:0.3595310907331013\n",
      "train loss:0.043266347454612136, test loss:0.35944390786220815\n",
      "train loss:0.04324255198681973, test loss:0.35935598989330086\n",
      "train loss:0.04321874570612646, test loss:0.3592694182116106\n",
      "train loss:0.043194940080884446, test loss:0.3591807227516259\n",
      "train loss:0.043171214687871294, test loss:0.3590927476479679\n",
      "train loss:0.043123797011330914, test loss:0.3589151385359225\n",
      "train loss:0.043100065836860435, test loss:0.3588257701727186\n",
      "train loss:0.04307638073814049, test loss:0.35873691396913826\n",
      "train loss:0.04305274267434386, test loss:0.3586463640824419\n",
      "train loss:0.04302908297472153, test loss:0.3585568646450143\n",
      "train loss:0.04300546282652463, test loss:0.35846737956690333\n",
      "train loss:0.042981830625470556, test loss:0.3583773918101378\n",
      "train loss:0.042958260543739774, test loss:0.3582888128120387\n",
      "train loss:0.04293468638515692, test loss:0.35819969449707156\n",
      "train loss:0.0428875961631141, test loss:0.3580239238638131\n",
      "train loss:0.04286410161896405, test loss:0.3579366887580467\n",
      "train loss:0.04284058330153239, test loss:0.3578481342429768\n",
      "train loss:0.04281706092574726, test loss:0.35776084630621785\n",
      "train loss:0.04279358269690275, test loss:0.35767269946276975\n",
      "train loss:0.04277015047922835, test loss:0.35758530343756556\n",
      "train loss:0.042746685566377736, test loss:0.35749730396637924\n",
      "train loss:0.04272326820449547, test loss:0.35740887071193495\n",
      "train loss:0.04269981743663506, test loss:0.3573207805764433\n",
      "train loss:0.04265304507289659, test loss:0.357142821629155\n",
      "train loss:0.04262964689894736, test loss:0.3570531323959714\n",
      "train loss:0.042606307657056294, test loss:0.3569645302239673\n",
      "train loss:0.042582955797115775, test loss:0.3568749308448161\n",
      "train loss:0.04255963953350199, test loss:0.3567842014497998\n",
      "train loss:0.0425363063325391, test loss:0.3566925679764084\n",
      "train loss:0.042513011037654334, test loss:0.35660170571554783\n",
      "train loss:0.04248972040117529, test loss:0.3565103786058578\n",
      "train loss:0.04246643813755471, test loss:0.35641933956916816\n",
      "train loss:0.04241996177187913, test loss:0.3562338989149232\n",
      "train loss:0.0423967521586454, test loss:0.35614167606752034\n",
      "train loss:0.04237352930277445, test loss:0.3560481380783438\n",
      "train loss:0.0423503301526365, test loss:0.3559545153768754\n",
      "train loss:0.04232717116352294, test loss:0.3558604675926664\n",
      "train loss:0.042303979435900425, test loss:0.3557657466659899\n",
      "train loss:0.04228085898366265, test loss:0.3556697082675961\n",
      "train loss:0.04225771580347541, test loss:0.35557436315179713\n",
      "train loss:0.04223461229917335, test loss:0.3554789624561625\n",
      "train loss:0.04218844390417627, test loss:0.3552842282327308\n",
      "train loss:0.042165399759524325, test loss:0.35518525312617216\n",
      "train loss:0.04214239989456422, test loss:0.35508656305487674\n",
      "train loss:0.04211936316967722, test loss:0.354988154401625\n",
      "train loss:0.042096329109357566, test loss:0.35488972107444866\n",
      "train loss:0.04207335281673609, test loss:0.35479128307663077\n",
      "train loss:0.042050364656506814, test loss:0.3546919390416102\n",
      "train loss:0.04202738158654143, test loss:0.3545943440093159\n",
      "train loss:0.04200444325013007, test loss:0.35449607341021905\n",
      "train loss:0.04195859866380781, test loss:0.3543012767088255\n",
      "train loss:0.04193572008133206, test loss:0.3542041124042744\n",
      "train loss:0.04191282469533981, test loss:0.35410964007137524\n",
      "train loss:0.041890008329957755, test loss:0.3540137947992875\n",
      "train loss:0.04186713730393267, test loss:0.35391883906497046\n",
      "train loss:0.04184430714627194, test loss:0.3538247243874109\n",
      "train loss:0.04182153412700723, test loss:0.35372987914432064\n",
      "train loss:0.04179873845242083, test loss:0.3536365279161192\n",
      "train loss:0.04177596782219993, test loss:0.35354299354800345\n",
      "train loss:0.04173044410022192, test loss:0.3533576665626592\n",
      "train loss:0.04170774816641364, test loss:0.35326548830458804\n",
      "train loss:0.0416850624628451, test loss:0.35317383869910396\n",
      "train loss:0.04166235834646796, test loss:0.3530827403748708\n",
      "train loss:0.04163973404298618, test loss:0.3529936662460634\n",
      "train loss:0.041616983773094016, test loss:0.35290501968797244\n",
      "train loss:0.04159439690924562, test loss:0.3528174514862023\n",
      "train loss:0.04157178382088506, test loss:0.35273065100732803\n",
      "train loss:0.041549189846038705, test loss:0.35264378195774576\n",
      "train loss:0.041503985255096444, test loss:0.35247112731932756\n",
      "train loss:0.04148141458424449, test loss:0.35238529980199323\n",
      "train loss:0.041458873171453775, test loss:0.35229750388689757\n",
      "train loss:0.041436337870674016, test loss:0.3522110127877646\n",
      "train loss:0.04141377780194147, test loss:0.3521240964868155\n",
      "train loss:0.041391259942523115, test loss:0.35203704940236735\n",
      "train loss:0.04136877729000633, test loss:0.3519494551858204\n",
      "train loss:0.0413463046590452, test loss:0.35186351789037773\n",
      "train loss:0.041323818097180734, test loss:0.3517775485141432\n",
      "train loss:0.041278943041368706, test loss:0.35160432115955065\n",
      "train loss:0.04125652241097228, test loss:0.3515174398727825\n",
      "train loss:0.041234125962645606, test loss:0.3514315496190074\n",
      "train loss:0.04121174119662249, test loss:0.35134458249794887\n",
      "train loss:0.0411893854826114, test loss:0.351257741772687\n",
      "train loss:0.04116703022974505, test loss:0.3511719470690187\n",
      "train loss:0.041144695311709535, test loss:0.35108576936839464\n",
      "train loss:0.04112239190535286, test loss:0.35099894687436345\n",
      "train loss:0.04110013930863112, test loss:0.35091350114053166\n",
      "train loss:0.04105560772157061, test loss:0.3507404625547963\n",
      "train loss:0.0410333676454215, test loss:0.35065412696650083\n",
      "train loss:0.041011146981261605, test loss:0.3505679664456801\n",
      "train loss:0.040988972890709414, test loss:0.3504827600576977\n",
      "train loss:0.04096676606082507, test loss:0.3503971594509956\n",
      "train loss:0.040944584823826555, test loss:0.3503120515824671\n",
      "train loss:0.04092242616066323, test loss:0.350227706251536\n",
      "train loss:0.04090028448213644, test loss:0.35014242582320226\n",
      "train loss:0.04087814363789355, test loss:0.3500583600137853\n",
      "train loss:0.04083395121405282, test loss:0.3498882082524312\n",
      "train loss:0.040811874346580936, test loss:0.3498041878581183\n",
      "train loss:0.04078985339127899, test loss:0.3497190873123704\n",
      "train loss:0.04076781814562328, test loss:0.34963453680913675\n",
      "train loss:0.040745815163219144, test loss:0.34955038314590287\n",
      "train loss:0.04072380460320168, test loss:0.3494674217464997\n",
      "train loss:0.04070182275377785, test loss:0.3493836164734476\n",
      "train loss:0.040679860591653495, test loss:0.3493002178139462\n",
      "train loss:0.04065791761565762, test loss:0.3492150340609654\n",
      "train loss:0.04061403306911709, test loss:0.34904235274133594\n",
      "train loss:0.04059212321838223, test loss:0.34895778636702457\n",
      "train loss:0.040570200699024304, test loss:0.34887212626590935\n",
      "train loss:0.040548340140672025, test loss:0.3487853388512033\n",
      "train loss:0.04052647695903833, test loss:0.3486996118166958\n",
      "train loss:0.04050465172757924, test loss:0.3486139945927469\n",
      "train loss:0.04048278584014283, test loss:0.34852935630052356\n",
      "train loss:0.040460941150157054, test loss:0.34844334803457766\n",
      "train loss:0.040439140085915905, test loss:0.34835885614476286\n",
      "train loss:0.040395543885398504, test loss:0.3481892732237182\n",
      "train loss:0.04037376277662355, test loss:0.3481053803376844\n",
      "train loss:0.04035202042457105, test loss:0.3480212547792355\n",
      "train loss:0.040330280652225575, test loss:0.3479377434752894\n",
      "train loss:0.04030857428154521, test loss:0.3478542233793736\n",
      "train loss:0.04028688357132861, test loss:0.34777050272977866\n",
      "train loss:0.04026519917977269, test loss:0.347686752005899\n",
      "train loss:0.04024352084254269, test loss:0.34760315896809174\n",
      "train loss:0.04022181714177579, test loss:0.34752001073863087\n",
      "train loss:0.040178589886771976, test loss:0.34735477586543867\n",
      "train loss:0.040156995086679054, test loss:0.34727249227831836\n",
      "train loss:0.04013541542117968, test loss:0.34718972012378785\n",
      "train loss:0.04011384993407705, test loss:0.3471076798909044\n",
      "train loss:0.0400922702749883, test loss:0.3470254831339265\n",
      "train loss:0.04007071243463307, test loss:0.34694288249858113\n",
      "train loss:0.04004913970321742, test loss:0.34685945514651495\n",
      "train loss:0.04002765616454229, test loss:0.34677768027491085\n",
      "train loss:0.04000611403278502, test loss:0.34669607050397405\n",
      "train loss:0.03996306448872666, test loss:0.34653454087187935\n",
      "train loss:0.03994157188629467, test loss:0.3464543387934632\n",
      "train loss:0.03992008552373634, test loss:0.34637324824531845\n",
      "train loss:0.039898590573674925, test loss:0.3462948300414111\n",
      "train loss:0.03987716043026711, test loss:0.34621456560310704\n",
      "train loss:0.03985570104218942, test loss:0.3461354400691529\n",
      "train loss:0.03983428774677375, test loss:0.3460553996233225\n",
      "train loss:0.039812827276516735, test loss:0.3459758800298463\n",
      "train loss:0.03979142048098349, test loss:0.34589521178811816\n",
      "train loss:0.03974866665696407, test loss:0.3457336373009566\n",
      "train loss:0.03972728601428595, test loss:0.3456519985851957\n",
      "train loss:0.03970593644261037, test loss:0.3455699551989155\n",
      "train loss:0.039684586199184836, test loss:0.34548760444866067\n",
      "train loss:0.039663281139293606, test loss:0.34540583849490286\n",
      "train loss:0.03964195248126094, test loss:0.3453224951218405\n",
      "train loss:0.039620662330251376, test loss:0.34523971723767827\n",
      "train loss:0.03959937515310982, test loss:0.34515601250774364\n",
      "train loss:0.03957816551897731, test loss:0.3450721700796518\n",
      "train loss:0.03953566050923474, test loss:0.3449052886247904\n",
      "train loss:0.039514445462262286, test loss:0.3448216049826618\n",
      "train loss:0.03949321155370396, test loss:0.34473811009753114\n",
      "train loss:0.03947203854883864, test loss:0.3446549021290064\n",
      "train loss:0.039450888045934204, test loss:0.3445719600447436\n",
      "train loss:0.039429711033292916, test loss:0.34448858363478635\n",
      "train loss:0.03940859426552657, test loss:0.3444053517405189\n",
      "train loss:0.03938745579670307, test loss:0.3443229660411925\n",
      "train loss:0.039366354512273105, test loss:0.3442414017179333\n",
      "train loss:0.0393241403122917, test loss:0.3440781022545935\n",
      "train loss:0.039303077761494415, test loss:0.3439949264074075\n",
      "train loss:0.03928201209337068, test loss:0.3439131315369837\n",
      "train loss:0.039260966211062676, test loss:0.3438314880128538\n",
      "train loss:0.03923990260073439, test loss:0.343750217596545\n",
      "train loss:0.03921891383485952, test loss:0.34366902575868996\n",
      "train loss:0.039197936997994474, test loss:0.34358686717591136\n",
      "train loss:0.039176947443744316, test loss:0.3435041815819262\n",
      "train loss:0.03915598673676107, test loss:0.3434213364900073\n",
      "train loss:0.03911409861825446, test loss:0.3432570722843895\n",
      "train loss:0.03909313944274867, test loss:0.34317504354961675\n",
      "train loss:0.039072242316873826, test loss:0.3430930229791756\n",
      "train loss:0.03905131838409952, test loss:0.3430121172398024\n",
      "train loss:0.03903045430336573, test loss:0.3429323536337736\n",
      "train loss:0.03900960053359483, test loss:0.3428523035275208\n",
      "train loss:0.038988749497553585, test loss:0.3427712778518022\n",
      "train loss:0.03896787647912829, test loss:0.3426908529377921\n",
      "train loss:0.038947096625799364, test loss:0.34260839411058475\n",
      "train loss:0.03890548233246913, test loss:0.3424429504422577\n",
      "train loss:0.038884648937180424, test loss:0.34235954581854333\n",
      "train loss:0.0388639354539307, test loss:0.34227616479685957\n",
      "train loss:0.03884321119851485, test loss:0.34219259235521254\n",
      "train loss:0.038822469782442214, test loss:0.34210918883703717\n",
      "train loss:0.03880172651518939, test loss:0.3420261803348766\n",
      "train loss:0.03878100255188907, test loss:0.3419420818424461\n",
      "train loss:0.03876031825919616, test loss:0.34185808150863967\n",
      "train loss:0.038739644637324266, test loss:0.34177313428621064\n",
      "train loss:0.038698297271240335, test loss:0.3416049938335712\n",
      "train loss:0.0386776840637697, test loss:0.341520681134144\n",
      "train loss:0.03865704507147107, test loss:0.34143562359665397\n",
      "train loss:0.038636438403248916, test loss:0.34135058312996136\n",
      "train loss:0.03861581871918467, test loss:0.3412641363550723\n",
      "train loss:0.03859521844078562, test loss:0.34117758329047504\n",
      "train loss:0.038574643202987105, test loss:0.3410912579178695\n",
      "train loss:0.03855409585552087, test loss:0.34100488721364036\n",
      "train loss:0.03853355303449065, test loss:0.34091877128533954\n",
      "train loss:0.038492470188310854, test loss:0.3407456046088309\n",
      "train loss:0.03847202133105659, test loss:0.3406576342705636\n",
      "train loss:0.0384515020512087, test loss:0.34057134835926584\n",
      "train loss:0.038431028526148305, test loss:0.34048553989217195\n",
      "train loss:0.038410550598076854, test loss:0.3403986758927552\n",
      "train loss:0.03839005004406285, test loss:0.34031265787658005\n",
      "train loss:0.038369630826214154, test loss:0.34022700465016525\n",
      "train loss:0.038349180924192004, test loss:0.34014098532080356\n",
      "train loss:0.03832877583569109, test loss:0.3400550617551556\n",
      "train loss:0.03828797459373592, test loss:0.33988336836563443\n",
      "train loss:0.038267566609000485, test loss:0.33979850275364215\n",
      "train loss:0.03824723304978147, test loss:0.33971335646937467\n",
      "train loss:0.03822685011900233, test loss:0.33962894518313413\n",
      "train loss:0.03820653255075224, test loss:0.3395448208556153\n",
      "train loss:0.03818618008742734, test loss:0.339462157780807\n",
      "train loss:0.03816587823169833, test loss:0.3393790008574982\n",
      "train loss:0.03814556499298702, test loss:0.3392956036342077\n",
      "train loss:0.03812527434793174, test loss:0.3392110270240503\n",
      "train loss:0.038084697542518614, test loss:0.3390441817313105\n",
      "train loss:0.0380644066320131, test loss:0.33895988669210386\n",
      "train loss:0.03804414151884943, test loss:0.338877453646073\n",
      "train loss:0.0380239160357354, test loss:0.33879494283735745\n",
      "train loss:0.038003690390145534, test loss:0.33871170904757075\n",
      "train loss:0.03798348085451366, test loss:0.3386287285840461\n",
      "train loss:0.037963280343396674, test loss:0.338544617116079\n",
      "train loss:0.037943079918551954, test loss:0.33846168817273087\n",
      "train loss:0.03792292253464189, test loss:0.33837856436239677\n",
      "train loss:0.03788263182564362, test loss:0.33821222647698407\n",
      "train loss:0.037862506829967, test loss:0.33812826321035777\n",
      "train loss:0.03784238980226131, test loss:0.3380443185158636\n",
      "train loss:0.0378222988769676, test loss:0.3379593411358714\n",
      "train loss:0.03780221051424197, test loss:0.33787556284691433\n",
      "train loss:0.03778216237289426, test loss:0.3377908241145446\n",
      "train loss:0.03776213839004276, test loss:0.33770640952890907\n",
      "train loss:0.03774209307371962, test loss:0.3376220672669971\n",
      "train loss:0.03772206929936455, test loss:0.337537144954486\n",
      "train loss:0.03768208072521036, test loss:0.3373686878262099\n",
      "train loss:0.037662078770900435, test loss:0.3372836543485673\n",
      "train loss:0.03764214126957641, test loss:0.33719939483113615\n",
      "train loss:0.037622165738632064, test loss:0.33711595889898927\n",
      "train loss:0.03760223422742817, test loss:0.3370304522877409\n",
      "train loss:0.03758231390601967, test loss:0.33694560561964026\n",
      "train loss:0.03756242585430227, test loss:0.3368605288892566\n",
      "train loss:0.03754250782220168, test loss:0.33677460599446113\n",
      "train loss:0.03752261914333881, test loss:0.3366888508724471\n",
      "train loss:0.03748290210166114, test loss:0.33651807661925853\n",
      "train loss:0.03746306840790753, test loss:0.33643298305271035\n",
      "train loss:0.03744322652262088, test loss:0.33634870077012385\n",
      "train loss:0.037423405112814674, test loss:0.33626429881404934\n",
      "train loss:0.03740362660303689, test loss:0.3361819620326812\n",
      "train loss:0.03738381482310024, test loss:0.33609902128216856\n",
      "train loss:0.03736403945506065, test loss:0.33601704966234225\n",
      "train loss:0.037344297352254974, test loss:0.3359344899319112\n",
      "train loss:0.03732456589665136, test loss:0.33585195518792793\n",
      "train loss:0.037285136684196044, test loss:0.33568878465062585\n",
      "train loss:0.03726540514790325, test loss:0.33560668664430127\n",
      "train loss:0.037245739869948995, test loss:0.33552383901795746\n",
      "train loss:0.03722606402893405, test loss:0.3354421776532508\n",
      "train loss:0.03720639090676947, test loss:0.3353601231469057\n",
      "train loss:0.0371867531654714, test loss:0.3352795730080903\n",
      "train loss:0.037167109877145846, test loss:0.3351983675019468\n",
      "train loss:0.03714747336446798, test loss:0.3351180228628318\n",
      "train loss:0.037127900604870055, test loss:0.33503628266905233\n",
      "train loss:0.037088695949960025, test loss:0.33487244568424435\n",
      "train loss:0.03706913446689192, test loss:0.33479188522181774\n",
      "train loss:0.03704955712543273, test loss:0.33470980972956316\n",
      "train loss:0.03702999439117108, test loss:0.3346291024572903\n",
      "train loss:0.03701044096537596, test loss:0.33454884556098285\n",
      "train loss:0.03699094223633802, test loss:0.3344679453261532\n",
      "train loss:0.03697140825799654, test loss:0.3343876250509405\n",
      "train loss:0.03695186099024713, test loss:0.3343052730883108\n",
      "train loss:0.03693240143151271, test loss:0.33422463752194703\n",
      "train loss:0.036893441356571646, test loss:0.3340612171724667\n",
      "train loss:0.03687389815994649, test loss:0.3339801720541407\n",
      "train loss:0.036854492496823765, test loss:0.33389728752671116\n",
      "train loss:0.03683502790983652, test loss:0.33381365013324393\n",
      "train loss:0.036815607418888695, test loss:0.33373112898784957\n",
      "train loss:0.036796184923972686, test loss:0.3336481706452754\n",
      "train loss:0.03677679569604056, test loss:0.33356536070321685\n",
      "train loss:0.03675739577023853, test loss:0.3334808709777667\n",
      "train loss:0.03673803819890043, test loss:0.3333972099649446\n",
      "train loss:0.03669929817496504, test loss:0.33322897828291215\n",
      "train loss:0.03667997345945165, test loss:0.3331441694900914\n",
      "train loss:0.03666066597906199, test loss:0.33306038946873234\n",
      "train loss:0.0366413448936538, test loss:0.33297719387742103\n",
      "train loss:0.03662206760414448, test loss:0.33289473093838134\n",
      "train loss:0.03660277252342564, test loss:0.3328126951392735\n",
      "train loss:0.03658348382922577, test loss:0.3327303280058567\n",
      "train loss:0.03656426387615473, test loss:0.3326499296145445\n",
      "train loss:0.0365450211679237, test loss:0.33256998413916916\n",
      "train loss:0.03650656300690457, test loss:0.33240932111910976\n",
      "train loss:0.03648732709486446, test loss:0.3323290690300838\n",
      "train loss:0.03646815277754959, test loss:0.33225006699731224\n",
      "train loss:0.03644895590101212, test loss:0.3321702620873523\n",
      "train loss:0.036429807721464355, test loss:0.3320918958711643\n",
      "train loss:0.036410649997101904, test loss:0.3320117437590793\n",
      "train loss:0.03639151560544207, test loss:0.33193306177773535\n",
      "train loss:0.03637233590192587, test loss:0.3318520866930177\n",
      "train loss:0.03635326323417452, test loss:0.3317718138842914\n",
      "train loss:0.036315057012303135, test loss:0.3316110857801388\n",
      "train loss:0.03629596982491458, test loss:0.3315300738593401\n",
      "train loss:0.03627690085978017, test loss:0.331449806873981\n",
      "train loss:0.03625784856516666, test loss:0.3313704559213578\n",
      "train loss:0.03623883817955665, test loss:0.33129186420050993\n",
      "train loss:0.036219816908275096, test loss:0.3312126829742579\n",
      "train loss:0.036200808513993404, test loss:0.33113529844205397\n",
      "train loss:0.03618179716560588, test loss:0.3310571107617702\n",
      "train loss:0.036162817958496546, test loss:0.33098010197089794\n",
      "train loss:0.036124889532666994, test loss:0.3308240001981172\n",
      "train loss:0.036105935530869854, test loss:0.330746061735233\n",
      "train loss:0.03608701448444856, test loss:0.3306694776367\n",
      "train loss:0.036068069428163485, test loss:0.33059334084703795\n",
      "train loss:0.03604916113882107, test loss:0.33051853164443873\n",
      "train loss:0.036030269433994566, test loss:0.3304432197410147\n",
      "train loss:0.036011343339494586, test loss:0.33036873875951034\n",
      "train loss:0.03599249310521479, test loss:0.3302937569221535\n",
      "train loss:0.03597363629474294, test loss:0.3302184705195771\n",
      "train loss:0.03593596341375272, test loss:0.3300693588232845\n",
      "train loss:0.03591709482905135, test loss:0.3299941344457985\n",
      "train loss:0.03589832642807325, test loss:0.3299177324430241\n",
      "train loss:0.035879487456948564, test loss:0.3298413681803244\n",
      "train loss:0.03586073143874506, test loss:0.3297643151090055\n",
      "train loss:0.035841927531648776, test loss:0.32968672276287175\n",
      "train loss:0.03582317688780396, test loss:0.32960828422934657\n",
      "train loss:0.03580443878852789, test loss:0.3295304808674362\n",
      "train loss:0.03578570890610992, test loss:0.3294523872431897\n",
      "train loss:0.035748268848960284, test loss:0.3292962396519251\n",
      "train loss:0.03572960005410374, test loss:0.32921659418442756\n",
      "train loss:0.035710920223290886, test loss:0.32913773037053484\n",
      "train loss:0.035692233815153906, test loss:0.329058651796746\n",
      "train loss:0.035673601925679296, test loss:0.32897942385220846\n",
      "train loss:0.035654940784657384, test loss:0.3289006622955628\n",
      "train loss:0.03563633110396793, test loss:0.3288204648972774\n",
      "train loss:0.03561774745850559, test loss:0.3287406120526047\n",
      "train loss:0.035599156432096775, test loss:0.3286618540858168\n",
      "train loss:0.03556202821248634, test loss:0.32850469086879164\n",
      "train loss:0.03554348238439106, test loss:0.32842641696862146\n",
      "train loss:0.035524980736536683, test loss:0.3283491191577412\n",
      "train loss:0.0355064380344582, test loss:0.32827297494974533\n",
      "train loss:0.03548796837692424, test loss:0.3281964709342991\n",
      "train loss:0.03546946609678484, test loss:0.32811980219704706\n",
      "train loss:0.03545100440113828, test loss:0.328043188697835\n",
      "train loss:0.03543256759514565, test loss:0.32796751619615466\n",
      "train loss:0.03541410858873183, test loss:0.32788989381609057\n",
      "train loss:0.035377242051495464, test loss:0.3277384423100155\n",
      "train loss:0.03535882446437049, test loss:0.3276641249393914\n",
      "train loss:0.0353404702308315, test loss:0.3275902970942699\n",
      "train loss:0.03532205448790535, test loss:0.3275156286790355\n",
      "train loss:0.03530369892992157, test loss:0.32744194829222195\n",
      "train loss:0.03528532773410264, test loss:0.32736816949881375\n",
      "train loss:0.03526698827976852, test loss:0.32729487140803337\n",
      "train loss:0.03524869704903555, test loss:0.32722250679715226\n",
      "train loss:0.03523037732148496, test loss:0.32714927234760327\n",
      "train loss:0.03519375751664823, test loss:0.3270063317308815\n",
      "train loss:0.03517548715707315, test loss:0.3269343241149805\n",
      "train loss:0.035157228513062785, test loss:0.32686206346236085\n",
      "train loss:0.035138977823239693, test loss:0.32678911138071426\n",
      "train loss:0.03512070557560051, test loss:0.3267167378488029\n",
      "train loss:0.0351024843247731, test loss:0.32664347219000067\n",
      "train loss:0.035084232800056146, test loss:0.32656839839858987\n",
      "train loss:0.03506607484812781, test loss:0.3264943805739917\n",
      "train loss:0.03504781619593984, test loss:0.32642047378414313\n",
      "train loss:0.035011476225334515, test loss:0.3262744487479939\n",
      "train loss:0.03499328095486702, test loss:0.32620249515645405\n",
      "train loss:0.034975127759675484, test loss:0.326130337217287\n",
      "train loss:0.03495697248337087, test loss:0.3260578895884635\n",
      "train loss:0.034938837988723354, test loss:0.32598592979898994\n",
      "train loss:0.03492071251794748, test loss:0.3259145125925002\n",
      "train loss:0.034902593219929307, test loss:0.3258435423909503\n",
      "train loss:0.0348844862872, test loss:0.32577272968901594\n",
      "train loss:0.03486639593372593, test loss:0.3257008786571874\n",
      "train loss:0.03483022358231745, test loss:0.3255572175712623\n",
      "train loss:0.03481217102353384, test loss:0.3254837260621701\n",
      "train loss:0.03479409371809549, test loss:0.3254117996740141\n",
      "train loss:0.0347760782040855, test loss:0.3253381664837544\n",
      "train loss:0.03475804966805857, test loss:0.32526493111390903\n",
      "train loss:0.03474003658559468, test loss:0.3251917637308044\n",
      "train loss:0.03472201723345171, test loss:0.32511818490796673\n",
      "train loss:0.034704003935870704, test loss:0.3250445628880597\n",
      "train loss:0.034686039670644195, test loss:0.32496977043196806\n",
      "train loss:0.03465013277451267, test loss:0.32482174733257135\n",
      "train loss:0.034632147171442015, test loss:0.3247490047869286\n",
      "train loss:0.03461420540149579, test loss:0.3246748443339292\n",
      "train loss:0.03459629158944131, test loss:0.3246016369605198\n",
      "train loss:0.034578357626075536, test loss:0.32452953021230485\n",
      "train loss:0.034560478163260785, test loss:0.32445694731860214\n",
      "train loss:0.034542565431973385, test loss:0.32438374999998815\n",
      "train loss:0.03452473591318681, test loss:0.32431058450386896\n",
      "train loss:0.03450684549574417, test loss:0.32423647312201637\n",
      "train loss:0.03447109801481776, test loss:0.3240905900687924\n",
      "train loss:0.034453257577551945, test loss:0.3240180766342\n",
      "train loss:0.03443544066269168, test loss:0.3239438699671621\n",
      "train loss:0.034417610485449696, test loss:0.32387015590724727\n",
      "train loss:0.03439979134062885, test loss:0.3237952916568229\n",
      "train loss:0.03438197664494306, test loss:0.3237204617203565\n",
      "train loss:0.034364154688047197, test loss:0.3236447747596026\n",
      "train loss:0.034346384732823795, test loss:0.32357008562662093\n",
      "train loss:0.034328572738001106, test loss:0.3234956697982777\n",
      "train loss:0.0342930414229534, test loss:0.3233478479048457\n",
      "train loss:0.03427526402178924, test loss:0.3232751181943104\n",
      "train loss:0.03425754581001233, test loss:0.32320201500080015\n",
      "train loss:0.03423979069348942, test loss:0.32312983368629095\n",
      "train loss:0.03422207391216452, test loss:0.3230566043549335\n",
      "train loss:0.03420437286349028, test loss:0.3229845310160621\n",
      "train loss:0.03418666408116187, test loss:0.32291339660731677\n",
      "train loss:0.0341689919465091, test loss:0.3228408471037733\n",
      "train loss:0.03415131908185058, test loss:0.3227707658399497\n",
      "train loss:0.0341159581987042, test loss:0.3226302177946311\n",
      "train loss:0.034098316059849525, test loss:0.32256015655073145\n",
      "train loss:0.03408067455343118, test loss:0.3224909163472456\n",
      "train loss:0.03406304580867246, test loss:0.32242140409745745\n",
      "train loss:0.0340454356314896, test loss:0.32235250381030456\n",
      "train loss:0.03402780803359297, test loss:0.3222833256251794\n",
      "train loss:0.034010216166044445, test loss:0.3222150286431748\n",
      "train loss:0.0339926454201054, test loss:0.3221471743891396\n",
      "train loss:0.03397506739054564, test loss:0.32207902958357404\n",
      "train loss:0.03393991892945674, test loss:0.32194220023888326\n",
      "train loss:0.033922365278647656, test loss:0.3218751671438803\n",
      "train loss:0.03390482141673916, test loss:0.3218071409368144\n",
      "train loss:0.033887278899682724, test loss:0.3217400732642493\n",
      "train loss:0.03386977865104205, test loss:0.3216729087553921\n",
      "train loss:0.03385225639304678, test loss:0.32160600316535704\n",
      "train loss:0.03383477028581611, test loss:0.3215382605553263\n",
      "train loss:0.03381729115771618, test loss:0.32146978965248496\n",
      "train loss:0.03379983073618705, test loss:0.32140037733473176\n",
      "train loss:0.03376488861940199, test loss:0.321259190114125\n",
      "train loss:0.03374744529473958, test loss:0.3211871652931721\n",
      "train loss:0.033730004862161427, test loss:0.3211151733542676\n",
      "train loss:0.03371258526519667, test loss:0.3210439889897645\n",
      "train loss:0.033695176633762355, test loss:0.3209720871615856\n",
      "train loss:0.03367777244401757, test loss:0.32089974060897036\n",
      "train loss:0.03366038891300901, test loss:0.32082748104049585\n",
      "train loss:0.033643023499431456, test loss:0.3207553321170251\n",
      "train loss:0.033625654079440064, test loss:0.32068274263124413\n",
      "train loss:0.03359095395463721, test loss:0.3205366529745026\n",
      "train loss:0.033573622012305696, test loss:0.32046321904464825\n",
      "train loss:0.0335563112754401, test loss:0.3203890581911201\n",
      "train loss:0.033539011228466024, test loss:0.32031629959453617\n",
      "train loss:0.03352171856720108, test loss:0.32024374373389425\n",
      "train loss:0.033504451944194776, test loss:0.3201721848411801\n",
      "train loss:0.03348718256248161, test loss:0.3200998839296421\n",
      "train loss:0.0334698888194575, test loss:0.32002703806221733\n",
      "train loss:0.03345265277100644, test loss:0.31995282915475143\n",
      "train loss:0.03341818977423225, test loss:0.31980712922156673\n",
      "train loss:0.03340098696837528, test loss:0.3197333485894852\n",
      "train loss:0.033383786855047316, test loss:0.31966034458378845\n",
      "train loss:0.03336657759061353, test loss:0.3195862416736451\n",
      "train loss:0.03334939896656324, test loss:0.3195130008103085\n",
      "train loss:0.033332206179635754, test loss:0.3194401909936026\n",
      "train loss:0.03331504702921979, test loss:0.3193669227404133\n",
      "train loss:0.03329791579039154, test loss:0.31929367961265454\n",
      "train loss:0.033280736887528195, test loss:0.3192197607574833\n",
      "train loss:0.03324649021926303, test loss:0.3190736526850305\n",
      "train loss:0.03322940091984931, test loss:0.3190018188218232\n",
      "train loss:0.033212276650224234, test loss:0.3189293499237464\n",
      "train loss:0.03319519348081969, test loss:0.31885639079869016\n",
      "train loss:0.03317812035603235, test loss:0.3187853057969152\n",
      "train loss:0.03316106637845602, test loss:0.3187136794793137\n",
      "train loss:0.0331440018172016, test loss:0.3186413753243051\n",
      "train loss:0.03312698326420884, test loss:0.3185701065488003\n",
      "train loss:0.0331099418382809, test loss:0.31849777489957565\n",
      "train loss:0.0330759028284632, test loss:0.31835512184916537\n",
      "train loss:0.033058908008741034, test loss:0.3182850418022752\n",
      "train loss:0.03304193846280058, test loss:0.3182151180702353\n",
      "train loss:0.033024938859666765, test loss:0.31814576528403765\n",
      "train loss:0.03300800184494999, test loss:0.3180770648228828\n",
      "train loss:0.03299103634765473, test loss:0.3180091216720889\n",
      "train loss:0.0329741127543796, test loss:0.3179416303477739\n",
      "train loss:0.03295714813594451, test loss:0.31787501770158644\n",
      "train loss:0.03294025462124044, test loss:0.3178084856139109\n",
      "train loss:0.032906478494993854, test loss:0.3176751389636542\n",
      "train loss:0.0328896262421188, test loss:0.31760710427520655\n",
      "train loss:0.032872734371327754, test loss:0.3175409641201868\n",
      "train loss:0.032855857547211466, test loss:0.31747264282606596\n",
      "train loss:0.0328390160819033, test loss:0.3174055556639185\n",
      "train loss:0.03282217932676818, test loss:0.3173373641505292\n",
      "train loss:0.03280536650138328, test loss:0.31726999449000143\n",
      "train loss:0.03278857558433026, test loss:0.31720219730179366\n",
      "train loss:0.03277175902294235, test loss:0.31713473386389895\n",
      "train loss:0.03273819781598631, test loss:0.3170010542501189\n",
      "train loss:0.032721387404513344, test loss:0.3169343332806234\n",
      "train loss:0.03270463872293302, test loss:0.31686725444842856\n",
      "train loss:0.03268788062727526, test loss:0.31680159284179144\n",
      "train loss:0.032671124138470085, test loss:0.3167345793421203\n",
      "train loss:0.032654415487227265, test loss:0.31666969476508533\n",
      "train loss:0.032637644614416685, test loss:0.31660428085358017\n",
      "train loss:0.03262093118240813, test loss:0.3165388986559167\n",
      "train loss:0.032604222185718894, test loss:0.31647393537171226\n",
      "train loss:0.03257083219073424, test loss:0.31634404554014967\n",
      "train loss:0.032554150628796893, test loss:0.3162797795987358\n",
      "train loss:0.03253749661960012, test loss:0.31621659193311313\n",
      "train loss:0.03252082342411989, test loss:0.3161520274135974\n",
      "train loss:0.03250419383817096, test loss:0.3160898565142837\n",
      "train loss:0.03248754326685671, test loss:0.316025450406404\n",
      "train loss:0.03247089610614983, test loss:0.31596146813537956\n",
      "train loss:0.0324542947847646, test loss:0.315897834793632\n",
      "train loss:0.03243768578819201, test loss:0.31583304098751136\n",
      "train loss:0.03240451493701851, test loss:0.3157025297052635\n",
      "train loss:0.032387954860767856, test loss:0.31563657542904644\n",
      "train loss:0.032371391149133726, test loss:0.31557086997781136\n",
      "train loss:0.03235481844815273, test loss:0.3155059777938035\n",
      "train loss:0.03233825914992413, test loss:0.31544165712253\n",
      "train loss:0.03232172970389197, test loss:0.31537689060020196\n",
      "train loss:0.032305212159261784, test loss:0.31531151442615163\n",
      "train loss:0.032288730295117876, test loss:0.31524727108506834\n",
      "train loss:0.03227219208160769, test loss:0.31518222287121767\n",
      "train loss:0.032239246783451574, test loss:0.31505313618036607\n",
      "train loss:0.03222277361875769, test loss:0.3149884350238225\n",
      "train loss:0.03220634163996324, test loss:0.3149243035185756\n",
      "train loss:0.03218986211138695, test loss:0.3148591778078199\n",
      "train loss:0.03217341950449153, test loss:0.31479575911239516\n",
      "train loss:0.03215700476526398, test loss:0.3147307230380819\n",
      "train loss:0.03214056208405651, test loss:0.3146665229197911\n",
      "train loss:0.03212416959029846, test loss:0.31460158666997856\n",
      "train loss:0.03210775474364595, test loss:0.31453513391872245\n",
      "train loss:0.03207502870713081, test loss:0.3144016216006706\n",
      "train loss:0.0320586425968777, test loss:0.31433519760711787\n",
      "train loss:0.03204230047368462, test loss:0.31426850323382916\n",
      "train loss:0.03202596462465876, test loss:0.3142013934371883\n",
      "train loss:0.03200965169837077, test loss:0.314133714022566\n",
      "train loss:0.03199333399050807, test loss:0.31406537963230974\n",
      "train loss:0.03197701620942815, test loss:0.31399840542692725\n",
      "train loss:0.03196072244887118, test loss:0.31393116306035435\n",
      "train loss:0.03194446292152521, test loss:0.3138629045067622\n",
      "train loss:0.03191193058225104, test loss:0.3137288340026698\n",
      "train loss:0.03189570528765661, test loss:0.3136611694291664\n",
      "train loss:0.03187944991652222, test loss:0.3135930571725177\n",
      "train loss:0.031863233373285386, test loss:0.31352453655114565\n",
      "train loss:0.031846986463659036, test loss:0.31345581293449964\n",
      "train loss:0.03183080218915948, test loss:0.3133857640472267\n",
      "train loss:0.0318145807561281, test loss:0.3133158618807253\n",
      "train loss:0.031798406355764, test loss:0.3132461762841969\n",
      "train loss:0.03178217764402616, test loss:0.3131749162124235\n",
      "train loss:0.03174984566256143, test loss:0.3130332152657535\n",
      "train loss:0.031733702625574185, test loss:0.31296191419889974\n",
      "train loss:0.03171754609393913, test loss:0.3128913117822971\n",
      "train loss:0.031701386879495745, test loss:0.312819603008148\n",
      "train loss:0.0316852497754811, test loss:0.3127473500542134\n",
      "train loss:0.0316691238729882, test loss:0.3126767574858988\n",
      "train loss:0.031653025292679896, test loss:0.31260595937735\n",
      "train loss:0.031636916102230794, test loss:0.31253463236918433\n",
      "train loss:0.031620836359456325, test loss:0.3124643688338507\n",
      "train loss:0.031588661694555924, test loss:0.31232313161345654\n",
      "train loss:0.031572584104879675, test loss:0.31225327634230365\n",
      "train loss:0.03155654322344971, test loss:0.312184656973272\n",
      "train loss:0.03154051752767125, test loss:0.31211654866319777\n",
      "train loss:0.031524473443752705, test loss:0.31204820444593817\n",
      "train loss:0.031508464778563026, test loss:0.31197999573555435\n",
      "train loss:0.03149243706968536, test loss:0.31191105325222607\n",
      "train loss:0.03147644964663786, test loss:0.3118438186112072\n",
      "train loss:0.03146044639429564, test loss:0.3117754253227511\n",
      "train loss:0.03142849791327713, test loss:0.311637484615687\n",
      "train loss:0.03141250339198101, test loss:0.3115682948429851\n",
      "train loss:0.0313965606864341, test loss:0.3114985854602527\n",
      "train loss:0.0313806125923221, test loss:0.3114295020303995\n",
      "train loss:0.03136466974205665, test loss:0.3113606859254501\n",
      "train loss:0.03134874335772114, test loss:0.31129229607260694\n",
      "train loss:0.03133285199757008, test loss:0.31122355068197777\n",
      "train loss:0.031316944883244074, test loss:0.3111543499416208\n",
      "train loss:0.03130105650404268, test loss:0.31108658731485667\n",
      "train loss:0.031269275398246144, test loss:0.31094891356556836\n",
      "train loss:0.0312534400273367, test loss:0.31088145662710653\n",
      "train loss:0.031237589151267526, test loss:0.3108141341930193\n",
      "train loss:0.03122175240087314, test loss:0.3107466479104221\n",
      "train loss:0.031205927163685514, test loss:0.3106808146234936\n",
      "train loss:0.0311901049214874, test loss:0.31061440772536564\n",
      "train loss:0.031174308688524985, test loss:0.3105485812677038\n",
      "train loss:0.031158518684424763, test loss:0.3104828452896196\n",
      "train loss:0.031142735751757872, test loss:0.31041618104322843\n",
      "train loss:0.03111118663237966, test loss:0.31028208547670955\n",
      "train loss:0.031095431951298713, test loss:0.3102160892158053\n",
      "train loss:0.031079691499932234, test loss:0.3101496840365446\n",
      "train loss:0.03106396979967067, test loss:0.3100833456150807\n",
      "train loss:0.031048302598147694, test loss:0.3100173033115681\n",
      "train loss:0.031032571882861928, test loss:0.30995291455762225\n",
      "train loss:0.031016904359343074, test loss:0.3098887223667766\n",
      "train loss:0.031001222965375566, test loss:0.30982390043583585\n",
      "train loss:0.03098554965652714, test loss:0.30975951633996807\n",
      "train loss:0.030954265319926004, test loss:0.30963293286199745\n",
      "train loss:0.030938654127515664, test loss:0.30956841742969093\n",
      "train loss:0.030923008382240363, test loss:0.3095046140999772\n",
      "train loss:0.03090741584595477, test loss:0.30944273419483087\n",
      "train loss:0.03089180691554279, test loss:0.30938008171701775\n",
      "train loss:0.03087622459452432, test loss:0.3093167009273415\n",
      "train loss:0.03086065669434981, test loss:0.30925433089536647\n",
      "train loss:0.030845093108584874, test loss:0.3091903438007272\n",
      "train loss:0.030829517503822224, test loss:0.3091280278831186\n",
      "train loss:0.03079845944097362, test loss:0.30899919173539925\n",
      "train loss:0.030782911727231885, test loss:0.30893620688012763\n",
      "train loss:0.030767426425463516, test loss:0.30887140384342743\n",
      "train loss:0.030751928707641107, test loss:0.3088062938683611\n",
      "train loss:0.030736423494489174, test loss:0.3087410073396612\n",
      "train loss:0.03072094104232026, test loss:0.30867585892999605\n",
      "train loss:0.030705474758859205, test loss:0.3086109973606185\n",
      "train loss:0.030690029484596933, test loss:0.3085465279303149\n",
      "train loss:0.03067457723654171, test loss:0.308481494661553\n",
      "train loss:0.0306437345444402, test loss:0.3083523422505346\n",
      "train loss:0.030628323019453993, test loss:0.3082885648650622\n",
      "train loss:0.030612918952960023, test loss:0.3082230797588745\n",
      "train loss:0.0305975072290468, test loss:0.30815883368681274\n",
      "train loss:0.030582144186258896, test loss:0.30809439016454915\n",
      "train loss:0.030566791301031973, test loss:0.30803030781080076\n",
      "train loss:0.030551432586415184, test loss:0.30796618525817754\n",
      "train loss:0.030536078449870743, test loss:0.3079024592851191\n",
      "train loss:0.03052075278433472, test loss:0.30783745672126883\n",
      "train loss:0.030490114788214576, test loss:0.30770704552507017\n",
      "train loss:0.030474810601829133, test loss:0.3076436803328965\n",
      "train loss:0.030459520532299868, test loss:0.30757935509268647\n",
      "train loss:0.030444235500923712, test loss:0.3075146532274376\n",
      "train loss:0.030428953175589194, test loss:0.3074504915060659\n",
      "train loss:0.030413680586233557, test loss:0.30738537566301766\n",
      "train loss:0.03039843718568489, test loss:0.307320839215984\n",
      "train loss:0.030383180119499174, test loss:0.3072559206867848\n",
      "train loss:0.030367949217595197, test loss:0.307190216421185\n",
      "train loss:0.03033751345434879, test loss:0.307059342916037\n",
      "train loss:0.030322314673489163, test loss:0.3069933147166806\n",
      "train loss:0.030307097072380215, test loss:0.30692650667529253\n",
      "train loss:0.030291918810606413, test loss:0.3068594783851501\n",
      "train loss:0.030276709533454405, test loss:0.3067922795188519\n",
      "train loss:0.03026154367921249, test loss:0.3067253170839143\n",
      "train loss:0.030246368858411372, test loss:0.3066592899257221\n",
      "train loss:0.03023123036415834, test loss:0.3065930677251198\n",
      "train loss:0.030216090665981916, test loss:0.30652749219110814\n",
      "train loss:0.030185866219404625, test loss:0.30639660824406806\n",
      "train loss:0.030170710776914352, test loss:0.30633259268486285\n",
      "train loss:0.030155655426571445, test loss:0.30626878207081354\n",
      "train loss:0.030140539636731734, test loss:0.3062044809473259\n",
      "train loss:0.030125463312496183, test loss:0.30614104006977105\n",
      "train loss:0.03011038421468843, test loss:0.3060762518313633\n",
      "train loss:0.030095333138140105, test loss:0.3060115794869526\n",
      "train loss:0.030080287065987026, test loss:0.305947808387054\n",
      "train loss:0.030065238552559548, test loss:0.3058820239848172\n",
      "train loss:0.030035148555698294, test loss:0.30575075495679305\n",
      "train loss:0.030020157228288363, test loss:0.30568453221808717\n",
      "train loss:0.030005142090471306, test loss:0.30561994282406596\n",
      "train loss:0.029990122019724993, test loss:0.3055542067441802\n",
      "train loss:0.029975125034985634, test loss:0.3054892401773659\n",
      "train loss:0.029960141126968664, test loss:0.3054240678936059\n",
      "train loss:0.029945149936770398, test loss:0.3053590874128494\n",
      "train loss:0.029930175931486516, test loss:0.30529489365140505\n",
      "train loss:0.029915201605551837, test loss:0.30523251734737056\n",
      "train loss:0.029885333204359696, test loss:0.3051086319769355\n",
      "train loss:0.02987040585794264, test loss:0.3050465935516137\n",
      "train loss:0.029855498615561013, test loss:0.3049849274407795\n",
      "train loss:0.029840563985797992, test loss:0.304923402070073\n",
      "train loss:0.029825690491787276, test loss:0.30486188761869565\n",
      "train loss:0.029810768836476004, test loss:0.30480216424255535\n",
      "train loss:0.02979589865173822, test loss:0.30474247591684\n",
      "train loss:0.029780990798219248, test loss:0.304682437022034\n",
      "train loss:0.029766121179452616, test loss:0.30462380308661946\n",
      "train loss:0.029736418494005785, test loss:0.304507915188962\n",
      "train loss:0.02972158147739792, test loss:0.3044501699804606\n",
      "train loss:0.029706748562943378, test loss:0.30439261778350774\n",
      "train loss:0.029691911714028657, test loss:0.30433529377968366\n",
      "train loss:0.029677088629167053, test loss:0.3042788513058282\n",
      "train loss:0.02966229090382424, test loss:0.3042230122191008\n",
      "train loss:0.029647465230459077, test loss:0.3041659989172182\n",
      "train loss:0.029632666490217986, test loss:0.30410959237402135\n",
      "train loss:0.029617884339900657, test loss:0.3040526832641603\n",
      "train loss:0.02958831180096038, test loss:0.30393884440587704\n",
      "train loss:0.029573554637631748, test loss:0.30388072088162227\n",
      "train loss:0.029558793939043645, test loss:0.30382348453175884\n",
      "train loss:0.02954403853674683, test loss:0.3037664526679322\n",
      "train loss:0.02952928617714639, test loss:0.3037102155430354\n",
      "train loss:0.029514541547609578, test loss:0.30365273931511627\n",
      "train loss:0.0294998246640851, test loss:0.3035947945291614\n",
      "train loss:0.029485097922609945, test loss:0.3035366812380854\n",
      "train loss:0.029470408283706076, test loss:0.30347877170043125\n",
      "train loss:0.029441018783463727, test loss:0.30336295900698407\n",
      "train loss:0.029426300562953096, test loss:0.3033043984811769\n",
      "train loss:0.029411645555310315, test loss:0.303246697760007\n",
      "train loss:0.02939698666626414, test loss:0.303188805740802\n",
      "train loss:0.02938233779684843, test loss:0.30312919559597595\n",
      "train loss:0.029367664552299902, test loss:0.30307194705000146\n",
      "train loss:0.02935301891341024, test loss:0.30301209820999864\n",
      "train loss:0.02933839766699578, test loss:0.3029512228117869\n",
      "train loss:0.029323782080563163, test loss:0.30289022179338315\n",
      "train loss:0.029294585218819606, test loss:0.3027667858263778\n",
      "train loss:0.029279976559115507, test loss:0.3027060419167977\n",
      "train loss:0.02926539042896123, test loss:0.30264395467584365\n",
      "train loss:0.029250793679876656, test loss:0.30258242575368716\n",
      "train loss:0.029236233139904467, test loss:0.3025200475641534\n",
      "train loss:0.02922166272420404, test loss:0.30245947013379043\n",
      "train loss:0.029207127181095364, test loss:0.3023972270076595\n",
      "train loss:0.02919258444723161, test loss:0.3023347378594766\n",
      "train loss:0.02917804623024658, test loss:0.3022727319593112\n",
      "train loss:0.029148993102735738, test loss:0.3021477825106059\n",
      "train loss:0.029134504617827878, test loss:0.3020859628224492\n",
      "train loss:0.029119998224043757, test loss:0.3020244650132116\n",
      "train loss:0.02910551066169063, test loss:0.30196252131701146\n",
      "train loss:0.029091039861581, test loss:0.3019008968980533\n",
      "train loss:0.02907655153767041, test loss:0.3018395860694237\n",
      "train loss:0.02906207508610874, test loss:0.3017786048106131\n",
      "train loss:0.02904761639887168, test loss:0.3017176825069698\n",
      "train loss:0.029033192261205334, test loss:0.3016563368814407\n",
      "train loss:0.029004322593256218, test loss:0.30153441142192694\n",
      "train loss:0.028989875693105845, test loss:0.30147383535370703\n",
      "train loss:0.028975466620749155, test loss:0.30141265217323454\n",
      "train loss:0.02896103605347692, test loss:0.30135267588899994\n",
      "train loss:0.028946650042614857, test loss:0.3012915124114564\n",
      "train loss:0.0289322934455304, test loss:0.3012315293755974\n",
      "train loss:0.028917900986986832, test loss:0.30117189184648846\n",
      "train loss:0.028903502418532987, test loss:0.3011122603703938\n",
      "train loss:0.02888913418465144, test loss:0.301053686806101\n",
      "train loss:0.028860422637794526, test loss:0.30093654596660063\n",
      "train loss:0.028846092817655232, test loss:0.3008782508861603\n",
      "train loss:0.02883169641822238, test loss:0.30082026476466084\n",
      "train loss:0.028817393644190538, test loss:0.30076203229128257\n",
      "train loss:0.028803064408029013, test loss:0.300704498177567\n",
      "train loss:0.02878873045849942, test loss:0.30064719383299054\n",
      "train loss:0.028774428051738122, test loss:0.3005907888116035\n",
      "train loss:0.028760114237066436, test loss:0.3005346320679451\n",
      "train loss:0.02874585479863842, test loss:0.3004780300111952\n",
      "train loss:0.028717271284852013, test loss:0.30036501128722676\n",
      "train loss:0.02870297718319665, test loss:0.300307844759796\n",
      "train loss:0.028688755357662535, test loss:0.3002499396218162\n",
      "train loss:0.028674462855672583, test loss:0.300192884302514\n",
      "train loss:0.02866023049304394, test loss:0.3001350859193714\n",
      "train loss:0.028645977792079846, test loss:0.3000751265823069\n",
      "train loss:0.028631758930743427, test loss:0.30001749050859766\n",
      "train loss:0.028617533997481685, test loss:0.2999597528865174\n",
      "train loss:0.028603328360513127, test loss:0.2999013377624328\n",
      "train loss:0.02857491505940099, test loss:0.29978598594548994\n",
      "train loss:0.028560732388996413, test loss:0.2997288882041551\n",
      "train loss:0.028546533411384283, test loss:0.29967230543481976\n",
      "train loss:0.028532361108188724, test loss:0.2996146924467088\n",
      "train loss:0.028518235130317267, test loss:0.2995574416509078\n",
      "train loss:0.02850402826583741, test loss:0.29949884448777614\n",
      "train loss:0.02848993251842883, test loss:0.29944148553599637\n",
      "train loss:0.02847577828364005, test loss:0.2993825235813803\n",
      "train loss:0.02846162930307961, test loss:0.29932240770754404\n",
      "train loss:0.0284333786159779, test loss:0.2992019694327091\n",
      "train loss:0.02841927510886787, test loss:0.2991408314194653\n",
      "train loss:0.028405177294911817, test loss:0.29907955921321316\n",
      "train loss:0.028391074142166912, test loss:0.2990173168115149\n",
      "train loss:0.028376961361345543, test loss:0.29895575675298863\n",
      "train loss:0.028362868546883437, test loss:0.2988950166179582\n",
      "train loss:0.028348783859617244, test loss:0.29883382493188626\n",
      "train loss:0.028334701738286817, test loss:0.2987738465761704\n",
      "train loss:0.028320677162590416, test loss:0.29871315684523625\n",
      "train loss:0.02829258377047482, test loss:0.29859335393446146\n",
      "train loss:0.028278532520893317, test loss:0.2985337630451711\n",
      "train loss:0.028264477438691458, test loss:0.29847472644449424\n",
      "train loss:0.028250468601576654, test loss:0.2984163432195551\n",
      "train loss:0.028236447943220336, test loss:0.2983584663266859\n",
      "train loss:0.02822245325926995, test loss:0.2983015102860255\n",
      "train loss:0.028208461153191738, test loss:0.29824293560542064\n",
      "train loss:0.02819445621741411, test loss:0.298185532154214\n",
      "train loss:0.028180460559547406, test loss:0.2981274741941988\n",
      "train loss:0.028152513729243465, test loss:0.2980115382658052\n",
      "train loss:0.028138535376438786, test loss:0.29795364629340076\n",
      "train loss:0.028124587567897513, test loss:0.2978942393825857\n",
      "train loss:0.02811064049601616, test loss:0.29783655589246505\n",
      "train loss:0.028096685713242385, test loss:0.2977784102645089\n",
      "train loss:0.02808274431638873, test loss:0.2977189129499583\n",
      "train loss:0.028068788115793376, test loss:0.29765988448069247\n",
      "train loss:0.02805486975649589, test loss:0.2976003566670474\n",
      "train loss:0.02804089848001919, test loss:0.2975416644798779\n",
      "train loss:0.028013101487433512, test loss:0.2974248411271979\n",
      "train loss:0.02799917043040735, test loss:0.2973664928372596\n",
      "train loss:0.027985263158644694, test loss:0.2973070049107133\n",
      "train loss:0.027971363616789268, test loss:0.2972484822949864\n",
      "train loss:0.027957499382461497, test loss:0.29719096518677973\n",
      "train loss:0.027943599160862732, test loss:0.29713164146725357\n",
      "train loss:0.027929750943054235, test loss:0.2970731577027396\n",
      "train loss:0.02791588676595918, test loss:0.29701343152891835\n",
      "train loss:0.027902008412639834, test loss:0.2969535039718904\n",
      "train loss:0.027874341938702602, test loss:0.2968339632286843\n",
      "train loss:0.02786053552488072, test loss:0.296773397359531\n",
      "train loss:0.02784669493633164, test loss:0.296713579832533\n",
      "train loss:0.02783290347154381, test loss:0.2966541361300387\n",
      "train loss:0.02781909084497269, test loss:0.29659504906256895\n",
      "train loss:0.02780528110171091, test loss:0.2965367718723718\n",
      "train loss:0.027791487147333233, test loss:0.2964769656862842\n",
      "train loss:0.027777685601666362, test loss:0.2964185339426414\n",
      "train loss:0.02776391519747466, test loss:0.2963587407281387\n",
      "train loss:0.027736375934518474, test loss:0.29623934645677064\n",
      "train loss:0.027722652484998687, test loss:0.2961794130403346\n",
      "train loss:0.02770886368265493, test loss:0.29611920452725177\n",
      "train loss:0.027695117421168945, test loss:0.2960593723007132\n",
      "train loss:0.02768138363610529, test loss:0.2960004107267381\n",
      "train loss:0.02766767793134581, test loss:0.2959419825510719\n",
      "train loss:0.02765392816404397, test loss:0.2958826981255402\n",
      "train loss:0.02764021751790586, test loss:0.2958238444467432\n",
      "train loss:0.02762650927026757, test loss:0.2957638206194143\n",
      "train loss:0.027599162518442213, test loss:0.2956444715554204\n",
      "train loss:0.027585455569590054, test loss:0.295585439554628\n",
      "train loss:0.027571788338768843, test loss:0.2955260763218347\n",
      "train loss:0.027558118367526325, test loss:0.2954666509620437\n",
      "train loss:0.027544464409004737, test loss:0.29540813260964566\n",
      "train loss:0.027530815464095793, test loss:0.29535002304246577\n",
      "train loss:0.027517174618334642, test loss:0.29529065571871677\n",
      "train loss:0.027503557386393932, test loss:0.295231363403645\n",
      "train loss:0.02748992294729878, test loss:0.29517305062219806\n",
      "train loss:0.027462695571636213, test loss:0.29505326288451494\n",
      "train loss:0.027449122770463966, test loss:0.2949930091033748\n",
      "train loss:0.027435497736433105, test loss:0.2949319769427774\n",
      "train loss:0.027421924685520453, test loss:0.2948701631994478\n",
      "train loss:0.02740836710593804, test loss:0.29480790298569143\n",
      "train loss:0.027394794783146447, test loss:0.2947444304676288\n",
      "train loss:0.027381246073278475, test loss:0.2946810773242353\n",
      "train loss:0.027367688882729713, test loss:0.29461663453060916\n",
      "train loss:0.02735417081172196, test loss:0.2945516233940511\n",
      "train loss:0.027327124141535958, test loss:0.29442352615808937\n",
      "train loss:0.02731359891754321, test loss:0.29436046254895787\n",
      "train loss:0.027300093939221396, test loss:0.29429638672132086\n",
      "train loss:0.02728659718770267, test loss:0.2942338368481323\n",
      "train loss:0.027273109208127824, test loss:0.2941703982477218\n",
      "train loss:0.02725961348294574, test loss:0.29410764209700696\n",
      "train loss:0.027246147127652685, test loss:0.2940453080521783\n",
      "train loss:0.027232682789897832, test loss:0.2939816432842634\n",
      "train loss:0.02721921179189188, test loss:0.29391926590395706\n",
      "train loss:0.027192326063923657, test loss:0.29379440204158663\n",
      "train loss:0.027178900252984618, test loss:0.2937310524815673\n",
      "train loss:0.027165455471644694, test loss:0.2936682436554014\n",
      "train loss:0.02715202104535337, test loss:0.29360501635048597\n",
      "train loss:0.027138618884699525, test loss:0.2935410488467387\n",
      "train loss:0.027125238753536337, test loss:0.2934786655903308\n",
      "train loss:0.027111830566698452, test loss:0.29341608045568524\n",
      "train loss:0.0270984433318009, test loss:0.2933524365145517\n",
      "train loss:0.027085074104041575, test loss:0.29328994612615295\n",
      "train loss:0.027058362943804176, test loss:0.29316410331676146\n",
      "train loss:0.027045013706515426, test loss:0.29310149377440875\n",
      "train loss:0.027031673069036904, test loss:0.2930375447823796\n",
      "train loss:0.027018326898916825, test loss:0.29297588497679905\n",
      "train loss:0.027004997323847035, test loss:0.2929130944974644\n",
      "train loss:0.026991696512809884, test loss:0.2928498046453589\n",
      "train loss:0.026978362130844702, test loss:0.2927879135205739\n",
      "train loss:0.02696508332175911, test loss:0.29272549156496164\n",
      "train loss:0.026951775611253954, test loss:0.29266431979463603\n",
      "train loss:0.026925207693414083, test loss:0.2925434619405295\n",
      "train loss:0.026911955464987074, test loss:0.2924852261246801\n",
      "train loss:0.026898649860996333, test loss:0.292426718796459\n",
      "train loss:0.026885414471730207, test loss:0.2923697422927702\n",
      "train loss:0.0268721921218837, test loss:0.29231281117741625\n",
      "train loss:0.0268589301636291, test loss:0.2922575925387138\n",
      "train loss:0.026845679399285564, test loss:0.29220241399590835\n",
      "train loss:0.02683244970453548, test loss:0.2921470232652506\n",
      "train loss:0.026819239912509948, test loss:0.29209284770653104\n",
      "train loss:0.026792801463852454, test loss:0.2919843245472247\n",
      "train loss:0.02677961768048364, test loss:0.2919285635369888\n",
      "train loss:0.026766462886199896, test loss:0.2918755757220634\n",
      "train loss:0.02675324497534465, test loss:0.29182138705129634\n",
      "train loss:0.026740068750512276, test loss:0.2917665610165001\n",
      "train loss:0.026726888760619485, test loss:0.2917108750704694\n",
      "train loss:0.026713735585775618, test loss:0.2916538571496267\n",
      "train loss:0.026700591342461735, test loss:0.29159831707145856\n",
      "train loss:0.026687460296470907, test loss:0.29154121775138053\n",
      "train loss:0.02666119225283031, test loss:0.2914277594768872\n",
      "train loss:0.026648110174653312, test loss:0.29136974656006626\n",
      "train loss:0.026634951624877834, test loss:0.29131175970937806\n",
      "train loss:0.026621863206797776, test loss:0.29125406163809664\n",
      "train loss:0.02660875814021878, test loss:0.2911965113451507\n",
      "train loss:0.026595663344596152, test loss:0.2911402513408088\n",
      "train loss:0.026582588327647205, test loss:0.29108370529057737\n",
      "train loss:0.026569511742583377, test loss:0.29102774173063667\n",
      "train loss:0.026556451762923608, test loss:0.29097187838095034\n",
      "train loss:0.026530342010735966, test loss:0.290859942428756\n",
      "train loss:0.026517292703424983, test loss:0.29080399660436257\n",
      "train loss:0.026504252670690204, test loss:0.29074849839218664\n",
      "train loss:0.026491224371729526, test loss:0.290692571235799\n",
      "train loss:0.0264781876723357, test loss:0.29063564545304366\n",
      "train loss:0.026465173603857972, test loss:0.2905797666958589\n",
      "train loss:0.02645217718386981, test loss:0.2905234165386238\n",
      "train loss:0.026439153767426123, test loss:0.2904662270422232\n",
      "train loss:0.02642615614152743, test loss:0.2904098219908486\n",
      "train loss:0.026400167827481256, test loss:0.29029423123452197\n",
      "train loss:0.026387220760994708, test loss:0.2902363762877823\n",
      "train loss:0.026374228788220027, test loss:0.2901781205280338\n",
      "train loss:0.026361304968449788, test loss:0.29012038072419605\n",
      "train loss:0.02634835844098001, test loss:0.2900637747582377\n",
      "train loss:0.02633540901949368, test loss:0.2900054173692257\n",
      "train loss:0.02632246516163082, test loss:0.28994937217924954\n",
      "train loss:0.02630953439512825, test loss:0.28989212678877374\n",
      "train loss:0.02629662962653455, test loss:0.28983543782683885\n",
      "train loss:0.02627080423931928, test loss:0.2897184201685062\n",
      "train loss:0.026257920282371654, test loss:0.28966006971740293\n",
      "train loss:0.02624500511580525, test loss:0.28960024640331095\n",
      "train loss:0.026232129469627115, test loss:0.2895404048288262\n",
      "train loss:0.026219272726956712, test loss:0.28948041256445917\n",
      "train loss:0.02620637577140374, test loss:0.28941962557806644\n",
      "train loss:0.026193532608823053, test loss:0.28935858712454443\n",
      "train loss:0.026180683817047926, test loss:0.28929761820946015\n",
      "train loss:0.026167844362151988, test loss:0.28923557482906537\n",
      "train loss:0.02614216190022744, test loss:0.2891128202874911\n",
      "train loss:0.026129344606222793, test loss:0.2890504617046795\n",
      "train loss:0.02611651804115755, test loss:0.28898887992978683\n",
      "train loss:0.02610373127024474, test loss:0.28892762085251006\n",
      "train loss:0.026090894690711477, test loss:0.2888665075087257\n",
      "train loss:0.026078119100537468, test loss:0.2888055919154544\n",
      "train loss:0.026065310265803544, test loss:0.2887444125181846\n",
      "train loss:0.026052523600899796, test loss:0.2886831509428139\n",
      "train loss:0.026039766625551308, test loss:0.28862112558957187\n",
      "train loss:0.02601420015690918, test loss:0.2884990844734901\n",
      "train loss:0.02600146010955095, test loss:0.2884366755077622\n",
      "train loss:0.02598867632965365, test loss:0.2883757035197292\n",
      "train loss:0.025975958817556762, test loss:0.2883123771049084\n",
      "train loss:0.02596321597400264, test loss:0.28825007926785884\n",
      "train loss:0.02595048482375599, test loss:0.28818691790691126\n",
      "train loss:0.025937760150935747, test loss:0.28812324098705255\n",
      "train loss:0.025925045289316506, test loss:0.2880600890933933\n",
      "train loss:0.025912313799031778, test loss:0.2879973449275584\n",
      "train loss:0.025886893089622283, test loss:0.2878706371411959\n",
      "train loss:0.02587423075877647, test loss:0.28780825479624444\n",
      "train loss:0.025861555971788644, test loss:0.2877450506165062\n",
      "train loss:0.025848876703947886, test loss:0.28768152468211244\n",
      "train loss:0.025836221607947196, test loss:0.2876183044547115\n",
      "train loss:0.025823549807235197, test loss:0.287554494271232\n",
      "train loss:0.02581090478985265, test loss:0.28749116284947895\n",
      "train loss:0.02579825878971527, test loss:0.28742753422341755\n",
      "train loss:0.02578563469239273, test loss:0.2873619259457969\n",
      "train loss:0.025760388996278017, test loss:0.2872325677795577\n",
      "train loss:0.02574777450954398, test loss:0.28716805690372715\n",
      "train loss:0.02573514932151531, test loss:0.287103252743411\n",
      "train loss:0.025722549849457023, test loss:0.28703956172901485\n",
      "train loss:0.025709954310137808, test loss:0.2869756613180892\n",
      "train loss:0.025697385929465504, test loss:0.28691117845514663\n",
      "train loss:0.025684819075875113, test loss:0.28684710442131406\n",
      "train loss:0.02567223374057469, test loss:0.28678490221417885\n",
      "train loss:0.025659688850497256, test loss:0.2867218632982502\n",
      "train loss:0.025634589275689523, test loss:0.2866011546413702\n",
      "train loss:0.025622059573033457, test loss:0.28654084720684264\n",
      "train loss:0.02560951877969926, test loss:0.28648122367006507\n",
      "train loss:0.025597008041859225, test loss:0.2864209484715018\n",
      "train loss:0.025584475923822254, test loss:0.28636156274658114\n",
      "train loss:0.025571962182175834, test loss:0.2863022503599619\n",
      "train loss:0.025559464312119248, test loss:0.2862431894231157\n",
      "train loss:0.025546942896862622, test loss:0.2861842975066287\n",
      "train loss:0.02553445186409954, test loss:0.2861249736428186\n",
      "train loss:0.025509510474352105, test loss:0.2860069584659211\n",
      "train loss:0.02549703084328633, test loss:0.2859477897761268\n",
      "train loss:0.025484561366835953, test loss:0.2858886180714389\n",
      "train loss:0.025472113350200328, test loss:0.2858290329686542\n",
      "train loss:0.025459634718124062, test loss:0.2857691284468248\n",
      "train loss:0.025447182303391713, test loss:0.2857096284955304\n",
      "train loss:0.02543477671787131, test loss:0.28564913689573057\n",
      "train loss:0.025422294107061393, test loss:0.28558778273151975\n",
      "train loss:0.02540988392284643, test loss:0.2855276683112167\n",
      "train loss:0.025385045855471446, test loss:0.285404391655109\n",
      "train loss:0.025372606199383103, test loss:0.28534371106657624\n",
      "train loss:0.025360223087813268, test loss:0.2852821297327428\n",
      "train loss:0.025347804834792962, test loss:0.28522105561430355\n",
      "train loss:0.02533544204189358, test loss:0.2851593924758394\n",
      "train loss:0.02532304910084155, test loss:0.2850979476234681\n",
      "train loss:0.02531068727598072, test loss:0.28503593339124333\n",
      "train loss:0.025298304752926295, test loss:0.2849734666228507\n",
      "train loss:0.025285939483474674, test loss:0.28491199698756026\n",
      "train loss:0.02526123955659917, test loss:0.2847857815430026\n",
      "train loss:0.025248892694363596, test loss:0.28472207480354655\n",
      "train loss:0.025236532774508328, test loss:0.28465828981405644\n",
      "train loss:0.025224235742951108, test loss:0.28459536011718317\n",
      "train loss:0.025211915135153713, test loss:0.28452982965260915\n",
      "train loss:0.025199599484986224, test loss:0.284465904814827\n",
      "train loss:0.025187280092913347, test loss:0.284401333295965\n",
      "train loss:0.0251749736761175, test loss:0.2843362288480743\n",
      "train loss:0.025162688750314462, test loss:0.2842706062133127\n",
      "train loss:0.025138118569241915, test loss:0.2841423991002172\n",
      "train loss:0.025125837058506753, test loss:0.28407680929649054\n",
      "train loss:0.025113572347644532, test loss:0.28401339507917656\n",
      "train loss:0.025101296756737092, test loss:0.28394943649859206\n",
      "train loss:0.025089066689171526, test loss:0.28388592572385307\n",
      "train loss:0.02507683848371769, test loss:0.28382303378526147\n",
      "train loss:0.02506460599086491, test loss:0.28376110310885144\n",
      "train loss:0.025052382001227513, test loss:0.28369729147282446\n",
      "train loss:0.025040144543639432, test loss:0.2836347905947501\n",
      "train loss:0.02501572415004802, test loss:0.2835087906838167\n",
      "train loss:0.025003555115656304, test loss:0.2834452367464743\n",
      "train loss:0.024991339038744082, test loss:0.28338234672138435\n",
      "train loss:0.024979176189803536, test loss:0.28331908564140307\n",
      "train loss:0.024966993521603323, test loss:0.28325745768713817\n",
      "train loss:0.024954808607294137, test loss:0.28319626923519836\n",
      "train loss:0.02494265082785011, test loss:0.2831356445521661\n",
      "train loss:0.024930512565354095, test loss:0.2830752103724091\n",
      "train loss:0.024918362056829905, test loss:0.28301586593602407\n",
      "train loss:0.02489408311342493, test loss:0.28289690294258363\n",
      "train loss:0.024881982587915037, test loss:0.28283747526594566\n",
      "train loss:0.024869858592731442, test loss:0.282777762620408\n",
      "train loss:0.024857759338580053, test loss:0.2827189508402815\n",
      "train loss:0.024845670138397363, test loss:0.28265938457091977\n",
      "train loss:0.02483356752523974, test loss:0.28260034060005224\n",
      "train loss:0.024821470564652625, test loss:0.2825418368505798\n",
      "train loss:0.02480940651021107, test loss:0.2824829666277481\n",
      "train loss:0.02479735252622631, test loss:0.28242437424938627\n",
      "train loss:0.02477323472429859, test loss:0.28230896785067333\n",
      "train loss:0.024761173034336566, test loss:0.2822507684984477\n",
      "train loss:0.02474912354581069, test loss:0.28219299380780555\n",
      "train loss:0.024737122538625887, test loss:0.28213639229041715\n",
      "train loss:0.02472511043343813, test loss:0.282079661624851\n",
      "train loss:0.024713073602329827, test loss:0.2820231883653223\n",
      "train loss:0.024701068400331143, test loss:0.28196713582040694\n",
      "train loss:0.024689062539296206, test loss:0.2819106192051258\n",
      "train loss:0.024677062788839653, test loss:0.28185473397656285\n"
>>>>>>> Added gradient descent for fine tuning model for mod q
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "tensor(11.6124, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
||||||| constructed merge base
     "data": {
      "text/plain": [
       "tensor(11.5251, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
=======
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25000\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         run_batch()\n",
      "Cell \u001b[0;32mIn[111], line 6\u001b[0m, in \u001b[0;36mrun_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m train_y_preds \u001b[38;5;241m=\u001b[39m clock(train_dataset)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43mclock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m,test_labels)\n\u001b[1;32m      7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(train_y_preds, train_labels)\n\u001b[1;32m      9\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gbmi-SjKVSTWC-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gbmi-SjKVSTWC-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[101], line 10\u001b[0m, in \u001b[0;36mDifferentModClock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 10\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     12\u001b[0m     y \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_e \u001b[38;5;241m@\u001b[39m embedding_matrix\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
>>>>>>> Added gradient descent for fine tuning model for mod q
    }
   ],
   "source": [
    "for l in range(25000):\n",
    "\n",
    "    if l%10:\n",
    "        print(run_batch())\n",
    "    else:\n",
    "        run_batch()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6447f23-cd5a-435d-a801-6370b8d6cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
