{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0088943-7b2e-45cd-a7c1-b1462c6e543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672aa963-c548-4760-b059-5679d20092be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "else:\n",
    "    print(\"Not in IPython, not loading autoreload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb450f-b4da-490d-a726-9f0075d8d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "#!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended cm-super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe6529-0c85-450b-89a2-237effba159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: perf cpu instruction counting not available (Failed to start collector.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to open event 1: Operation not permitted.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import traceback\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from functools import partial, reduce\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from typing import (\n",
    "    Any,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Iterator,\n",
    "    Callable,\n",
    ")\n",
    "\n",
    "from gbmi.analysis_tools.decomp import analyze_svd, split_svd_contributions\n",
    "from gbmi.verification_tools.l1h1 import all_EVOU, all_PVOU\n",
    "from gbmi.verification_tools.general import EU_PU\n",
    "from gbmi.exp_max_of_n.verification import LargestWrongLogitQuadraticConfig\n",
    "import gbmi.exp_max_of_n.verification.quadratic as quadratic\n",
    "from gbmi.utils.dataclass import enumerate_dataclass_values\n",
    "from gbmi.utils.memoshelve import memoshelve, uncache as memoshelve_uncache\n",
    "from gbmi.analysis_tools.plot import (\n",
    "    EVOU_max_logit_diff,\n",
    ")\n",
    "from gbmi.exp_max_of_n.plot import (\n",
    "    EVOU_max_minus_diag_logit_diff,\n",
    "    attention_difference_over_gap,\n",
    ")\n",
    "from gbmi.exp_max_of_n.analysis import (\n",
    "    find_second_singular_contributions,\n",
    "    find_size_and_query_direction,\n",
    ")\n",
    "\n",
    "from gbmi.exp_max_of_n.train import (\n",
    "    IterableDatasetCfg,\n",
    "    MaxOfN,\n",
    "    MaxOfNDataModule,\n",
    "    MaxOfNTrainingWrapper,\n",
    "    train_or_load_model,\n",
    ")\n",
    "from gbmi.model import Config\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from pathlib import Path\n",
    "from gbmi.utils import default_device\n",
    "from gbmi.utils.sequences import (\n",
    "    SequenceDataset,\n",
    ")\n",
    "from gbmi.utils.latex_export import (\n",
    "    to_latex_defs,\n",
    "    latex_values_of_counter,\n",
    "    latex_values_of_instruction_count,\n",
    ")\n",
    "from gbmi.utils import default_device, dropnan, shuffle_tensors, shuffle_tensor\n",
    "from gbmi.utils.gc import PeriodicGarbageCollector\n",
    "from gbmi.utils.hashing import get_hash_ascii\n",
    "import gbmi.utils.git as git\n",
    "import gbmi.exp_max_of_n.verification.cubic as cubic\n",
    "import gbmi.exp_max_of_n.verification.subcubic as subcubic\n",
    "import gbmi.exp_max_of_n.analysis.quadratic as analysis_quadratic\n",
    "import gbmi.exp_max_of_n.analysis.subcubic as analysis_subcubic\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import gbmi.utils.ein as ein\n",
    "import gbmi.utils.instructions as instructions\n",
    "from gbmi.utils.instructions import (\n",
    "    InstructionCount,\n",
    "    CountTensor,\n",
    "    PatchTorch,\n",
    "    CountHookedTransformer,\n",
    "    PerfCounter,\n",
    "    PerfCollector,\n",
    "    int_or_value,\n",
    "    CountTensorOperations,\n",
    "    PERF_WORKING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c537d4-4ea2-4e6a-bbf2-a130f5f5677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--seeds\",\n",
    "    type=str,\n",
    "    default=\"50,104,123,519,742,913,1185,1283,1412,1490,1681,1696,1895,1951,2236,2306,2345,2549,2743,2773,3175,3254,3284,4157,4305,4430,4647,4729,4800,4810,5358,5615,5781,5928,6082,6155,6159,6204,6532,6549,6589,6910,7098,7238,7310,7467,7790,7884,8048,8299,8721,8745,8840,8893,9132,9134,9504,9816,10248,11124,11130,11498,11598,11611,12141,12287,12457,12493,12552,12561,13036,13293,13468,13654,13716,14095,14929,15043,15399,15622,15662,16069,16149,16197,16284,17080,17096,17194,17197,18146,18289,18668,19004,19093,19451,19488,19538,19917,20013,20294,20338,20415,20539,20751,20754,20976,21317,21598,22261,22286,22401,22545,23241,23367,23447,23633,23696,24144,24173,24202,24262,24438,24566,25516,26278,26374,26829,26932,27300,27484,27584,27671,27714,28090,28716,28778,29022,29052,29110,29195,29565,29725,29726,30371,30463,30684,30899,31308,32103,32374,32382\",\n",
    "    help=\"Comma-separated list of seeds to use\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-j\", dest=\"n_threads\", type=int, default=1, help=\"number of threads\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-perf\",\n",
    "    action=\"store_const\",\n",
    "    const=True,\n",
    "    default=None,\n",
    "    help=\"Forcibly disable perf\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ignore-csv\",\n",
    "    action=\"store_const\",\n",
    "    const=True,\n",
    "    default=None,\n",
    "    help=\"Recompute seeds that appear in csvs\",\n",
    ")\n",
    "cli_args = parser.parse_args(None if ipython is None else [\"--ignore-csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed6ef9-246d-4d73-81cb-094faf089aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "cache_dir = Path(__file__).parent / \".cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "OVERWRITE_CSV_FROM_CACHE: bool = not cli_args.ignore_csv  # @param {type:\"boolean\"}\n",
    "compute_expensive_average_across_many_models: bool = True  # @param {type:\"boolean\"}\n",
    "TRAIN_CSV_PATH = Path(__file__).with_suffix(\"\") / \"all-models-train-values.csv\"\n",
    "TRAIN_CSV_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "BRUTE_FORCE_CSV_PATH = (\n",
    "    Path(__file__).with_suffix(\"\") / \"all-models-brute-force-values.csv\"\n",
    ")\n",
    "BRUTE_FORCE_CSV_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "CUBIC_CSV_PATH = Path(__file__).with_suffix(\"\") / \"all-models-cubic-values.csv\"\n",
    "CUBIC_CSV_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "SUBCUBIC_CSV_PATH = Path(__file__).with_suffix(\"\") / \"all-models-subcubic-values.csv\"\n",
    "SUBCUBIC_CSV_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "SUBCUBIC_ANALYSIS_CSV_PATH = (\n",
    "    Path(__file__).with_suffix(\"\") / \"all-models-subcubic-analysis-values.csv\"\n",
    ")\n",
    "SUBCUBIC_ANALYSIS_CSV_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "PYTHON_VERSION_PATH = (\n",
    "    Path(__file__).with_suffix(\"\") / \"all-models-values-python-version.txt\"\n",
    ")\n",
    "PYTHON_VERSION_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "TORCH_VERSION_PATH = (\n",
    "    Path(__file__).with_suffix(\"\") / \"all-models-values-torch-version.txt\"\n",
    ")\n",
    "TORCH_VERSION_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "GIT_DIFF_PATH = Path(__file__).with_suffix(\"\") / \"all-models-values-git-diff-info.diff\"\n",
    "GIT_DIFF_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "GIT_SHA_PATH = Path(__file__).with_suffix(\"\") / \"all-models-values-git-sha.txt\"\n",
    "GIT_SHA_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "GIT_SHA_SHORT_PATH = (\n",
    "    Path(__file__).with_suffix(\"\") / \"all-models-values-git-sha-short.txt\"\n",
    ")\n",
    "GIT_SHA_SHORT_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "LATEX_VALUES_PATH = Path(__file__).with_suffix(\"\") / \"all-models-values.tex\"\n",
    "LATEX_VALUES_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "SHARED_CACHE_STEM = Path(__file__).name.replace(\"_all_models\", \"\")\n",
    "N_THREADS: Optional[int] = cli_args.n_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dce982-4ba0-4a34-bf3b-83d70b7189c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "if cli_args.no_perf:\n",
    "    PERF_WORKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d3a7e-8602-4107-a85e-301fad2cd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "latex_values: dict[str, Union[int, float, str]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec626dc-35e2-4e4c-a227-1f3cf59777b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def maybe_parallel_map(func, *args):\n",
    "    if N_THREADS is None or N_THREADS <= 1:\n",
    "        result = list(map(func, *args))\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=N_THREADS) as executor:\n",
    "            result = executor.map(func, *args)\n",
    "    gc.collect()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab923503-f7f1-4efe-b088-e69b89f7f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def data_summary(\n",
    "    data, prefix: str = \"\", float_postfix: str = \"Float\", int_postfix: str = \"\"\n",
    "):\n",
    "    if isinstance(data, dict):\n",
    "        keys = list(data.keys())\n",
    "        values = [data[k] for k in keys]\n",
    "    else:\n",
    "        keys = None\n",
    "        values = data\n",
    "    if isinstance(values, torch.Tensor):\n",
    "        values = values.cpu().numpy()\n",
    "    elif not isinstance(values, np.ndarray):\n",
    "        values = np.array(values)  # turn to float\n",
    "\n",
    "    wf = lambda k: f\"{prefix}{k}{float_postfix}\"\n",
    "\n",
    "    result = {\n",
    "        f\"{prefix}Len{int_postfix}\": len(values.flatten()),\n",
    "        wf(\"Min\"): values.min(),\n",
    "        wf(\"Max\"): values.max(),\n",
    "    }\n",
    "    values = values + 0.0  # floatify\n",
    "    result |= {\n",
    "        wf(\"Mean\"): values.mean(),\n",
    "        wf(\"StdDev\"): values.std(),\n",
    "        wf(\"SqrMean\"): (values**2).mean(),\n",
    "    }\n",
    "\n",
    "    s = twenty_five_percent_in_std_dev = stats.norm.ppf(0.75) * 2\n",
    "    percentiles = stats.norm.cdf([-3 * s, -2 * s, -s, 0, s, 2 * s, 3 * s])\n",
    "    percentile_names = [\n",
    "        \"LowerWhiskerBottomEnd\",\n",
    "        \"LowerWhiskerCrosshatch\",\n",
    "        \"QuartileOne\",\n",
    "        \"Median\",\n",
    "        \"QuartileThree\",\n",
    "        \"UpperWhiskerCrosshatch\",\n",
    "        \"UpperWhiskerTopEnd\",\n",
    "    ]\n",
    "    percentile_values = np.percentile(values, percentiles)\n",
    "\n",
    "    result.update({wf(pct): v for pct, v in zip(percentile_names, percentile_values)})\n",
    "\n",
    "    if keys is not None:\n",
    "        closest_keys = {}\n",
    "\n",
    "        def find_closest_key(value):\n",
    "            return keys[np.argmin(np.abs(values - value))]\n",
    "\n",
    "        closest_keys.update(\n",
    "            {\n",
    "                f\"{prefix}MeanKey\": find_closest_key(values.mean()),\n",
    "                f\"{prefix}MinKey\": find_closest_key(values.min()),\n",
    "                f\"{prefix}MaxKey\": find_closest_key(values.max()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for pct, value in zip(percentile_names, percentile_values):\n",
    "            closest_keys[f\"{prefix}{pct}Key\"] = find_closest_key(value)\n",
    "\n",
    "        result.update(closest_keys)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50711b89-4b89-44df-80c7-2ac76bdcc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lscpu...\n",
      "Running cat-proc-cpuinfo...\n",
      "Running lspci-vga...\n",
      "Running nvidia-smi...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "for name, (args, kwargs) in [\n",
    "    (\"lscpu\", ((\"lscpu\",), {})),\n",
    "    (\"cat-proc-cpuinfo\", ((\"cat\", \"/proc/cpuinfo\"), {})),\n",
    "    (\"lspci-vga\", ((\"lspci | grep -i vga\",), dict(shell=True))),\n",
    "    (\"nvidia-smi\", ((\"nvidia-smi\",), {})),\n",
    "]:\n",
    "    try:\n",
    "        print(f\"Running {name}...\")\n",
    "        result = subprocess.check_output(args, **kwargs).decode()\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {name}: {e}\")\n",
    "    else:\n",
    "        with open(Path(__file__).with_suffix(\"\") / f\"{name}.txt\", \"w\") as f:\n",
    "            f.write(result)\n",
    "\n",
    "with open(GIT_DIFF_PATH, \"w\") as f:\n",
    "    f.write(git.get_diff())\n",
    "\n",
    "with open(GIT_SHA_PATH, \"w\") as f:\n",
    "    f.write(git.get_head_sha(short=False))\n",
    "\n",
    "with open(GIT_SHA_SHORT_PATH, \"w\") as f:\n",
    "    f.write(git.get_head_sha(short=True))\n",
    "\n",
    "with open(PYTHON_VERSION_PATH, \"w\") as f:\n",
    "    f.write(sys.version)\n",
    "\n",
    "with open(TORCH_VERSION_PATH, \"w\") as f:\n",
    "    f.write(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff9515-2777-4a88-8a95-c86caaacf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# hack around newlines of black formatting\n",
    "seeds = (\n",
    "    sorted(set(map(int, cli_args.seeds.split(\",\"))))\n",
    "    if compute_expensive_average_across_many_models\n",
    "    else []\n",
    ")\n",
    "if 123 in seeds:\n",
    "    seeds = [123] + [s for s in seeds if s != 123]\n",
    "cfgs = {\n",
    "    seed: Config(\n",
    "        experiment=MaxOfN(\n",
    "            model_config=HookedTransformerConfig(\n",
    "                act_fn=None,\n",
    "                attn_only=True,\n",
    "                d_head=32,\n",
    "                d_mlp=None,\n",
    "                d_model=32,\n",
    "                d_vocab=64,\n",
    "                device=\"cpu\",\n",
    "                n_ctx=4,\n",
    "                n_heads=1,\n",
    "                n_layers=1,\n",
    "                normalization_type=None,\n",
    "            ),\n",
    "            zero_biases=True,\n",
    "            use_log1p=True,\n",
    "            use_end_of_sequence=False,\n",
    "            seq_len=4,\n",
    "            optimizer=\"AdamW\",\n",
    "            optimizer_kwargs={\"lr\": 0.001, \"betas\": (0.9, 0.999)},\n",
    "            train_dataset_cfg=IterableDatasetCfg(pick_max_first=False),\n",
    "            test_dataset_cfg=IterableDatasetCfg(n_samples=1024),\n",
    "        ),\n",
    "        deterministic=True,\n",
    "        seed=seed,\n",
    "        batch_size=128,\n",
    "        train_for=(3000, \"steps\"),\n",
    "    )\n",
    "    for seed in seeds\n",
    "}\n",
    "cfg_hashes = {seed: get_hash_ascii(cfg) for seed, cfg in cfgs.items()}\n",
    "datamodules = {seed: MaxOfNDataModule(cfg) for seed, cfg in cfgs.items()}\n",
    "cfg_hashes_for_filename = {\n",
    "    seed: f\"{seed}_{cfg_hashes[seed].replace('/', '__SLASH__')}\"\n",
    "    for seed, cfg in cfgs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff86e0-27a2-454d-8a10-03bd11aa092e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348bb297c8204a779cdfbe0d209dd62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "with memoshelve(\n",
    "    train_or_load_model,\n",
    "    filename=cache_dir / f\"{SHARED_CACHE_STEM}.train_or_load_model\",\n",
    "    get_hash=get_hash_ascii,\n",
    ")() as memo_train_or_load_model:\n",
    "    runtime_models = {}\n",
    "\n",
    "    def _handle_memo_train_or_load_model(arg):\n",
    "        seed, cfg = arg\n",
    "        try:\n",
    "            runtime_models[seed] = memo_train_or_load_model(cfg, force=\"load\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model for seed {seed}: {e}\")\n",
    "\n",
    "    maybe_parallel_map(_handle_memo_train_or_load_model, tqdm(cfgs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b1cbf-0431-4740-a70d-8be004dbeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "training_wrappers = {\n",
    "    seed: MaxOfNTrainingWrapper(cfgs[seed], model)\n",
    "    for seed, (_runtime, model) in runtime_models.items()\n",
    "}\n",
    "\n",
    "\n",
    "# training_wrapper.run_batch = Memoize(training_wrapper.run_batch, name=f\"{__file__}.training_wrapper.run_batch\", use_pandas=False, use_shelf=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbea1a-7ef3-47b6-bd46-6cd2ad6fbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def update_csv_with_rows(\n",
    "    csv_path: Path,\n",
    "    new_data: list[dict[str, Union[float, int, str]]],\n",
    "    *,\n",
    "    columns: list[str],\n",
    "    subset: str = \"seed\",\n",
    "):\n",
    "    results = None\n",
    "    if os.path.exists(csv_path):\n",
    "        results = pd.read_csv(csv_path)\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=columns)\n",
    "    if results is None or results.empty:\n",
    "        results = new_df\n",
    "    elif not new_df.empty:\n",
    "        results = pd.concat([results, new_df], ignore_index=True).drop_duplicates(\n",
    "            subset=subset, keep=\"last\"\n",
    "        )\n",
    "    results.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def update_csv(\n",
    "    csv_path: Path,\n",
    "    data: dict[int, dict[str, Union[float, int, str]]],\n",
    "    columns: list[str],\n",
    "    *,\n",
    "    subset: str = \"seed\",\n",
    "):\n",
    "    new_data = [data[seed] for seed in sorted(data.keys())]\n",
    "    update_csv_with_rows(csv_path, new_data, columns=columns, subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa913f-4df9-4e98-8058-53228de09721",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e96bb4-ca85-415e-b4e4-2a1005d6ce83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4cf6a2367445d18d8c3a5da63bad36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches for training:   0%|          | 0/453000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "train_total_loss = {}\n",
    "train_total_accuracy = {}\n",
    "train_total_samples = {}\n",
    "train_measurement_deterministic: bool = False  # @param {type:\"boolean\"}\n",
    "train_average_loss = {}\n",
    "train_average_accuracy = {}\n",
    "\n",
    "\n",
    "# loop for computing overall loss and accuracy\n",
    "@torch.no_grad()\n",
    "def _run_train_batch_loss_accuracy(\n",
    "    seed: int, i: int, batch_size: int, *, dataloader_iter: Iterator\n",
    ") -> Tuple[float, float, int]:\n",
    "    xs, ys = next(dataloader_iter)\n",
    "    device = default_device(deterministic=train_measurement_deterministic)\n",
    "    xs.to(device)\n",
    "    ys.to(device)\n",
    "    loss, accuracy = training_wrappers[seed].run_batch((xs, ys), log_output=False)\n",
    "    loss = loss.item()\n",
    "    return loss, accuracy, batch_size\n",
    "\n",
    "\n",
    "def train_seed(seed: int, *, pbar: tqdm):\n",
    "    train_total_loss[seed] = 0.0\n",
    "    train_total_accuracy[seed] = 0.0\n",
    "    train_total_samples[seed] = 0\n",
    "\n",
    "    datamodule = datamodules[seed]\n",
    "    datamodule.setup(\"train\")\n",
    "    dataloader = datamodule.train_dataloader()\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    with memoshelve(\n",
    "        partial(_run_train_batch_loss_accuracy, dataloader_iter=dataloader_iter),\n",
    "        filename=cache_dir\n",
    "        / f\"{SHARED_CACHE_STEM}.run_batch_loss_accuracy-{cfg_hashes_for_filename[seed]}-{train_measurement_deterministic}\",\n",
    "        get_hash_mem=(lambda x: x[0]),\n",
    "        get_hash=str,\n",
    "    )() as run_batch_loss_accuracy:\n",
    "        for i in range(0, len(dataloader)):\n",
    "            loss, accuracy, size = run_batch_loss_accuracy(seed, i, cfgs[seed].batch_size)  # type: ignore\n",
    "            # Accumulate loss and accuracy\n",
    "            train_total_loss[seed] += loss * size\n",
    "            train_total_accuracy[seed] += accuracy * size\n",
    "            train_total_samples[seed] += size\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    train_average_loss[seed] = train_total_loss[seed] / train_total_samples[seed]\n",
    "    train_average_accuracy[seed] = (\n",
    "        train_total_accuracy[seed] / train_total_samples[seed]\n",
    "    )\n",
    "\n",
    "\n",
    "def _handle_train_seed(seed: int, *, pbar: tqdm):\n",
    "    try:\n",
    "        return train_seed(seed, pbar=pbar)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training seed {seed}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "for datamodule in datamodules.values():\n",
    "    datamodule.setup(\"train\")\n",
    "\n",
    "total_batches = sum(\n",
    "    len(datamodules[seed].train_dataloader()) for seed in runtime_models.keys()\n",
    ")\n",
    "\n",
    "with tqdm(total=total_batches, desc=\"batches for training\", position=0) as pbar:\n",
    "    # with PeriodicGarbageCollector(60):\n",
    "    maybe_parallel_map(\n",
    "        partial(_handle_train_seed, pbar=pbar), sorted(runtime_models.keys())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36c94c-570c-4b2f-a049-baf1149727a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# load csv\n",
    "train_columns = [\"seed\", \"loss\", \"accuracy\", \"model-seed\", \"dataset-seed\"]\n",
    "\n",
    "train_data = {\n",
    "    seed: {\n",
    "        \"seed\": seed,\n",
    "        \"loss\": train_average_loss[seed],\n",
    "        \"accuracy\": train_average_accuracy[seed],\n",
    "        \"model-seed\": cfgs[seed].experiment.model_config.seed,\n",
    "        \"dataset-seed\": datamodules[seed].dataset_seed,\n",
    "    }\n",
    "    for seed in runtime_models.keys()\n",
    "}\n",
    "\n",
    "update_csv(TRAIN_CSV_PATH, train_data, columns=train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb2c1f-bbe2-433e-b5dd-e936adf1b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "num_seeds = len(train_average_loss)\n",
    "avg_train_average_loss = sum(train_average_loss.values()) / num_seeds\n",
    "avg_train_average_accuracy = sum(train_average_accuracy.values()) / num_seeds\n",
    "std_dev_train_average_loss = float(np.std(list(train_average_loss.values())))\n",
    "std_dev_train_average_accuracy = float(np.std(list(train_average_accuracy.values())))\n",
    "latex_values[\"NumSeeds\"] = num_seeds\n",
    "latex_values |= data_summary(train_average_accuracy, prefix=\"TrainAccuracy\")\n",
    "latex_values |= data_summary(train_average_loss, prefix=\"TrainLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07709267-65b1-49aa-a08a-1abbe20b3e4e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Brute Force Proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ac6f9-d74e-4847-8176-f125bf4b1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "all_tokens_datasets = {\n",
    "    seed: SequenceDataset(seq_len=model.cfg.n_ctx, vocab_size=model.cfg.d_vocab)\n",
    "    for seed, (_runtime, model) in runtime_models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577b37d-a4e0-4fd3-9db7-f70b62f91e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3cb600d2af4e86a96b72e0ab68395f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches for brute force: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "brute_force_columns = [\n",
    "    \"seed\",\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"num_correct\",\n",
    "    \"num_incorrect\",\n",
    "    \"cpu\",\n",
    "    \"duration\",\n",
    "]\n",
    "if os.path.exists(BRUTE_FORCE_CSV_PATH):\n",
    "    brute_force_results = pd.read_csv(BRUTE_FORCE_CSV_PATH)\n",
    "else:\n",
    "    brute_force_results = pd.DataFrame(columns=brute_force_columns)\n",
    "\n",
    "brute_force_proof_deterministic: bool = True  # @param {type:\"boolean\"}\n",
    "\n",
    "batch_size = 4096  # 16_384 # 8182\n",
    "\n",
    "all_seeds = set(runtime_models.keys())\n",
    "unknown_seeds = all_seeds - set(brute_force_results[\"seed\"])\n",
    "known_seeds = all_seeds - unknown_seeds\n",
    "relevant_seeds = all_seeds if OVERWRITE_CSV_FROM_CACHE else unknown_seeds\n",
    "brute_force_data = {\n",
    "    seed: brute_force_results[brute_force_results[\"seed\"] == seed].iloc[0].to_dict()\n",
    "    for seed in known_seeds\n",
    "}\n",
    "\n",
    "\n",
    "def get_brute_force_for(seed: int, *, pbar: tqdm):\n",
    "    cfg = cfgs[seed]\n",
    "    cfg_hash = cfg_hashes[seed]\n",
    "    cfg_hash_for_filename = cfg_hashes_for_filename[seed]\n",
    "    runtime, model = runtime_models[seed]\n",
    "    training_wrapper = training_wrappers[seed]\n",
    "    assert cfg.experiment.model_config.seed is not None\n",
    "    all_tokens_dataset = all_tokens_datasets[seed]\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    total_duration = 0.0\n",
    "    # all_incorrect_sequences = []\n",
    "\n",
    "    # loop for computing overall loss and accuracy\n",
    "    @torch.no_grad()\n",
    "    def _run_batch_loss_accuracy(\n",
    "        i: int, batch_size: int, return_incorrect_sequences: bool = True\n",
    "    ) -> Tuple[\n",
    "        Union[Tuple[float, float, int], Tuple[Tuple[float, float, int], Tensor]], float\n",
    "    ]:\n",
    "        batch = all_tokens_dataset[i : i + batch_size]\n",
    "        size = batch.shape[0]\n",
    "        device = default_device(deterministic=brute_force_proof_deterministic)\n",
    "        batch.to(device)\n",
    "        duration = 0.0\n",
    "        start = time.time()\n",
    "        labels = training_wrapper.config.experiment.get_ground_truth(batch)\n",
    "        xs, ys, y_preds = training_wrapper.compute_batch((batch, labels), device=device)\n",
    "        loss = training_wrapper.loss_fn(\n",
    "            y_preds, ys, log_softmax=training_wrapper.log_softmax\n",
    "        ).item()\n",
    "        full_accuracy = training_wrapper.acc_fn_per_seq(y_preds, ys)\n",
    "        accuracy = full_accuracy.float().mean().item()\n",
    "        duration += time.time() - start\n",
    "        if return_incorrect_sequences:\n",
    "            return ((loss, accuracy, size), xs[~full_accuracy]), duration\n",
    "        return (loss, accuracy, size), duration\n",
    "\n",
    "    with memoshelve(\n",
    "        _run_batch_loss_accuracy,\n",
    "        filename=cache_dir\n",
    "        / f\"{SHARED_CACHE_STEM}.run_batch_loss_accuracy-{cfg_hash_for_filename}-{brute_force_proof_deterministic}\",\n",
    "        get_hash_mem=(lambda x: x[0]),\n",
    "        get_hash=str,\n",
    "        cache={},\n",
    "    )() as run_batch_loss_accuracy_heavy:\n",
    "\n",
    "        def _run_batch_loss_accuracy_lightweight(*args, **kwargs):\n",
    "            res = run_batch_loss_accuracy_heavy(*args, **kwargs)\n",
    "            ((loss, accuracy, size), incorrect_sequences), duration = res\n",
    "            return (loss, accuracy, size), duration\n",
    "\n",
    "        with memoshelve(\n",
    "            _run_batch_loss_accuracy_lightweight,\n",
    "            filename=cache_dir\n",
    "            / f\"{SHARED_CACHE_STEM}.run_batch_loss_accuracy-lightweight-{cfg_hash_for_filename}-{brute_force_proof_deterministic}\",\n",
    "            get_hash_mem=(lambda x: x[0]),\n",
    "            get_hash=str,\n",
    "        )() as run_batch_loss_accuracy:\n",
    "            for i in range(0, len(all_tokens_dataset), batch_size):\n",
    "                (loss, accuracy, size), duration = run_batch_loss_accuracy(i, batch_size)  # type: ignore\n",
    "                total_duration += duration\n",
    "                # Accumulate loss and accuracy\n",
    "                start = time.time()\n",
    "                total_loss += loss * size\n",
    "                total_accuracy += accuracy * size\n",
    "                total_samples += size\n",
    "                total_duration += time.time() - start\n",
    "                # all_incorrect_sequences.append(incorrect_sequences)\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    average_loss = total_loss / total_samples\n",
    "    average_accuracy = total_accuracy / total_samples\n",
    "    # incorrect_sequences = torch.cat(all_incorrect_sequences, dim=0)\n",
    "    num_correct_sequences = int(round(average_accuracy * all_tokens_dataset.length))\n",
    "    num_incorrect_sequences = all_tokens_dataset.length - num_correct_sequences\n",
    "\n",
    "    row = {\n",
    "        \"seed\": seed,\n",
    "        \"cpu\": brute_force_proof_deterministic,\n",
    "        \"loss\": average_loss,\n",
    "        \"accuracy\": average_accuracy,\n",
    "        \"num_correct\": num_correct_sequences,\n",
    "        \"num_incorrect\": num_incorrect_sequences,\n",
    "        \"duration\": total_duration,\n",
    "    }\n",
    "    return row\n",
    "\n",
    "\n",
    "def _handle_brute_force_for(seed: int, *, pbar: tqdm):\n",
    "    try:\n",
    "        brute_force_data[seed] = get_brute_force_for(seed, pbar=pbar)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing brute force proof for seed {seed}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "lengths = [\n",
    "    len(\n",
    "        SequenceDataset(\n",
    "            seq_len=runtime_models[seed][1].cfg.n_ctx,\n",
    "            vocab_size=runtime_models[seed][1].cfg.d_vocab,\n",
    "        )\n",
    "    )\n",
    "    for seed in relevant_seeds\n",
    "]\n",
    "\n",
    "total_batches = sum(\n",
    "    length - length % batch_size + (batch_size if length % batch_size != 0 else 0)\n",
    "    for length in lengths\n",
    ")\n",
    "\n",
    "with tqdm(total=total_batches, desc=\"batches for brute force\", position=0) as pbar:\n",
    "    # with PeriodicGarbageCollector(60):\n",
    "    maybe_parallel_map(\n",
    "        partial(_handle_brute_force_for, pbar=pbar), sorted(relevant_seeds)\n",
    "    )\n",
    "\n",
    "update_csv(BRUTE_FORCE_CSV_PATH, brute_force_data, columns=brute_force_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f1f83-1c3a-42a7-8e30-854e82d85f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "assert len(brute_force_data) == len(\n",
    "    runtime_models\n",
    "), f\"len(brute_force_data) == {len(brute_force_data)} != {len(runtime_models)} == len(runtime_models)\"\n",
    "all_tokens_datasets_lens = {seed: len(d) for seed, d in all_tokens_datasets.items()}\n",
    "assert (\n",
    "    len(set(all_tokens_datasets_lens.values())) == 1\n",
    "), f\"Multiple dataset lengths! {set(all_tokens_datasets_lens.values())}\"\n",
    "latex_values[\"BruteForceCPU\"] = brute_force_proof_deterministic\n",
    "latex_values[\"BruteForceBatchSize\"] = batch_size\n",
    "latex_values[\"BruteForceNumBatches\"] = int(\n",
    "    math.ceil(list(all_tokens_datasets_lens.values())[0] / batch_size)\n",
    ")\n",
    "\n",
    "brute_force_data_by_key = defaultdict(dict)\n",
    "for seed, d in brute_force_data.items():\n",
    "    for k, v in d.items():\n",
    "        brute_force_data_by_key[k][seed] = v\n",
    "\n",
    "\n",
    "for key, latex_key in (\n",
    "    (\"loss\", \"BruteForceLoss\"),\n",
    "    (\"accuracy\", \"BruteForceAccuracy\"),\n",
    "    (\"num_correct\", \"BruteForceNumCorrect\"),\n",
    "    (\"num_incorrect\", \"BruteForceNumIncorrect\"),\n",
    "    (\"duration\", \"BruteForceTime\"),\n",
    "):\n",
    "    latex_values |= data_summary(brute_force_data_by_key[key], prefix=latex_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0034232-ddda-42fd-9651-b7242e8afdba",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Cubic proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7799301-9764-483c-9753-02ee46913b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b7213156e47efb0f38f2ffe203e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches for cubic: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "cubic_columns = [\n",
    "    \"seed\",\n",
    "    \"accuracy-bound\",\n",
    "    \"normalized-accuracy-bound\",\n",
    "    \"correct-count-lower-bound\",\n",
    "    \"duration-proof-search\",\n",
    "    \"duration\",\n",
    "]\n",
    "if os.path.exists(CUBIC_CSV_PATH):\n",
    "    cubic_results = pd.read_csv(CUBIC_CSV_PATH)\n",
    "else:\n",
    "    cubic_results = pd.DataFrame(columns=cubic_columns)\n",
    "\n",
    "all_seeds = set(runtime_models.keys())\n",
    "unknown_seeds = all_seeds - set(cubic_results[\"seed\"])\n",
    "known_seeds = all_seeds - unknown_seeds\n",
    "relevant_seeds = all_seeds if OVERWRITE_CSV_FROM_CACHE else unknown_seeds\n",
    "cubic_data = {\n",
    "    seed: cubic_results[cubic_results[\"seed\"] == seed].iloc[0].to_dict()\n",
    "    for seed in known_seeds\n",
    "}\n",
    "\n",
    "\n",
    "def get_cubic_row(seed: int, *, pbar: tqdm) -> dict:\n",
    "    cfg = cfgs[seed]\n",
    "    cfg_hash = cfg_hashes[seed]\n",
    "    cfg_hash_for_filename = cfg_hashes_for_filename[seed]\n",
    "    runtime, model = runtime_models[seed]\n",
    "    training_wrapper = training_wrappers[seed]\n",
    "    assert cfg.experiment.model_config.seed is not None\n",
    "\n",
    "    # loop for computing overall loss and accuracy\n",
    "    @torch.no_grad()\n",
    "    def _find_proof() -> Tuple[dict, float]:\n",
    "        start = time.time()\n",
    "        cubic_proof_args = cubic.find_proof(model)\n",
    "        duration = time.time() - start\n",
    "        return cubic_proof_args, duration\n",
    "\n",
    "    with memoshelve(\n",
    "        _find_proof,\n",
    "        filename=cache_dir\n",
    "        / f\"{SHARED_CACHE_STEM}.cubic_find_proof-{cfg_hash_for_filename}\",\n",
    "        get_hash_mem=(lambda x: x[0]),\n",
    "        get_hash=str,\n",
    "    )() as find_proof:\n",
    "        cubic_proof_args, duration_proof_search = find_proof()\n",
    "\n",
    "    with memoshelve(\n",
    "        partial(\n",
    "            cubic.verify_proof,\n",
    "            model,\n",
    "            pbar=pbar,\n",
    "            print_complexity=False,\n",
    "            print_results=False,\n",
    "        ),\n",
    "        filename=cache_dir\n",
    "        / f\"{SHARED_CACHE_STEM}.cubic_verify_proof-{cfg_hash_for_filename}\",\n",
    "        get_hash_mem=(lambda x: 0),\n",
    "        get_hash=(lambda x: \"0\"),\n",
    "    )() as verify_proof:\n",
    "        cubic_proof_results = verify_proof(cubic_proof_args)\n",
    "\n",
    "    # largest_wrong_logit_cubic = cubic_proof_results[\"largest_wrong_logit\"]\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"accuracy-bound\": cubic_proof_results[\"accuracy_lower_bound\"],\n",
    "        \"normalized-accuracy-bound\": cubic_proof_results[\"accuracy_lower_bound\"]\n",
    "        / brute_force_data_by_key[\"accuracy\"][seed],\n",
    "        \"correct-count-lower-bound\": cubic_proof_results[\"correct_count_lower_bound\"],\n",
    "        \"duration-proof-search\": duration_proof_search,\n",
    "        \"duration\": cubic_proof_results[\"prooftime\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def _handle_cubic(seed: int, *, pbar: tqdm):\n",
    "    try:\n",
    "        cubic_data[seed] = get_cubic_row(seed, pbar=pbar)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing cubic proof for seed {seed}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# \\sum_{i=0}^{k} i^2 = k * (k+1) * (k*2+1) // 6\n",
    "ks = [cfgs[seed].experiment.model_config.d_vocab - 2 for seed in relevant_seeds]\n",
    "total_batches = sum(k * (k + 1) * (k * 2 + 1) // 6 for k in ks)\n",
    "with tqdm(total=total_batches, desc=\"batches for cubic\", position=0) as pbar:\n",
    "    # with PeriodicGarbageCollector(60):\n",
    "    maybe_parallel_map(partial(_handle_cubic, pbar=pbar), sorted(relevant_seeds))\n",
    "update_csv(CUBIC_CSV_PATH, cubic_data, columns=cubic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fafe4-040d-460d-9f4d-5069eca11a4e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Summary satistics cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c257a54-91bd-4e1b-88a0-a9431d1d81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "cubic_data_by_key = defaultdict(dict)\n",
    "for seed, d in cubic_data.items():\n",
    "    for k, v in d.items():\n",
    "        cubic_data_by_key[k][seed] = v\n",
    "\n",
    "assert len(cubic_data) == len(\n",
    "    brute_force_data\n",
    "), f\"len(cubic_data) == {len(cubic_data)} != {len(brute_force_data)} == len(brute_force_data)\"\n",
    "for key, latex_key in (\n",
    "    # (\"loss\", \"CubicLoss\"),\n",
    "    (\"accuracy-bound\", \"CubicAccuracy\"),\n",
    "    (\"normalized-accuracy-bound\", \"CubicNormalizedAccuracy\"),\n",
    "    (\"correct-count-lower-bound\", \"CubicCorrectCount\"),\n",
    "    (\"duration\", \"CubicProofTime\"),\n",
    "):\n",
    "    latex_values |= data_summary(cubic_data_by_key[key], prefix=latex_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31175e98-4aff-4bb4-b829-104fd45cafc8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Intermediate interp values for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3ae46-8cb4-4db0-b431-6c9cdd01e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "max_logit_diffs = {\n",
    "    seed: EVOU_max_logit_diff(model)\n",
    "    for seed, (_runtime, model) in runtime_models.items()\n",
    "}\n",
    "latex_values |= data_summary(\n",
    "    {\n",
    "        seed: max_logit_diff.mean().item()\n",
    "        for seed, max_logit_diff in max_logit_diffs.items()\n",
    "    },\n",
    "    prefix=\"EVOUMeanMaxRowDiff\",\n",
    ")\n",
    "\n",
    "# hold some data before summarizing it\n",
    "latex_values_tmp_data = defaultdict(dict)\n",
    "for seed, (_runtime, model) in runtime_models.items():\n",
    "    for duplicate_by_sequence_count in [False, True]:\n",
    "        key = \"EVOU-hist-min-above-diag\"\n",
    "        if duplicate_by_sequence_count:\n",
    "            key += \"-dup-by-seq-count\"\n",
    "        (max_logit_minus_diag, duplication_factors) = EVOU_max_minus_diag_logit_diff(\n",
    "            model,\n",
    "            duplicate_by_sequence_count=duplicate_by_sequence_count,\n",
    "        )\n",
    "        mean = np.average(\n",
    "            max_logit_minus_diag.numpy(), weights=duplication_factors.numpy()\n",
    "        )\n",
    "        std = np.average(\n",
    "            (max_logit_minus_diag - mean).numpy() ** 2,\n",
    "            weights=duplication_factors.numpy(),\n",
    "        )\n",
    "        num_std = 1\n",
    "        most_below_value = int(mean + num_std * std)\n",
    "        frac_below = (\n",
    "            duplication_factors[max_logit_minus_diag <= most_below_value].sum()\n",
    "            / duplication_factors.sum()\n",
    "        ).item()\n",
    "        value_key = \"\".join(\n",
    "            v.capitalize() if v[0] != v[0].capitalize() else v for v in key.split(\"-\")\n",
    "        )\n",
    "        latex_values_tmp_data[value_key + \"MostBelowValue\"][seed] = most_below_value\n",
    "        latex_values_tmp_data[value_key + \"MostBelowValueNumStd\"][seed] = num_std\n",
    "        latex_values_tmp_data[value_key + \"MostBelowValueSequenceFrac\"][\n",
    "            seed\n",
    "        ] = frac_below\n",
    "\n",
    "    for duplicate_by_sequence_count in [False, True]:\n",
    "        flat_diffs, duplication_factors = attention_difference_over_gap(\n",
    "            model,\n",
    "            duplicate_by_sequence_count=duplicate_by_sequence_count,\n",
    "        )\n",
    "        key = \"EQKE-hist-attention-difference-over-gap\" + (\n",
    "            \"-dup-by-seq-count\" if duplicate_by_sequence_count else \"\"\n",
    "        )\n",
    "        mean = np.average(flat_diffs.numpy(), weights=duplication_factors.numpy())\n",
    "        std = np.average(\n",
    "            (flat_diffs - mean).numpy() ** 2,\n",
    "            weights=duplication_factors.numpy(),\n",
    "        )\n",
    "        value_key = \"\".join(\n",
    "            v.capitalize() if v[0] != v[0].capitalize() else v for v in key.split(\"-\")\n",
    "        )\n",
    "        latex_values_tmp_data[value_key + \"Mean\"][seed] = mean\n",
    "        latex_values_tmp_data[value_key + \"Std\"][seed] = std\n",
    "\n",
    "for k, v in latex_values_tmp_data.items():\n",
    "    latex_values |= data_summary(v, prefix=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec01dc4-74b2-40b4-98f4-4139b8239e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def analyze_EVOU(model: HookedTransformer):\n",
    "    EPVOU = all_EVOU(model)\n",
    "    PVOU = all_PVOU(model)\n",
    "    PVOU_mean = PVOU.mean(dim=0)\n",
    "    EPVOU += PVOU_mean\n",
    "    PVOU -= PVOU_mean\n",
    "    EPU = EU_PU(model)\n",
    "    EPVOU_diag = EPVOU.diagonal()\n",
    "    EPVOU_centered = EPVOU - EPVOU_diag.unsqueeze(-1)\n",
    "    EPVOU_minf_diag = EPVOU_centered.clone()\n",
    "    EPVOU_minf_diag[tuple(torch.arange(d) for d in EPVOU.shape)] = -torch.inf\n",
    "    EPVOU_max_above_diag = EPVOU_minf_diag.amax(dim=-1)\n",
    "    EPVOU_largest_index_above_diag = torch.arange(EPVOU.shape[0])[\n",
    "        EPVOU_max_above_diag > 0\n",
    "    ]\n",
    "    EPVOU_off_diag = EPVOU.clone()\n",
    "    EPVOU_off_diag[tuple(torch.arange(d) for d in EPVOU.shape)] = torch.nan\n",
    "    EPVOU_off_diag = EPVOU_off_diag[~EPVOU_off_diag.isnan()]\n",
    "    EPVOU_centered_off_diag = EPVOU_centered.clone()\n",
    "    EPVOU_centered_off_diag[tuple(torch.arange(d) for d in EPVOU_centered.shape)] = (\n",
    "        torch.nan\n",
    "    )\n",
    "    EPVOU_centered_off_diag = EPVOU_centered_off_diag[~EPVOU_centered_off_diag.isnan()]\n",
    "\n",
    "    result = {}\n",
    "    result |= data_summary(EPU.flatten(), \"EUPU\")\n",
    "    result |= data_summary(EPU.abs().flatten(), \"EUPUAbs\")\n",
    "    result |= data_summary(EPU.amax(dim=-1) - EPU.amin(dim=-1), \"EUPUMaxRowDiff\")\n",
    "\n",
    "    result |= data_summary(PVOU.flatten(), \"PVOU\")\n",
    "    result |= data_summary(PVOU.abs().flatten(), \"PVOUAbs\")\n",
    "    result |= data_summary(PVOU.amax(dim=-1) - PVOU.amin(dim=-1), \"PVOUMaxRowDiff\")\n",
    "\n",
    "    result |= data_summary(EPVOU.flatten(), \"EPVOU\")\n",
    "    result |= data_summary(EPVOU.abs().flatten(), \"EPVOUAbs\")\n",
    "    result |= data_summary(EPVOU.amax(dim=-1) - EPVOU.amin(dim=-1), \"EPVOUMaxRowDiff\")\n",
    "    result |= data_summary(EPVOU_diag, \"EPVOUDiagonal\")\n",
    "    result |= data_summary(EPVOU_centered.flatten(), \"EPVOUCentered\")\n",
    "    result |= data_summary(EPVOU_max_above_diag, \"EPVOUMaxAboveDiag\")\n",
    "    result |= data_summary(\n",
    "        EPVOU_largest_index_above_diag, \"EPVOUInputsWithCopyingFailure\"\n",
    "    )\n",
    "    result |= data_summary(EPVOU_off_diag, \"EPVOUOffDiagonal\")\n",
    "    result |= data_summary(EPVOU_off_diag.abs(), \"EPVOUOffDiagonalAbs\")\n",
    "    result |= data_summary(EPVOU_centered_off_diag, \"EPVOUCenteredOffDiagonal\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446355a-b164-475b-a198-b040536fca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af75cf6ea2fc4e818cbdd6e4411cb051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EVOU analysis:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# with memoshelve(\n",
    "#     (lambda seed, with_attn_scale: compute_EQKE_SVD_analysis(runtime_models[seed][1], with_attn_scale=with_attn_scale)),\n",
    "#     filename=cache_dir / f\"{SHARED_CACHE_STEM}.compute_EQKE_SVD_analysis\",\n",
    "#     get_hash_mem=(lambda x: x[0]),\n",
    "#     get_hash=str,\n",
    "# )() as memo_compute_EQKE_SVD_analysis:\n",
    "EVOU_analyses = {\n",
    "    seed: analyze_EVOU(runtime_models[seed][1])\n",
    "    for seed in tqdm(list(sorted(runtime_models.keys())), desc=\"EVOU analysis\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd67dbc-86c6-4fd8-9b24-a51edea6d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "EVOU_analyses_by_key = defaultdict(dict)\n",
    "for seed, d in EVOU_analyses.items():\n",
    "    for k, v in d.items():\n",
    "        EVOU_analyses_by_key[k][seed] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084b322-4bd5-49a3-930a-0b516192f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "for k, v in EVOU_analyses_by_key.items():\n",
    "    if k.endswith(\"Float\"):\n",
    "        latex_values |= data_summary(v, prefix=k[: -len(\"Float\")])\n",
    "    else:\n",
    "        latex_values |= data_summary(v, prefix=k)\n",
    "        # vals = set(v.values())\n",
    "        # assert len(vals) == 1, f\"Too many values for {k}: {vals}\"\n",
    "        # latex_values[k] = list(vals)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f1d4-35c7-46e9-b8ab-705fe73fff68",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # SVD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701ecca-82a5-478b-925b-882f2e6e12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# random resampling of EQKE_err\n",
    "@torch.no_grad()\n",
    "def resample_EQKE_err(\n",
    "    *ms: torch.Tensor,\n",
    "    # QK_colorscale: Colorscale = \"Plasma\",\n",
    "    # QK_SVD_colorscale: Colorscale = \"Picnic_r\",\n",
    "    seed: int = 1234,\n",
    "    nsamples: int = 100,\n",
    ") -> dict[str, float]:\n",
    "    results_float = {}\n",
    "    # what if we randomize the order of all matrices without replacement?\n",
    "    torch.manual_seed(seed)\n",
    "    results_float[\"ResampleEQKEErrSeed\"] = seed\n",
    "    results_float[\"ResampleEQKEErrNumSamples\"] = nsamples\n",
    "    row_diffs = []\n",
    "    max_row_diffs = []\n",
    "    for _ in range(nsamples):\n",
    "        ms_no_replacement = [shuffle_tensor(m) for m in ms]\n",
    "        result = reduce(torch.matmul, ms_no_replacement)\n",
    "        row_diffs.extend(result.max(dim=-1).values - result.min(dim=-1).values)\n",
    "        max_row_diffs.append(\n",
    "            (result.max(dim=-1).values - result.min(dim=-1).values).max().item()\n",
    "        )\n",
    "    row_diffs = torch.stack(row_diffs)\n",
    "    results_float |= data_summary(max_row_diffs, prefix=\"ResampleEQKEErr\")\n",
    "    # print(f\"row diff: {pm_mean_std(row_diffs)}\")\n",
    "    # sampling from normal\n",
    "    row_diffs = []\n",
    "    max_row_diffs = []\n",
    "    for _ in range(nsamples):\n",
    "        ms_normal = [torch.randn_like(m) * m.std() + m.mean() for m in ms]\n",
    "        result = reduce(torch.matmul, ms_normal)\n",
    "        row_diffs.extend(result.max(dim=-1).values - result.min(dim=-1).values)\n",
    "        max_row_diffs.append(\n",
    "            (result.max(dim=-1).values - result.min(dim=-1).values).max().item()\n",
    "        )\n",
    "    row_diffs = torch.stack(row_diffs)\n",
    "    results_float |= data_summary(max_row_diffs, prefix=\"ResampleNormalEQKEErr\")\n",
    "    # print(f\"row diff: {pm_mean_std(row_diffs)}\")\n",
    "    return results_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64d600-2ffa-4456-b4aa-01176d409802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_EQKE_SVD_analysis(model: HookedTransformer) -> dict[str, float]:\n",
    "    results_float = {}\n",
    "    (\n",
    "        size_direction,\n",
    "        query_direction,\n",
    "        size_query_singular_value,\n",
    "    ), _ = find_size_and_query_direction(model)\n",
    "    (second_key_direction, second_key_singular_value), (\n",
    "        second_query_direction,\n",
    "        second_query_singular_value,\n",
    "    ) = find_second_singular_contributions(model, size_direction, query_direction)\n",
    "    (W_Q_U, W_Q_S, W_Q_Vh), (W_Q_contrib, W_Q_err) = split_svd_contributions(\n",
    "        model.W_Q[0, 0]\n",
    "    )\n",
    "    (W_K_U, W_K_S, W_K_Vh), (W_K_contrib, W_K_err) = split_svd_contributions(\n",
    "        model.W_K[0, 0]\n",
    "    )\n",
    "    (\n",
    "        (EQKE_query_key, err_accumulator),\n",
    "        EQKE_pos_err,\n",
    "        (err_upper_bound, (W_E_query_err2, W_Q_err, W_K_errT, W_E_key_err2T)),\n",
    "    ) = quadratic.decompose_EQKE_error_quadratic(\n",
    "        model,\n",
    "        key_direction=size_direction,\n",
    "        query_direction=query_direction,\n",
    "        second_key_direction=second_key_direction,\n",
    "        second_query_direction=second_query_direction,\n",
    "        W_Q_U=W_Q_U,\n",
    "        W_K_U=W_K_U,\n",
    "        sanity_check=False,\n",
    "    )\n",
    "\n",
    "    EQKE_pos_err_with_attn_scale = EQKE_pos_err / model.blocks[0].attn.attn_scale\n",
    "\n",
    "    for attn_scale, cur_EQKE_pos_err in (\n",
    "        (\"\", EQKE_pos_err),\n",
    "        (\"WithAttnScale\", EQKE_pos_err_with_attn_scale),\n",
    "    ):\n",
    "        results_float |= data_summary(\n",
    "            cur_EQKE_pos_err.flatten(), prefix=f\"EQKP{attn_scale}\"\n",
    "        )\n",
    "        results_float |= data_summary(\n",
    "            cur_EQKE_pos_err.abs().flatten(), prefix=f\"EQKP{attn_scale}Abs\"\n",
    "        )\n",
    "        results_float |= data_summary(\n",
    "            cur_EQKE_pos_err.amax(dim=-1) - cur_EQKE_pos_err.amin(dim=-1),\n",
    "            prefix=f\"EQKP{attn_scale}MaxRowDiff\",\n",
    "        )\n",
    "\n",
    "    EQKE_err = W_E_query_err2 @ W_Q_err @ W_K_errT @ W_E_key_err2T\n",
    "    EQKE_err_simple = EQKE_err + err_accumulator\n",
    "    EQKE_exact = EQKE_query_key + EQKE_err_simple\n",
    "\n",
    "    EQKE_err_with_attn_scale = EQKE_err / model.blocks[0].attn.attn_scale\n",
    "    EQKE_err_simple_with_attn_scale = EQKE_err_simple / model.blocks[0].attn.attn_scale\n",
    "    EQKE_exact_with_attn_scale = EQKE_exact / model.blocks[0].attn.attn_scale\n",
    "\n",
    "    U, S, Vh = torch.linalg.svd(EQKE_exact)\n",
    "    S_with_attn_scale = S / model.blocks[0].attn.attn_scale\n",
    "    mindim = np.min(model.W_Q[0, 0].shape)\n",
    "    for attn_scale, cur_S in ((\"\", S), (\"WithAttnScale\", S_with_attn_scale)):\n",
    "        results_float[f\"EQKE{attn_scale}FirstSingularFloat\"] = cur_S[0].item()\n",
    "        results_float[f\"EQKE{attn_scale}SecondSingularFloat\"] = cur_S[1].item()\n",
    "        results_float[f\"EQKE{attn_scale}ThirdSingularFloat\"] = cur_S[2].item()\n",
    "        results_float[f\"EQKE{attn_scale}RatioFirstTwoSingularFloat\"] = (\n",
    "            cur_S[0] / cur_S[1]\n",
    "        ).item()\n",
    "        results_float |= data_summary(S[:mindim], prefix=f\"EQKE{attn_scale}Singular\")\n",
    "    size_direction_diffs = size_direction.squeeze()[1:] - size_direction.squeeze()[:-1]\n",
    "    results_float |= data_summary(size_direction, prefix=\"EQKESizeDirection\")\n",
    "    results_float |= data_summary(size_direction_diffs, prefix=\"EQKESizeDirectionDiffs\")\n",
    "    results_float |= data_summary(query_direction, prefix=\"EQKEQueryDirection\")\n",
    "\n",
    "    for cur_EQKE_err, descr in (\n",
    "        (EQKE_err_simple, \"Simple\"),\n",
    "        (EQKE_err, \"\"),\n",
    "        (EQKE_err_simple_with_attn_scale, \"SimpleWithAttnScale\"),\n",
    "        (EQKE_err_with_attn_scale, \"WithAttnScale\"),\n",
    "    ):\n",
    "        results_float[f\"EQKEErr{descr}MaxRowDiffFloat\"] = (\n",
    "            (cur_EQKE_err.max(dim=-1).values - cur_EQKE_err.min(dim=-1).values)\n",
    "            .max()\n",
    "            .item()\n",
    "        )\n",
    "        results_float[f\"EQKEErr{descr}MaxAbsFloat\"] = cur_EQKE_err.abs().max().item()\n",
    "        results_float[f\"EQKEErr{descr}MeanDimZeroNormFloat\"] = (\n",
    "            cur_EQKE_err.mean(dim=0).norm().item()\n",
    "        )\n",
    "        results_float |= data_summary(cur_EQKE_err.flatten(), f\"EQKEErr{descr}\")\n",
    "        s1 = torch.linalg.matrix_norm(cur_EQKE_err, ord=2)\n",
    "        results_float[f\"EQKEErr{descr}FirstSingularFloat\"] = s1.item()\n",
    "        results_float[f\"EQKEErr{descr}FirstSingularSqrtTwoFloat\"] = (\n",
    "            s1 * np.sqrt(2)\n",
    "        ).item()\n",
    "        sf1 = torch.linalg.matrix_norm(cur_EQKE_err, ord=\"fro\")\n",
    "        results_float[f\"EQKEErr{descr}FroNormFloat\"] = sf1.item()\n",
    "        results_float[f\"EQKEErr{descr}FroNormSqrtTwoFloat\"] = (sf1 * np.sqrt(2)).item()\n",
    "\n",
    "    ss = [\n",
    "        torch.linalg.matrix_norm(m, ord=2).item()\n",
    "        for m in (W_E_query_err2, W_Q_err, W_K_errT, W_E_key_err2T)\n",
    "    ]\n",
    "    (\n",
    "        results_float[\"WEqqPerpFirstSingularFloat\"],\n",
    "        results_float[\"WQqPerpFirstSingularFloat\"],\n",
    "        results_float[\"WKkPerpFirstSingularFloat\"],\n",
    "        results_float[\"WEkkPerpFirstSingularFloat\"],\n",
    "    ) = ss\n",
    "    results_float[\"EQKEErrProdFirstSingularFloat\"] = np.prod(ss)\n",
    "    results_float[\"EQKEErrProdFirstSingularSqrtTwoFloat\"] = np.prod(ss) * np.sqrt(2)\n",
    "    sfs = [\n",
    "        torch.linalg.matrix_norm(m, ord=\"fro\").item()\n",
    "        for m in (W_E_query_err2, W_Q_err, W_K_errT, W_E_key_err2T)\n",
    "    ]\n",
    "    (\n",
    "        results_float[\"WEqqPerpFroNormFloat\"],\n",
    "        results_float[\"WQqPerpFroNormFloat\"],\n",
    "        results_float[\"WKkPerpFroNormFloat\"],\n",
    "        results_float[\"WEkkPerpFroNormFloat\"],\n",
    "    ) = sfs\n",
    "    results_float[\"EQKEErrProdFroNormFloat\"] = np.prod(sfs)\n",
    "    results_float[\"EQKEErrProdFroNormSqrtTwoFloat\"] = np.prod(sfs) * np.sqrt(2)\n",
    "\n",
    "    results_float |= resample_EQKE_err(W_E_query_err2, W_Q_err, W_K_errT, W_E_key_err2T)\n",
    "\n",
    "    return results_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f8e99-c8fe-4307-8421-50769aba2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3826d7beba4448129035204480b5c39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SVD analysis:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "with memoshelve(\n",
    "    (lambda seed: compute_EQKE_SVD_analysis(runtime_models[seed][1])),\n",
    "    filename=cache_dir / f\"{SHARED_CACHE_STEM}.compute_EQKE_SVD_analysis\",\n",
    "    get_hash_mem=(lambda x: x[0]),\n",
    "    get_hash=str,\n",
    ")() as memo_compute_EQKE_SVD_analysis:\n",
    "    EQKE_SVD_analyses = {\n",
    "        seed: memo_compute_EQKE_SVD_analysis(seed)\n",
    "        for seed in tqdm(list(sorted(runtime_models.keys())), desc=\"SVD analysis\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00395d-1466-4135-a538-413f7f9eb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "EQKE_SVD_analyses_by_key = defaultdict(dict)\n",
    "for seed, d in EQKE_SVD_analyses.items():\n",
    "    for k, v in d.items():\n",
    "        EQKE_SVD_analyses_by_key[k][seed] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579905e-b360-4661-aac6-510bc434d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "for k, v in EQKE_SVD_analyses_by_key.items():\n",
    "    if k.endswith(\"Float\"):\n",
    "        latex_values |= data_summary(v, prefix=k[: -len(\"Float\")])\n",
    "    else:\n",
    "        vals = set(v.values())\n",
    "        assert len(vals) == 1, f\"Too many values for {k}: {vals}\"\n",
    "        latex_values[k] = list(vals)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156a4e2-0b76-46e8-8a5b-917ab8bef238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "new_data = []\n",
    "for seed, d in EQKE_SVD_analyses.items():\n",
    "    new_data.append(d | {\"seed\": seed})\n",
    "\n",
    "for k, v in EQKE_SVD_analyses_by_key.items():\n",
    "    if k.endswith(\"Float\"):\n",
    "        latex_values |= data_summary(v, prefix=k[: -len(\"Float\")])\n",
    "    else:\n",
    "        vals = set(v.values())\n",
    "        assert len(vals) == 1, f\"Too many values for {k}: {vals}\"\n",
    "        latex_values[k] = list(vals)[0]\n",
    "\n",
    "update_csv_with_rows(\n",
    "    SUBCUBIC_ANALYSIS_CSV_PATH,\n",
    "    new_data,\n",
    "    columns=[\"seed\"] + list(EQKE_SVD_analyses_by_key.keys()),\n",
    "    subset=[\"seed\"] + list(EQKE_SVD_analyses_by_key.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d20e64-357b-456c-ad70-c99ba13ad6e0",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Sub-cubic Proofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad2962-80a3-4615-8056-48767e33b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "try_all_configurations: bool = True  # @param {type:\"boolean\"}\n",
    "use_tricks: bool = True  # @param {type:\"boolean\"}\n",
    "all_configs: list[LargestWrongLogitQuadraticConfig]\n",
    "if try_all_configurations:\n",
    "    all_configs = list(enumerate_dataclass_values(LargestWrongLogitQuadraticConfig))\n",
    "elif use_tricks:\n",
    "    all_configs = [LargestWrongLogitQuadraticConfig()]\n",
    "else:\n",
    "    all_configs = [LargestWrongLogitQuadraticConfig.OFF()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d5555-4e09-47af-9855-93c13fa8c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "def _subcubic_count_verify_proof(\n",
    "    model: HookedTransformer,\n",
    "    tricks: LargestWrongLogitQuadraticConfig,\n",
    "    *,\n",
    "    sanity_check_instructions: bool = False,\n",
    "    **kwargs,\n",
    ") -> Tuple[InstructionCount, dict[str, Any]]:\n",
    "    # must be outside PatchTorch to avoid triu, tril\n",
    "    cmodel = CountHookedTransformer(model)\n",
    "    with PatchTorch():\n",
    "        with instructions.set_sanity_check(sanity_check_instructions):\n",
    "            with CountTensorOperations() as subcubic_instruction_count:\n",
    "                results = subcubic.verify_proof(\n",
    "                    cmodel,\n",
    "                    tricks=tricks,\n",
    "                    **kwargs,\n",
    "                    print_complexity=False,\n",
    "                    print_results=False,\n",
    "                    sanity_check=False,\n",
    "                    # print_types=True,\n",
    "                )\n",
    "    return subcubic_instruction_count, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75acce5-8123-4d0f-b0df-1cbacc671460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "_some_runtime, some_model = runtime_models[123]\n",
    "d_vocab, n_ctx = some_model.cfg.d_vocab, some_model.cfg.n_ctx\n",
    "latex_values[\"BruteForceEffectiveDimensionalityEstimate\"] = brute_force_ed = (\n",
    "    d_vocab ** (n_ctx + 1)\n",
    ")\n",
    "EUPU_cost = d_vocab**2\n",
    "PVOU_cost = n_ctx * d_vocab\n",
    "EPQKE_cost = d_vocab**2\n",
    "EPQKP_cost = d_vocab * n_ctx\n",
    "EVOU_cost = d_vocab**2\n",
    "latex_values[\"CubicEffectiveDimensionalityEstimate\"] = cubic_ed = (\n",
    "    EUPU_cost + PVOU_cost + EPQKE_cost + EPQKP_cost + EVOU_cost\n",
    ")\n",
    "subcubic_PVOU_cost = d_vocab\n",
    "subcubic_EPQKP_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf24ce-e60f-44da-8ee7-7b70d36f17c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180470431a4145149482e7bd4ffe0107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configurations for subcubic: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e431d1c900a94653b5c3a2553ee4604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subconfig progress: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd9f8a28c064c639f42dcdadebbee69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "proofs for subcubic: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539100727fef4866ae77762390973ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "instruction counts for subcubic: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "subcubic_columns = [\n",
    "    \"seed\",\n",
    "    \"accuracy-bound\",\n",
    "    \"normalized-accuracy-bound\",\n",
    "    \"duration-proof-search\",\n",
    "    \"duration\",\n",
    "    \"tricks\",\n",
    "    \"err-upper-bound\",\n",
    "    \"err-upper-bound-is-max\",\n",
    "    \"total-sequences\",\n",
    "    \"dropped-sequences\",\n",
    "    \"dropped-sequences-frac\",\n",
    "    \"most-gap-below-value\",\n",
    "    \"most-gap-below-value-frac\",\n",
    "    \"most-gap-below-value-num-std\",\n",
    "    \"max-gap\",\n",
    "    \"perf-time-enabled-ns\",\n",
    "    \"perf-instruction-count\",\n",
    "    \"perf-branch-misses\",\n",
    "    \"perf-page-faults\",\n",
    "    \"proof-flop-estimate\",\n",
    "    \"proof-int-op-estimate\",\n",
    "    \"proof-branch-estimate\",\n",
    "]\n",
    "if os.path.exists(SUBCUBIC_CSV_PATH):\n",
    "    subcubic_results = pd.read_csv(SUBCUBIC_CSV_PATH)\n",
    "else:\n",
    "    subcubic_results = pd.DataFrame(columns=subcubic_columns)\n",
    "\n",
    "all_seeds = set(runtime_models.keys())\n",
    "unknown_seeds = all_seeds - set(\n",
    "    seed\n",
    "    for seed in subcubic_results[\"seed\"]\n",
    "    if len(subcubic_results[subcubic_results[\"seed\"] == seed].to_dict(orient=\"records\"))\n",
    "    >= len(all_configs)\n",
    ")\n",
    "subcubic_data = {\n",
    "    seed: subcubic_results[subcubic_results[\"seed\"] == seed].to_dict(orient=\"records\")\n",
    "    for seed in all_seeds\n",
    "    if seed not in unknown_seeds\n",
    "}\n",
    "known_seeds = all_seeds - unknown_seeds\n",
    "relevant_seeds = all_seeds if OVERWRITE_CSV_FROM_CACHE else unknown_seeds\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def try_all_proofs_subcubic(\n",
    "    seed: int,\n",
    "    *,\n",
    "    subcfg_pbar: tqdm,\n",
    "    cfg_pbar: tqdm,\n",
    "    proof_pbar: tqdm,\n",
    "    count_proof_pbar: tqdm,\n",
    ") -> list[dict]:\n",
    "    cfg = cfgs[seed]\n",
    "    cfg_hash = cfg_hashes[seed]\n",
    "    cfg_hash_for_filename = cfg_hashes_for_filename[seed]\n",
    "    runtime, model = runtime_models[seed]\n",
    "    training_wrapper = training_wrappers[seed]\n",
    "    assert cfg.experiment.model_config.seed is not None\n",
    "\n",
    "    min_gaps_lists = {}\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    shared_proof_search_duration = 0.0\n",
    "    start = time.time()\n",
    "    W_EP_direction_kwargs = analysis_quadratic.W_EP_direction_for_tricks_kwargs(model)\n",
    "    find_min_gaps_kwargs = analysis_subcubic.find_min_gaps_with_EQKE_kwargs(model)\n",
    "    size_and_query_directions_kwargs = analysis_quadratic.find_EKQE_error_directions(\n",
    "        model\n",
    "    )\n",
    "    shared_proof_search_duration += time.time() - start\n",
    "    with memoshelve(\n",
    "        (\n",
    "            lambda cfg: (\n",
    "                cfg,\n",
    "                *analysis_subcubic.find_min_gaps_with_EQKE(\n",
    "                    model=model,\n",
    "                    **find_min_gaps_kwargs,  # type: ignore\n",
    "                    **size_and_query_directions_kwargs,\n",
    "                    tricks=cfg,\n",
    "                    sub_pbar=subcfg_pbar,\n",
    "                    pbar=cfg_pbar,\n",
    "                    record_time=True,\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "        # cache={},\n",
    "        filename=cache_dir\n",
    "        / f\"{SHARED_CACHE_STEM}.find_min_gaps-{cfg_hash_for_filename}\",\n",
    "    )() as find_min_gaps_for:\n",
    "        min_gaps_lists = [find_min_gaps_for(cfg) for cfg in all_configs]\n",
    "\n",
    "    for tricks, min_gaps, proof_search_duration in min_gaps_lists:\n",
    "        if N_THREADS is None or N_THREADS <= 1:\n",
    "            proof_pbar.set_postfix(cfg=tricks.short_description(latex=True))\n",
    "        proof_search_duration += shared_proof_search_duration\n",
    "        # print(\n",
    "        #     f\"==========={descr}=============================\\nTricks: {tricks}\"\n",
    "        # )\n",
    "        # this is not part of the proof checking; the proof is correct regardless of what value is returned, so we don't count the complexity\n",
    "        start = time.time()\n",
    "        W_EP_direction = analysis_quadratic.W_EP_direction_for_tricks(\n",
    "            **W_EP_direction_kwargs, tricks=tricks\n",
    "        )\n",
    "        proof_search_duration += time.time() - start\n",
    "\n",
    "        def _verify_proof(tricks: LargestWrongLogitQuadraticConfig):\n",
    "            return subcubic.verify_proof(\n",
    "                model,\n",
    "                W_EP_direction=W_EP_direction,\n",
    "                **size_and_query_directions_kwargs,  # type: ignore\n",
    "                min_gaps=min_gaps,\n",
    "                tricks=tricks,\n",
    "                sanity_check=False,\n",
    "                print_complexity=False,\n",
    "                print_results=False,\n",
    "                include_perf=PERF_WORKING,\n",
    "            )\n",
    "\n",
    "        with memoshelve(\n",
    "            _verify_proof,\n",
    "            filename=cache_dir\n",
    "            / f\"{SHARED_CACHE_STEM}.subcubic_verify_proof{'' if not PERF_WORKING else '-with-perf'}-{cfg_hash_for_filename}\",\n",
    "            get_hash_mem=(lambda x: x[0]),\n",
    "            get_hash=str,\n",
    "        )() as verify_proof:\n",
    "            proof_results = verify_proof(tricks)\n",
    "\n",
    "        err_upper_bound = proof_results[\"err_upper_bound\"]\n",
    "        prooftime = proof_results[\"prooftime\"]\n",
    "        accuracy_bound = proof_results[\"accuracy_lower_bound\"]\n",
    "        total_sequences = proof_results[\"total_sequences\"]\n",
    "        left_behind = proof_results[\"left_behind\"]\n",
    "\n",
    "        if PERF_WORKING:\n",
    "            perf_results = {\n",
    "                \"perf-time-enabled-ns\": int_or_value(\n",
    "                    proof_results[\"proofinstructions\"].time_enabled_ns\n",
    "                ),\n",
    "                \"perf-instruction-count\": int_or_value(\n",
    "                    proof_results[\"proofinstructions\"].instruction_count\n",
    "                ),\n",
    "                \"perf-branch-misses\": int_or_value(\n",
    "                    proof_results[\"proofinstructions\"].branch_misses\n",
    "                ),\n",
    "                \"perf-page-faults\": int_or_value(\n",
    "                    proof_results[\"proofinstructions\"].page_faults\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            perf_results = {}\n",
    "\n",
    "        with memoshelve(\n",
    "            partial(\n",
    "                _subcubic_count_verify_proof,\n",
    "                model,\n",
    "                W_EP_direction=(\n",
    "                    CountTensor.from_numpy(W_EP_direction)\n",
    "                    if W_EP_direction is not None\n",
    "                    else W_EP_direction\n",
    "                ),\n",
    "                **{k: CountTensor.from_numpy(v) if isinstance(v, torch.Tensor) else v for k, v in size_and_query_directions_kwargs.items()},  # type: ignore\n",
    "                min_gaps=min_gaps,\n",
    "                sanity_check_instructions=False,\n",
    "            ),\n",
    "            filename=cache_dir\n",
    "            / f\"{SHARED_CACHE_STEM}.subcubic_count_verify_proof-{cfg_hash_for_filename}\",\n",
    "            get_hash_mem=(lambda x: x[0]),\n",
    "            get_hash=str,\n",
    "        )() as count_verify_proof:\n",
    "            (\n",
    "                subcubic_instruction_count,\n",
    "                subcubic_proof_instruction_count_results,\n",
    "            ) = count_verify_proof(tricks)\n",
    "        count_proof_pbar.update(1)\n",
    "\n",
    "        try:\n",
    "            # err_upper_bound_key = f\"SubcubicErrUpperBound{tricks.transform_description(tricks.attention_error_handling, latex=True)}Float\"\n",
    "            err_upper_bound_value = err_upper_bound.item()\n",
    "            err_upper_bound_is_max = False\n",
    "            # print(f\"err_upper_bound: {err_upper_bound_value}\")\n",
    "        except Exception:\n",
    "            # print(f\"err_upper_bound: {err_upper_bound}\")\n",
    "            # err_upper_bound_key = f\"SubcubicErrUpperBoundMax{tricks.transform_description(tricks.attention_error_handling, latex=True)}Float\"\n",
    "            err_upper_bound_value = err_upper_bound.max().item()\n",
    "            err_upper_bound_is_max = True\n",
    "            # print(f\"err_upper_bound.max(): {err_upper_bound_value}\")\n",
    "\n",
    "        def _analyze_gaps(*args, **kwargs):\n",
    "            d_vocab_q, d_vocab_max, n_ctx_nonmax_copies = min_gaps_lists[0][1].shape\n",
    "            weights = torch.zeros((d_vocab_q, d_vocab_max, n_ctx_nonmax_copies))\n",
    "            # weights = ein.array(\n",
    "            #     (\n",
    "            #         lambda q_tok, max_tok, n_copies_nonmax: torch.tensor(\n",
    "            #             (max_tok - 1) ** n_copies_nonmax\n",
    "            #             * math.comb(model.cfg.n_ctx - 1, n_copies_nonmax)\n",
    "            #         )\n",
    "            #     ),\n",
    "            #     sizes=[d_vocab_q, d_vocab_max, n_ctx_nonmax_copies],\n",
    "            #     device=torch.tensor(0).device,\n",
    "            # )\n",
    "            # weights[:, 0, :] = 1\n",
    "            # weights[:, 0, 1:] = 0\n",
    "            # weights = ein.array(\n",
    "            #     (\n",
    "            #         lambda q_tok, max_tok, n_copies_nonmax: torch.where(\n",
    "            #             (\n",
    "            #                 (q_tok > max_tok)\n",
    "            #                 | ( # TypeError: unsupported operand type(s) for |: 'Tensor' and 'Tensor'\n",
    "            #                     (n_copies_nonmax == n_ctx_nonmax_copies - 1)\n",
    "            #                     & (max_tok != q_tok)\n",
    "            #                 )\n",
    "            #                 | ((max_tok == 0) & (n_copies_nonmax > 0))\n",
    "            #             ),\n",
    "            #             torch.tensor(0),\n",
    "            #             torch.where(\n",
    "            #                 max_tok == 0,\n",
    "            #                 torch.tensor(1),\n",
    "            #                 torch.tensor(\n",
    "            #                     (max_tok - 1) ** n_copies_nonmax\n",
    "            #                     * math.comb(model.cfg.n_ctx - 1, n_copies_nonmax)\n",
    "            #                 ),\n",
    "            #             ),\n",
    "            #         )\n",
    "            #     ),\n",
    "            #     sizes=[d_vocab_q, d_vocab_max, n_ctx_nonmax_copies],\n",
    "            #     device=torch.tensor(0).device,\n",
    "            # )\n",
    "            for max_tok in range(d_vocab_max):\n",
    "                cur_n_ctx_nonmax_copies = 1 if max_tok == 0 else n_ctx_nonmax_copies\n",
    "                for n_copies_nonmax in range(cur_n_ctx_nonmax_copies):\n",
    "                    weights[: max_tok + 1, max_tok, n_copies_nonmax] = (\n",
    "                        max_tok - 1\n",
    "                    ) ** n_copies_nonmax * math.comb(\n",
    "                        model.cfg.n_ctx - 1, n_copies_nonmax\n",
    "                    )\n",
    "                weights[:max_tok, max_tok, n_ctx_nonmax_copies - 1] = 0\n",
    "                # for q_tok in range(max_tok+1):\n",
    "                #     if (\n",
    "                #         # (q_tok > max_tok) or\n",
    "                #          (\n",
    "                #             n_copies_nonmax == n_ctx_nonmax_copies - 1\n",
    "                #             and max_tok != q_tok\n",
    "                #         )\n",
    "                #         # or (max_tok == 0 and n_copies_nonmax > 0)\n",
    "                #     ):\n",
    "                #         weights[q_tok, max_tok, n_copies_nonmax] = 0\n",
    "                # if max_tok == 0:\n",
    "                #     assert q_tok == max_tok\n",
    "                #     assert n_copies_nonmax == 0\n",
    "            weights[1, 1, 0] = 1\n",
    "\n",
    "            v = min_gaps.flatten().detach().cpu()\n",
    "            mean = np.average(v.numpy(), weights=weights.flatten().numpy())\n",
    "            std = np.average(\n",
    "                (v - mean).numpy() ** 2,\n",
    "                weights=weights.flatten().numpy(),\n",
    "            )\n",
    "            num_std = 1.5\n",
    "            most_below_value = int(math.ceil(mean + num_std * std))\n",
    "            # print(v)\n",
    "            # print(most_below_value)\n",
    "            # print(list(sorted(v.tolist())))\n",
    "            # print(f\"max={(min_gaps==min_gaps.max()).nonzero()}\")\n",
    "            # if min_gaps.max() > 100:\n",
    "            #     print(f\"big! {min_gaps.max()}\")\n",
    "            #     args = (tricks,)\n",
    "            #     kwargs = dict(\n",
    "            #         filename=cache_dir\n",
    "            #         / f\"{SHARED_CACHE_STEM}.find_min_gaps-{descr}-{cfg_hash_for_filename}\"\n",
    "            #     )\n",
    "            #     print(f\"memoshelve_uncache(*{args}, **{kwargs})\")\n",
    "            #     memoshelve_uncache(*args, **kwargs)\n",
    "            #     args = (tricks, use_exact_EQKE)\n",
    "            #     kwargs = dict(\n",
    "            #         filename=cache_dir\n",
    "            #         / f\"{SHARED_CACHE_STEM}.subcubic_verify_proof-{cfg_hash_for_filename}\",\n",
    "            #         get_hash_mem=(lambda x: x[0]),\n",
    "            #         get_hash=str,\n",
    "            #     )\n",
    "            #     print(f\"memoshelve_uncache(*{args}, **{kwargs})\")\n",
    "            #     memoshelve_uncache(*args, **kwargs)\n",
    "            # print(f\"mean={mean}\")\n",
    "            # print(f\"std={std}\")\n",
    "            # print(f\"max={v.max().item()}\")\n",
    "            # print(f\"min={v.min().item()}\")\n",
    "            # print(v <= most_below_value)\n",
    "            frac_below = (\n",
    "                weights.flatten()[v <= most_below_value].sum() / weights.sum()\n",
    "            ).item()\n",
    "\n",
    "            return frac_below, v, most_below_value, mean, std, num_std\n",
    "\n",
    "        with memoshelve(\n",
    "            _analyze_gaps,\n",
    "            filename=cache_dir\n",
    "            / f\"{SHARED_CACHE_STEM}.subcubic_analyze_gaps-{cfg_hash_for_filename}\",\n",
    "            get_hash_mem=(lambda x: x[0]),\n",
    "            get_hash=str,\n",
    "        )() as analyze_gaps:\n",
    "            (frac_below, v, most_below_value, mean, std, num_std) = analyze_gaps(tricks)\n",
    "\n",
    "        row = {\n",
    "            \"seed\": seed,\n",
    "            \"accuracy-bound\": accuracy_bound,\n",
    "            \"normalized-accuracy-bound\": accuracy_bound\n",
    "            / brute_force_data_by_key[\"accuracy\"][seed],\n",
    "            \"duration-proof-search\": proof_search_duration,\n",
    "            \"duration\": prooftime,\n",
    "            \"tricks\": tricks.short_description(latex=True),\n",
    "            \"err-upper-bound\": err_upper_bound_value,\n",
    "            \"err-upper-bound-is-max\": err_upper_bound_is_max,\n",
    "            \"total-sequences\": total_sequences,\n",
    "            \"dropped-sequences\": left_behind,\n",
    "            \"dropped-sequences-frac\": left_behind / total_sequences,\n",
    "            \"most-gap-below-value\": most_below_value,\n",
    "            \"most-gap-below-value-frac\": frac_below,\n",
    "            \"most-gap-below-value-num-std\": num_std,\n",
    "            \"max-gap\": v.max().item(),\n",
    "            \"proof-flop-estimate\": subcubic_instruction_count.flop,\n",
    "            \"proof-int-op-estimate\": subcubic_instruction_count.int_op,\n",
    "            \"proof-branch-estimate\": subcubic_instruction_count.branch,\n",
    "        } | perf_results\n",
    "\n",
    "        rows.append(row)\n",
    "        proof_pbar.update(1)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _handle_subcubic(\n",
    "    seed: int,\n",
    "    *,\n",
    "    subcfg_pbar: tqdm,\n",
    "    cfg_pbar: tqdm,\n",
    "    proof_pbar: tqdm,\n",
    "    count_proof_pbar: tqdm,\n",
    "):\n",
    "    if N_THREADS is None or N_THREADS <= 1:\n",
    "        cfg_pbar.set_postfix(seed=seed)\n",
    "    try:\n",
    "        subcubic_data[seed] = try_all_proofs_subcubic(\n",
    "            seed,\n",
    "            subcfg_pbar=subcfg_pbar,\n",
    "            cfg_pbar=cfg_pbar,\n",
    "            proof_pbar=proof_pbar,\n",
    "            count_proof_pbar=count_proof_pbar,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing subcubic proof for seed {seed}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "cfg_counts = {\n",
    "    seed: sum(\n",
    "        2 if cfg.attention_error_handling == \"max_diff_exact\" else 1\n",
    "        for cfg in all_configs\n",
    "    )\n",
    "    for seed in relevant_seeds\n",
    "}\n",
    "sub_cfg_counts = {\n",
    "    seed: runtime_models[seed][1].cfg.d_vocab * num_cfgs\n",
    "    for seed, num_cfgs in cfg_counts.items()\n",
    "}\n",
    "\n",
    "n_cfgs = sum(cfg_counts.values())\n",
    "n_subcfgs = sum(sub_cfg_counts.values())\n",
    "with (\n",
    "    tqdm(total=n_cfgs, desc=\"configurations for subcubic\", position=0) as cfg_pbar,\n",
    "    tqdm(total=n_subcfgs, desc=\"subconfig progress\", position=1) as subcfg_pbar,\n",
    "    tqdm(total=n_cfgs, desc=\"proofs for subcubic\", position=2) as proof_pbar,\n",
    "    tqdm(\n",
    "        total=n_cfgs, desc=\"instruction counts for subcubic\", position=3\n",
    "    ) as count_proof_pbar,\n",
    "):\n",
    "    # with PeriodicGarbageCollector(60):\n",
    "    maybe_parallel_map(\n",
    "        partial(\n",
    "            _handle_subcubic,\n",
    "            subcfg_pbar=subcfg_pbar,\n",
    "            cfg_pbar=cfg_pbar,\n",
    "            proof_pbar=proof_pbar,\n",
    "            count_proof_pbar=count_proof_pbar,\n",
    "        ),\n",
    "        sorted(relevant_seeds),\n",
    "    )\n",
    "\n",
    "\n",
    "def subcubic_approx_effective_dimension(\n",
    "    model: HookedTransformer, tricks: LargestWrongLogitQuadraticConfig\n",
    "):\n",
    "    return (\n",
    "        int(tricks.effective_dimension_estimate(model.cfg))\n",
    "        + subcubic_PVOU_cost\n",
    "        + subcubic_EPQKP_cost\n",
    "        + EVOU_cost\n",
    "    )\n",
    "\n",
    "\n",
    "for seed in subcubic_data:\n",
    "    for row in subcubic_data[seed]:\n",
    "        row[\"effective-dimensionality-estimate\"] = subcubic_approx_effective_dimension(\n",
    "            runtime_models[seed][1],\n",
    "            LargestWrongLogitQuadraticConfig.parse(row[\"tricks\"], latex=True),\n",
    "        )\n",
    "\n",
    "new_data = []\n",
    "for seed in sorted(subcubic_data.keys()):\n",
    "    new_data.extend(subcubic_data[seed])\n",
    "\n",
    "update_csv_with_rows(\n",
    "    SUBCUBIC_CSV_PATH, new_data, columns=subcubic_columns, subset=[\"seed\", \"tricks\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54ec16-e0d7-40af-9039-1e996ec50bac",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " Summary satistics subcubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cf590-4d00-4f9a-9568-d5b5c710e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "assert len(subcubic_data) == len(\n",
    "    brute_force_data\n",
    "), f\"len(cubic_data) == {len(subcubic_data)} != {len(brute_force_data)} == len(brute_force_data)\"\n",
    "\n",
    "\n",
    "def leading_complexity(tricks: LargestWrongLogitQuadraticConfig):\n",
    "    # tricks = LargestWrongLogitQuadraticConfig.parse(tricks_str)\n",
    "    return (\n",
    "        \"AlmostQuadratic\"\n",
    "        if tricks.is_quadratic\n",
    "        else (\n",
    "            \"SubcubicWithoutVocabSquared\"\n",
    "            if tricks.is_subcubic_no_quadratic_vocab\n",
    "            else \"Subcubic\" if tricks.is_subcubic else \"FakeSubcubic\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def subcubic_group(tricks: LargestWrongLogitQuadraticConfig):\n",
    "    # tricks = LargestWrongLogitQuadraticConfig.parse(tricks_str)\n",
    "    EUPU_str = (\n",
    "        \"DirectQuadratic\"\n",
    "        if tricks.EUPU_handling_quadratic\n",
    "        else (\n",
    "            \"DirectModelSquaredVocab\"\n",
    "            if tricks.EUPU_handling_subcubic_no_quadratic_vocab\n",
    "            else None if tricks.EUPU_handling_subcubic else \"DirectCubic\"\n",
    "        )\n",
    "    )\n",
    "    EPQKE_str = (\n",
    "        \"AttentionQuadratic\"\n",
    "        if tricks.attention_error_handling_quadratic\n",
    "        and tricks.attention_handling_quadratic\n",
    "        else (\n",
    "            \"AttentionModelSquaredVocab\"\n",
    "            if tricks.attention_error_handling_subcubic_no_quadratic_vocab\n",
    "            and tricks.attention_handling_subcubic_no_quadratic_vocab\n",
    "            else (\n",
    "                None\n",
    "                if tricks.attention_error_handling_subcubic\n",
    "                and tricks.attention_handling_subcubic\n",
    "                else \"AttentionCubic\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    strs = [s for s in (EPQKE_str, EUPU_str) if s is not None]\n",
    "    return \"Subcubic\" + (f\"{''.join(strs)}\" if strs else \"Group\")\n",
    "\n",
    "\n",
    "def filter_tricks_str_eq(value: str, tricks_str: str):\n",
    "    return value == tricks_str\n",
    "\n",
    "\n",
    "def filter_tricks_by_func(\n",
    "    value: str, func: Callable[[LargestWrongLogitQuadraticConfig], str], tricks_str: str\n",
    "):\n",
    "    return value == func(LargestWrongLogitQuadraticConfig.parse(tricks_str, latex=True))\n",
    "\n",
    "\n",
    "subcubic_leading_complexities = defaultdict(set)\n",
    "subcubic_groups = defaultdict(set)\n",
    "\n",
    "for tricks in all_configs:\n",
    "    tricks_str = tricks.short_description(latex=True)\n",
    "    subcubic_leading_complexities[leading_complexity(tricks)].add(tricks_str)\n",
    "    subcubic_groups[subcubic_group(tricks)].add(tricks_str)\n",
    "\n",
    "\n",
    "for trick_filter_descr, trick_filter in (\n",
    "    [\n",
    "        (\"AnySubcubic\", lambda tricks_str: True),\n",
    "        (\n",
    "            \"RealSubcubic\",\n",
    "            lambda tricks_str: LargestWrongLogitQuadraticConfig.parse(\n",
    "                tricks_str, latex=True\n",
    "            ).is_subcubic,\n",
    "        ),\n",
    "        (\n",
    "            \"SubcubicModelSquaredVocab\",\n",
    "            lambda tricks_str: LargestWrongLogitQuadraticConfig.parse(\n",
    "                tricks_str, latex=True\n",
    "            ).is_subcubic_no_quadratic_vocab,\n",
    "        ),\n",
    "    ]\n",
    "    + [(k, partial(filter_tricks_by_func, k, subcubic_group)) for k in subcubic_groups]\n",
    "    + [\n",
    "        (k, partial(filter_tricks_by_func, k, leading_complexity))\n",
    "        for k in subcubic_leading_complexities\n",
    "    ]\n",
    "    + [\n",
    "        (\n",
    "            f\"Subcubic{tricks.short_description(latex=True)}\",\n",
    "            partial(filter_tricks_str_eq, tricks.short_description(latex=True)),\n",
    "        )\n",
    "        for tricks in all_configs\n",
    "    ]\n",
    "):\n",
    "    filtered_subcubic_data = {\n",
    "        seed: [row for row in rows if trick_filter(row[\"tricks\"])]\n",
    "        for seed, rows in subcubic_data.items()\n",
    "    }\n",
    "    filtered_subcubic_data_best_by_key = defaultdict(dict)\n",
    "    for seed, rows in filtered_subcubic_data.items():\n",
    "        best_row = max(rows, key=lambda row: row[\"accuracy-bound\"])\n",
    "        for k, v in best_row.items():\n",
    "            filtered_subcubic_data_best_by_key[k][seed] = v\n",
    "    for key, latex_key in [\n",
    "        (\"accuracy-bound\", \"Accuracy\"),\n",
    "        (\"duration-proof-search\", \"ProofSearchTime\"),\n",
    "        (\"duration\", \"ProofTime\"),\n",
    "        (\"normalized-accuracy-bound\", \"NormalizedAccuracy\"),\n",
    "        (\"perf-time-enabled-ns\", \"PerfTimeEnabledNS\"),\n",
    "        (\"perf-instruction-count\", \"PerfInstructionCount\"),\n",
    "        (\"perf-branch-misses\", \"PerfBranchMisses\"),\n",
    "        (\"perf-page-faults\", \"PerfPageFaults\"),\n",
    "        (\"proof-flop-estimate\", \"InstructionCount\"),\n",
    "        (\"proof-int-op-estimate\", \"InstructionCountInt\"),\n",
    "        (\"proof-branch-estimate\", \"InstructionCountBranch\"),\n",
    "        (\"err-upper-bound\", \"ErrUpperBound\"),\n",
    "        (\"dropped-sequences\", \"DroppedSequences\"),\n",
    "        (\"dropped-sequences-frac\", \"DroppedSequencesFrac\"),\n",
    "        (\"most-gap-below-value\", \"GapMostBelowValue\"),\n",
    "        (\"most-gap-below-value-frac\", \"GapMostBelowValueSequenceFrac\"),\n",
    "        (\"most-gap-below-value-num-std\", \"GapMostBelowValueNumStd\"),\n",
    "        (\"max-gap\", \"MaxGap\"),\n",
    "        (\"effective-dimensionality-estimate\", \"EffectiveDimensionalityEstimate\"),\n",
    "    ]:\n",
    "        if key not in filtered_subcubic_data_best_by_key:\n",
    "            print(f\"Warning! Missing key {key}\")\n",
    "            continue\n",
    "        latex_values |= data_summary(\n",
    "            filtered_subcubic_data_best_by_key[key],\n",
    "            prefix=f\"{trick_filter_descr}OnlyBestAccBoundPerSeed{latex_key}\",\n",
    "        )\n",
    "        if any(len(rows) > 1 for rows in filtered_subcubic_data.values()):\n",
    "            latex_values |= data_summary(\n",
    "                [row[key] for rows in filtered_subcubic_data.values() for row in rows],\n",
    "                prefix=f\"{trick_filter_descr}{latex_key}\",\n",
    "            )\n",
    "        else:\n",
    "            # print(\n",
    "            #     f\"Skipping key {key} since values have at most one corresponding configuration\"\n",
    "            # )\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609a6b8-7b52-40ba-8f0e-121c302cc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "latex_values[\"AllModelsHEADSHA\"] = git.get_head_sha(short=False)\n",
    "latex_values[\"AllModelsHEADSHASHORT\"] = git.get_head_sha(short=True)\n",
    "\n",
    "with open(LATEX_VALUES_PATH, \"w\") as f:\n",
    "    f.write(to_latex_defs(latex_values))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
